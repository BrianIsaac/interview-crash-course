{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning Crash Course for Data Science Assessments\n",
    "\n",
    "**Last Updated:** 25 January 2026\n",
    "\n",
    "This notebook covers unsupervised learning techniques commonly tested in data science interviews. We focus on clustering algorithms, dimensionality reduction, and anomaly detection methods.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction and Setup](#1-introduction-and-setup)\n",
    "2. [Clustering Overview](#2-clustering-overview)\n",
    "3. [K-Means Clustering](#3-k-means-clustering)\n",
    "4. [Hierarchical Clustering](#4-hierarchical-clustering)\n",
    "5. [DBSCAN](#5-dbscan)\n",
    "6. [Clustering Evaluation Metrics](#6-clustering-evaluation-metrics)\n",
    "7. [Dimensionality Reduction Overview](#7-dimensionality-reduction-overview)\n",
    "8. [Principal Component Analysis (PCA)](#8-principal-component-analysis-pca)\n",
    "9. [t-SNE](#9-t-sne)\n",
    "10. [UMAP](#10-umap)\n",
    "11. [Anomaly Detection](#11-anomaly-detection)\n",
    "12. [Practice Questions](#12-practice-questions)\n",
    "13. [Summary](#13-summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction and Setup\n",
    "\n",
    "**Unsupervised learning** finds patterns in data without labelled outcomes. Unlike supervised learning, there's no \"correct answer\" to learn from.\n",
    "\n",
    "**Main Categories:**\n",
    "- **Clustering**: Group similar data points together\n",
    "- **Dimensionality Reduction**: Reduce features while preserving information\n",
    "- **Anomaly Detection**: Identify unusual data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "from sklearn.datasets import make_blobs, make_moons, load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, calinski_harabasz_score, davies_bouldin_score,\n",
    "    adjusted_rand_score, normalized_mutual_info_score\n",
    ")\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_blobs, y_blobs = make_blobs(n_samples=300, centers=4, cluster_std=0.6, random_state=42)\n",
    "X_moons, y_moons = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
    "\n",
    "iris = load_iris()\n",
    "X_iris, y_iris = iris.data, iris.target\n",
    "\n",
    "print(f\"Blobs dataset: {X_blobs.shape}\")\n",
    "print(f\"Moons dataset: {X_moons.shape}\")\n",
    "print(f\"Iris dataset: {X_iris.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_blobs, cmap='viridis', alpha=0.6)\n",
    "axes[0].set_title('Blobs Dataset (Well-separated clusters)')\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "\n",
    "axes[1].scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap='viridis', alpha=0.6)\n",
    "axes[1].set_title('Moons Dataset (Non-convex clusters)')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Clustering Overview\n",
    "\n",
    "**Clustering** groups data points such that points within a cluster are more similar to each other than to points in other clusters.\n",
    "\n",
    "| Algorithm | Cluster Shape | Requires K | Handles Noise | Scalability |\n",
    "|-----------|--------------|------------|---------------|-------------|\n",
    "| K-Means | Spherical | Yes | No | Excellent |\n",
    "| Hierarchical | Any | No* | No | Poor |\n",
    "| DBSCAN | Any | No | Yes | Good |\n",
    "\n",
    "*Hierarchical clustering produces a tree; you choose where to cut it.\n",
    "\n",
    "**When to use each:**\n",
    "- **K-Means**: Large datasets, spherical clusters, known K\n",
    "- **Hierarchical**: Small datasets, want to visualise hierarchy\n",
    "- **DBSCAN**: Unknown K, non-spherical clusters, noisy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. K-Means Clustering\n",
    "\n",
    "**K-Means** partitions data into K clusters by minimising within-cluster variance (inertia).\n",
    "\n",
    "**Algorithm:**\n",
    "1. Initialise K cluster centroids randomly\n",
    "2. Assign each point to nearest centroid\n",
    "3. Update centroids as mean of assigned points\n",
    "4. Repeat until convergence\n",
    "\n",
    "**Limitations:**\n",
    "- Must specify K beforehand\n",
    "- Assumes spherical clusters\n",
    "- Sensitive to initialisation (use k-means++)\n",
    "- Sensitive to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_kmeans(\n",
    "    X: np.ndarray,\n",
    "    n_clusters: int,\n",
    "    random_state: int = 42\n",
    ") -> Tuple[KMeans, np.ndarray]:\n",
    "    \"\"\"Fit K-Means clustering model.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix.\n",
    "        n_clusters: Number of clusters.\n",
    "        random_state: Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (fitted KMeans model, cluster labels).\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    return kmeans, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model, kmeans_labels = fit_kmeans(X_blobs, n_clusters=4)\n",
    "\n",
    "print(f\"Cluster centres shape: {kmeans_model.cluster_centers_.shape}\")\n",
    "print(f\"Inertia (within-cluster sum of squares): {kmeans_model.inertia_:.2f}\")\n",
    "print(f\"Unique labels: {np.unique(kmeans_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=kmeans_labels, cmap='viridis', alpha=0.6)\n",
    "plt.scatter(\n",
    "    kmeans_model.cluster_centers_[:, 0],\n",
    "    kmeans_model.cluster_centers_[:, 1],\n",
    "    c='red', marker='X', s=200, edgecolors='black', linewidths=2,\n",
    "    label='Centroids'\n",
    ")\n",
    "plt.title('K-Means Clustering Results')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow Method for Choosing K\n",
    "\n",
    "The **elbow method** plots inertia vs K and looks for the \"elbow\" where adding more clusters provides diminishing returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_method(\n",
    "    X: np.ndarray,\n",
    "    k_range: range = range(1, 11)\n",
    ") -> List[float]:\n",
    "    \"\"\"Compute inertia for different values of K.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix.\n",
    "        k_range: Range of K values to try.\n",
    "    \n",
    "    Returns:\n",
    "        List of inertia values for each K.\n",
    "    \"\"\"\n",
    "    inertias = []\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(X)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    return inertias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = range(1, 11)\n",
    "inertias = elbow_method(X_blobs, k_range)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "plt.axvline(x=4, color='red', linestyle='--', label='Elbow at K=4')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Analysis\n",
    "\n",
    "The **silhouette score** measures how similar a point is to its own cluster compared to other clusters. Range: [-1, 1], higher is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_analysis(\n",
    "    X: np.ndarray,\n",
    "    k_range: range = range(2, 11)\n",
    ") -> List[float]:\n",
    "    \"\"\"Compute silhouette scores for different K values.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix.\n",
    "        k_range: Range of K values to try.\n",
    "    \n",
    "    Returns:\n",
    "        List of silhouette scores for each K.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        score = silhouette_score(X, labels)\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = range(2, 11)\n",
    "silhouette_scores = silhouette_analysis(X_blobs, k_range)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_range, silhouette_scores, 'go-', linewidth=2, markersize=8)\n",
    "plt.axvline(x=4, color='red', linestyle='--', label='Best K=4')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Analysis for Optimal K')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best K: {k_range[np.argmax(silhouette_scores)]} with score {max(silhouette_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Hierarchical Clustering\n",
    "\n",
    "**Hierarchical clustering** builds a tree (dendrogram) of clusters, either bottom-up (agglomerative) or top-down (divisive).\n",
    "\n",
    "**Agglomerative (most common):**\n",
    "1. Start with each point as its own cluster\n",
    "2. Merge the two closest clusters\n",
    "3. Repeat until one cluster remains\n",
    "\n",
    "**Linkage Methods:**\n",
    "\n",
    "| Method | Distance Between Clusters |\n",
    "|--------|---------------------------|\n",
    "| Single | Minimum distance between points |\n",
    "| Complete | Maximum distance between points |\n",
    "| Average | Average distance between all pairs |\n",
    "| Ward | Minimises variance increase |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(\n",
    "    X: np.ndarray,\n",
    "    method: str = 'ward',\n",
    "    truncate_mode: str = 'level',\n",
    "    p: int = 5\n",
    ") -> None:\n",
    "    \"\"\"Plot hierarchical clustering dendrogram.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix.\n",
    "        method: Linkage method.\n",
    "        truncate_mode: How to truncate dendrogram.\n",
    "        p: Truncation parameter.\n",
    "    \"\"\"\n",
    "    linked = linkage(X, method=method)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    dendrogram(\n",
    "        linked,\n",
    "        truncate_mode=truncate_mode,\n",
    "        p=p,\n",
    "        leaf_rotation=90,\n",
    "        leaf_font_size=10\n",
    "    )\n",
    "    plt.title(f'Hierarchical Clustering Dendrogram ({method} linkage)')\n",
    "    plt.xlabel('Sample Index or Cluster Size')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample = X_blobs[:100]\n",
    "plot_dendrogram(X_sample, method='ward', p=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_hierarchical(\n",
    "    X: np.ndarray,\n",
    "    n_clusters: int,\n",
    "    linkage: str = 'ward'\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Fit agglomerative hierarchical clustering.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix.\n",
    "        n_clusters: Number of clusters.\n",
    "        linkage: Linkage criterion.\n",
    "    \n",
    "    Returns:\n",
    "        Cluster labels.\n",
    "    \"\"\"\n",
    "    model = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)\n",
    "    return model.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_labels = fit_hierarchical(X_blobs, n_clusters=4)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=hier_labels, cmap='viridis', alpha=0.6)\n",
    "plt.title('Hierarchical Clustering Results (Ward Linkage)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. DBSCAN\n",
    "\n",
    "**DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) groups points that are closely packed together.\n",
    "\n",
    "**Key Parameters:**\n",
    "- **eps**: Maximum distance between two samples to be considered neighbours\n",
    "- **min_samples**: Minimum points required to form a dense region\n",
    "\n",
    "**Point Types:**\n",
    "- **Core point**: Has at least min_samples within eps distance\n",
    "- **Border point**: Within eps of a core point but not a core itself\n",
    "- **Noise point**: Neither core nor border (labelled as -1)\n",
    "\n",
    "**Advantages:**\n",
    "- No need to specify K\n",
    "- Can find arbitrarily shaped clusters\n",
    "- Robust to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_dbscan(\n",
    "    X: np.ndarray,\n",
    "    eps: float = 0.5,\n",
    "    min_samples: int = 5\n",
    ") -> Tuple[DBSCAN, np.ndarray]:\n",
    "    \"\"\"Fit DBSCAN clustering model.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix.\n",
    "        eps: Maximum neighbour distance.\n",
    "        min_samples: Minimum points for core point.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (fitted DBSCAN model, cluster labels).\n",
    "    \"\"\"\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(X)\n",
    "    return dbscan, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_model, dbscan_labels = fit_dbscan(X_moons, eps=0.2, min_samples=5)\n",
    "\n",
    "n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "n_noise = list(dbscan_labels).count(-1)\n",
    "\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "print(f\"Number of noise points: {n_noise}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "kmeans_moons, kmeans_moons_labels = fit_kmeans(X_moons, n_clusters=2)\n",
    "axes[0].scatter(X_moons[:, 0], X_moons[:, 1], c=kmeans_moons_labels, cmap='viridis', alpha=0.6)\n",
    "axes[0].set_title('K-Means on Moons (Fails)')\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "\n",
    "axes[1].scatter(X_moons[:, 0], X_moons[:, 1], c=dbscan_labels, cmap='viridis', alpha=0.6)\n",
    "axes[1].set_title('DBSCAN on Moons (Success)')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Clustering Evaluation Metrics\n",
    "\n",
    "### Internal Metrics (No Ground Truth)\n",
    "\n",
    "| Metric | Range | Interpretation |\n",
    "|--------|-------|----------------|\n",
    "| Silhouette Score | [-1, 1] | Higher = better separation |\n",
    "| Calinski-Harabasz | [0, inf) | Higher = denser, well-separated |\n",
    "| Davies-Bouldin | [0, inf) | Lower = better separation |\n",
    "\n",
    "### External Metrics (With Ground Truth)\n",
    "\n",
    "| Metric | Range | Interpretation |\n",
    "|--------|-------|----------------|\n",
    "| Adjusted Rand Index | [-1, 1] | 1 = perfect match |\n",
    "| Normalised Mutual Info | [0, 1] | 1 = perfect match |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clustering(\n",
    "    X: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    true_labels: np.ndarray = None\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate clustering quality with multiple metrics.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix.\n",
    "        labels: Predicted cluster labels.\n",
    "        true_labels: Ground truth labels (optional).\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of metric names and values.\n",
    "    \"\"\"\n",
    "    valid_mask = labels != -1\n",
    "    \n",
    "    if valid_mask.sum() < 2 or len(np.unique(labels[valid_mask])) < 2:\n",
    "        return {\"Error\": \"Not enough valid clusters\"}\n",
    "    \n",
    "    metrics = {\n",
    "        'Silhouette': silhouette_score(X[valid_mask], labels[valid_mask]),\n",
    "        'Calinski-Harabasz': calinski_harabasz_score(X[valid_mask], labels[valid_mask]),\n",
    "        'Davies-Bouldin': davies_bouldin_score(X[valid_mask], labels[valid_mask])\n",
    "    }\n",
    "    \n",
    "    if true_labels is not None:\n",
    "        metrics['Adjusted Rand Index'] = adjusted_rand_score(true_labels, labels)\n",
    "        metrics['Normalised Mutual Info'] = normalized_mutual_info_score(true_labels, labels)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"K-Means Evaluation (Blobs):\")\n",
    "kmeans_metrics = evaluate_clustering(X_blobs, kmeans_labels, y_blobs)\n",
    "for metric, value in kmeans_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nDBSCAN Evaluation (Moons):\")\n",
    "dbscan_metrics = evaluate_clustering(X_moons, dbscan_labels, y_moons)\n",
    "for metric, value in dbscan_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Dimensionality Reduction Overview\n",
    "\n",
    "**Dimensionality reduction** transforms high-dimensional data to lower dimensions while preserving important structure.\n",
    "\n",
    "**Use Cases:**\n",
    "- Visualisation (reduce to 2D/3D)\n",
    "- Noise reduction\n",
    "- Feature extraction\n",
    "- Computational efficiency\n",
    "\n",
    "| Method | Type | Preserves | Best For |\n",
    "|--------|------|-----------|----------|\n",
    "| PCA | Linear | Global variance | Feature extraction, preprocessing |\n",
    "| t-SNE | Non-linear | Local structure | Visualisation |\n",
    "| UMAP | Non-linear | Local + global | Visualisation, clustering |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Principal Component Analysis (PCA)\n",
    "\n",
    "**PCA** finds orthogonal directions (principal components) that capture maximum variance in the data.\n",
    "\n",
    "**Algorithm:**\n",
    "1. Centre the data (subtract mean)\n",
    "2. Compute covariance matrix\n",
    "3. Find eigenvectors and eigenvalues\n",
    "4. Sort by eigenvalue (variance explained)\n",
    "5. Project data onto top K eigenvectors\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Explained variance ratio**: How much variance each component captures\n",
    "- **Cumulative explained variance**: Total variance captured by first K components\n",
    "- **Loadings**: How much each original feature contributes to each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pca(\n",
    "    X: np.ndarray,\n",
    "    n_components: int = 2\n",
    ") -> Tuple[PCA, np.ndarray]:\n",
    "    \"\"\"Fit PCA and transform data.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix.\n",
    "        n_components: Number of components to keep.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (fitted PCA model, transformed data).\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    return pca, X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model, X_iris_pca = fit_pca(X_iris, n_components=2)\n",
    "\n",
    "print(f\"Original shape: {X_iris.shape}\")\n",
    "print(f\"Reduced shape: {X_iris_pca.shape}\")\n",
    "print(f\"\\nExplained variance ratio: {pca_model.explained_variance_ratio_}\")\n",
    "print(f\"Total variance explained: {sum(pca_model.explained_variance_ratio_):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_iris_pca[:, 0], X_iris_pca[:, 1], c=y_iris, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, label='Species')\n",
    "plt.xlabel(f'PC1 ({pca_model.explained_variance_ratio_[0]:.1%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca_model.explained_variance_ratio_[1]:.1%} variance)')\n",
    "plt.title('PCA of Iris Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Number of Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_explained_variance(X: np.ndarray) -> None:\n",
    "    \"\"\"Plot cumulative explained variance to choose n_components.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    pca = PCA()\n",
    "    pca.fit(X_scaled)\n",
    "    \n",
    "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    axes[0].bar(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "                pca.explained_variance_ratio_, alpha=0.7)\n",
    "    axes[0].set_xlabel('Principal Component')\n",
    "    axes[0].set_ylabel('Explained Variance Ratio')\n",
    "    axes[0].set_title('Scree Plot')\n",
    "    \n",
    "    axes[1].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'bo-')\n",
    "    axes[1].axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n",
    "    axes[1].set_xlabel('Number of Components')\n",
    "    axes[1].set_ylabel('Cumulative Explained Variance')\n",
    "    axes[1].set_title('Cumulative Explained Variance')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_explained_variance(X_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. t-SNE\n",
    "\n",
    "**t-SNE** (t-distributed Stochastic Neighbour Embedding) is a non-linear technique that preserves local structure, making it excellent for visualisation.\n",
    "\n",
    "**How it works:**\n",
    "1. Compute pairwise similarities in high-dimensional space (Gaussian)\n",
    "2. Compute pairwise similarities in low-dimensional space (t-distribution)\n",
    "3. Minimise KL divergence between the two distributions\n",
    "\n",
    "**Key Parameters:**\n",
    "- **perplexity**: Balance between local and global structure (typically 5-50)\n",
    "- **n_iter**: Number of iterations\n",
    "- **learning_rate**: Step size for optimisation\n",
    "\n",
    "**Limitations:**\n",
    "- Computationally expensive\n",
    "- Non-deterministic (different runs give different results)\n",
    "- Cannot transform new data (no `transform` method)\n",
    "- Distances in output are not meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tsne(\n",
    "    X: np.ndarray,\n",
    "    n_components: int = 2,\n",
    "    perplexity: float = 30.0,\n",
    "    random_state: int = 42\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Fit t-SNE and transform data.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix.\n",
    "        n_components: Number of output dimensions.\n",
    "        perplexity: Perplexity parameter.\n",
    "        random_state: Random seed.\n",
    "    \n",
    "    Returns:\n",
    "        Transformed data.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    tsne = TSNE(\n",
    "        n_components=n_components,\n",
    "        perplexity=perplexity,\n",
    "        random_state=random_state,\n",
    "        n_iter=1000\n",
    "    )\n",
    "    \n",
    "    return tsne.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_iris_tsne = fit_tsne(X_iris, perplexity=30)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_iris_tsne[:, 0], X_iris_tsne[:, 1], c=y_iris, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, label='Species')\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "plt.title('t-SNE of Iris Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "perplexities = [5, 30, 50]\n",
    "\n",
    "for ax, perp in zip(axes, perplexities):\n",
    "    X_tsne = fit_tsne(X_iris, perplexity=perp)\n",
    "    ax.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_iris, cmap='viridis', alpha=0.7)\n",
    "    ax.set_title(f'Perplexity = {perp}')\n",
    "    ax.set_xlabel('t-SNE 1')\n",
    "    ax.set_ylabel('t-SNE 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. UMAP\n",
    "\n",
    "**UMAP** (Uniform Manifold Approximation and Projection) is a newer technique that often outperforms t-SNE.\n",
    "\n",
    "**Advantages over t-SNE:**\n",
    "- Faster (especially for large datasets)\n",
    "- Preserves more global structure\n",
    "- Can transform new data\n",
    "- More deterministic\n",
    "\n",
    "**Key Parameters:**\n",
    "- **n_neighbors**: Size of local neighbourhood (like perplexity)\n",
    "- **min_dist**: Minimum distance between points in output\n",
    "- **metric**: Distance metric to use\n",
    "\n",
    "**Note**: UMAP requires the `umap-learn` package. We'll demonstrate the concept with sklearn's implementation notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import umap\n",
    "    UMAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    UMAP_AVAILABLE = False\n",
    "    print(\"UMAP not installed. Install with: pip install umap-learn\")\n",
    "    print(\"Skipping UMAP demonstration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if UMAP_AVAILABLE:\n",
    "    def fit_umap(\n",
    "        X: np.ndarray,\n",
    "        n_components: int = 2,\n",
    "        n_neighbors: int = 15,\n",
    "        min_dist: float = 0.1,\n",
    "        random_state: int = 42\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Fit UMAP and transform data.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature matrix.\n",
    "            n_components: Number of output dimensions.\n",
    "            n_neighbors: Size of local neighbourhood.\n",
    "            min_dist: Minimum distance in output.\n",
    "            random_state: Random seed.\n",
    "        \n",
    "        Returns:\n",
    "            Transformed data.\n",
    "        \"\"\"\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        reducer = umap.UMAP(\n",
    "            n_components=n_components,\n",
    "            n_neighbors=n_neighbors,\n",
    "            min_dist=min_dist,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        return reducer.fit_transform(X_scaled)\n",
    "    \n",
    "    X_iris_umap = fit_umap(X_iris)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(X_iris_umap[:, 0], X_iris_umap[:, 1], c=y_iris, cmap='viridis', alpha=0.7)\n",
    "    plt.colorbar(scatter, label='Species')\n",
    "    plt.xlabel('UMAP 1')\n",
    "    plt.ylabel('UMAP 2')\n",
    "    plt.title('UMAP of Iris Dataset')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Dimensionality Reduction Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3 if UMAP_AVAILABLE else 2, figsize=(15 if UMAP_AVAILABLE else 12, 5))\n",
    "\n",
    "axes[0].scatter(X_iris_pca[:, 0], X_iris_pca[:, 1], c=y_iris, cmap='viridis', alpha=0.7)\n",
    "axes[0].set_title('PCA')\n",
    "axes[0].set_xlabel('PC1')\n",
    "axes[0].set_ylabel('PC2')\n",
    "\n",
    "axes[1].scatter(X_iris_tsne[:, 0], X_iris_tsne[:, 1], c=y_iris, cmap='viridis', alpha=0.7)\n",
    "axes[1].set_title('t-SNE')\n",
    "axes[1].set_xlabel('t-SNE 1')\n",
    "axes[1].set_ylabel('t-SNE 2')\n",
    "\n",
    "if UMAP_AVAILABLE:\n",
    "    axes[2].scatter(X_iris_umap[:, 0], X_iris_umap[:, 1], c=y_iris, cmap='viridis', alpha=0.7)\n",
    "    axes[2].set_title('UMAP')\n",
    "    axes[2].set_xlabel('UMAP 1')\n",
    "    axes[2].set_ylabel('UMAP 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Anomaly Detection\n",
    "\n",
    "**Anomaly detection** identifies data points that deviate significantly from the norm.\n",
    "\n",
    "**Types of Anomalies:**\n",
    "- **Point anomalies**: Individual outliers\n",
    "- **Contextual anomalies**: Anomalous in specific context\n",
    "- **Collective anomalies**: Group of points that are anomalous together\n",
    "\n",
    "**Common Methods:**\n",
    "\n",
    "| Method | Approach | Best For |\n",
    "|--------|----------|----------|\n",
    "| Isolation Forest | Tree-based isolation | High-dimensional data |\n",
    "| Local Outlier Factor | Density-based | Local anomalies |\n",
    "| One-Class SVM | Boundary-based | Well-defined normal class |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation Forest\n",
    "\n",
    "**Isolation Forest** isolates anomalies by randomly selecting features and split values. Anomalies are easier to isolate (shorter path length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X_normal = np.random.randn(200, 2)\n",
    "X_anomalies = np.random.uniform(low=-4, high=4, size=(20, 2))\n",
    "X_combined = np.vstack([X_normal, X_anomalies])\n",
    "y_true = np.array([1] * 200 + [-1] * 20)\n",
    "\n",
    "print(f\"Normal points: {len(X_normal)}\")\n",
    "print(f\"Anomaly points: {len(X_anomalies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_isolation_forest(\n",
    "    X: np.ndarray,\n",
    "    contamination: float = 0.1,\n",
    "    random_state: int = 42\n",
    ") -> Tuple[IsolationForest, np.ndarray]:\n",
    "    \"\"\"Fit Isolation Forest for anomaly detection.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix.\n",
    "        contamination: Expected proportion of anomalies.\n",
    "        random_state: Random seed.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (fitted model, predictions: 1=normal, -1=anomaly).\n",
    "    \"\"\"\n",
    "    iso_forest = IsolationForest(\n",
    "        contamination=contamination,\n",
    "        random_state=random_state,\n",
    "        n_estimators=100\n",
    "    )\n",
    "    predictions = iso_forest.fit_predict(X)\n",
    "    return iso_forest, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_model, iso_predictions = fit_isolation_forest(X_combined, contamination=0.1)\n",
    "\n",
    "n_detected = (iso_predictions == -1).sum()\n",
    "print(f\"Detected anomalies: {n_detected}\")\n",
    "print(f\"True anomalies: 20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "normal_mask = iso_predictions == 1\n",
    "anomaly_mask = iso_predictions == -1\n",
    "\n",
    "plt.scatter(X_combined[normal_mask, 0], X_combined[normal_mask, 1], \n",
    "            c='blue', label='Normal', alpha=0.6)\n",
    "plt.scatter(X_combined[anomaly_mask, 0], X_combined[anomaly_mask, 1], \n",
    "            c='red', label='Anomaly', alpha=0.8, marker='x', s=100)\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Isolation Forest Anomaly Detection')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Outlier Factor (LOF)\n",
    "\n",
    "**LOF** compares the local density of a point to its neighbours. Points with substantially lower density are anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lof(\n",
    "    X: np.ndarray,\n",
    "    n_neighbors: int = 20,\n",
    "    contamination: float = 0.1\n",
    ") -> Tuple[LocalOutlierFactor, np.ndarray]:\n",
    "    \"\"\"Fit Local Outlier Factor for anomaly detection.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix.\n",
    "        n_neighbors: Number of neighbours for density estimation.\n",
    "        contamination: Expected proportion of anomalies.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (fitted model, predictions: 1=normal, -1=anomaly).\n",
    "    \"\"\"\n",
    "    lof = LocalOutlierFactor(\n",
    "        n_neighbors=n_neighbors,\n",
    "        contamination=contamination,\n",
    "        novelty=False\n",
    "    )\n",
    "    predictions = lof.fit_predict(X)\n",
    "    return lof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_model, lof_predictions = fit_lof(X_combined, contamination=0.1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, (predictions, title) in zip(axes, [(iso_predictions, 'Isolation Forest'), \n",
    "                                            (lof_predictions, 'Local Outlier Factor')]):\n",
    "    normal_mask = predictions == 1\n",
    "    anomaly_mask = predictions == -1\n",
    "    \n",
    "    ax.scatter(X_combined[normal_mask, 0], X_combined[normal_mask, 1], \n",
    "               c='blue', label='Normal', alpha=0.6)\n",
    "    ax.scatter(X_combined[anomaly_mask, 0], X_combined[anomaly_mask, 1], \n",
    "               c='red', label='Anomaly', alpha=0.8, marker='x', s=100)\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies_zscore(\n",
    "    X: np.ndarray,\n",
    "    threshold: float = 3.0\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Detect anomalies using Z-score method.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix.\n",
    "        threshold: Z-score threshold for anomaly.\n",
    "    \n",
    "    Returns:\n",
    "        Boolean array (True = anomaly).\n",
    "    \"\"\"\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    z_scores = np.abs((X - mean) / std)\n",
    "    return np.any(z_scores > threshold, axis=1)\n",
    "\n",
    "\n",
    "def detect_anomalies_iqr(\n",
    "    X: np.ndarray,\n",
    "    multiplier: float = 1.5\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Detect anomalies using IQR method.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix.\n",
    "        multiplier: IQR multiplier for bounds.\n",
    "    \n",
    "    Returns:\n",
    "        Boolean array (True = anomaly).\n",
    "    \"\"\"\n",
    "    Q1 = np.percentile(X, 25, axis=0)\n",
    "    Q3 = np.percentile(X, 75, axis=0)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - multiplier * IQR\n",
    "    upper_bound = Q3 + multiplier * IQR\n",
    "    \n",
    "    return np.any((X < lower_bound) | (X > upper_bound), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore_anomalies = detect_anomalies_zscore(X_combined, threshold=2.5)\n",
    "iqr_anomalies = detect_anomalies_iqr(X_combined, multiplier=1.5)\n",
    "\n",
    "print(f\"Z-score detected anomalies: {zscore_anomalies.sum()}\")\n",
    "print(f\"IQR detected anomalies: {iqr_anomalies.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 12. Practice Questions\n\nTest your understanding with these interview-style questions. Try to solve each question in the empty code cell before revealing the answer."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 1: K-Means from Scratch\n\nImplement a simplified K-Means algorithm from scratch."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nimport numpy as np\nfrom typing import Tuple\n\ndef kmeans_scratch(\n    X: np.ndarray,\n    k: int,\n    max_iters: int = 100,\n    random_state: int = 42\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"K-Means clustering from scratch.\n    \n    Args:\n        X: Feature matrix.\n        k: Number of clusters.\n        max_iters: Maximum iterations.\n        random_state: Random seed.\n    \n    Returns:\n        Tuple of (centroids, labels).\n    \"\"\"\n    np.random.seed(random_state)\n    n_samples = X.shape[0]\n    \n    # Randomly initialise centroids\n    indices = np.random.choice(n_samples, k, replace=False)\n    centroids = X[indices].copy()\n    \n    for _ in range(max_iters):\n        # Assign points to nearest centroid\n        distances = np.sqrt(((X[:, np.newaxis] - centroids) ** 2).sum(axis=2))\n        labels = np.argmin(distances, axis=1)\n        \n        # Update centroids\n        new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])\n        \n        # Check convergence\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    \n    return centroids, labels\n\n\n# Test\nX_test, _ = make_blobs(n_samples=100, centers=3, random_state=42)\ncentroids, labels = kmeans_scratch(X_test, k=3)\nprint(f\"Centroids shape: {centroids.shape}\")\nprint(f\"Unique labels: {np.unique(labels)}\")\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 2: Optimal K Selection\n\nWrite a function that uses both elbow and silhouette methods to suggest optimal K."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nimport numpy as np\n\ndef find_optimal_k(\n    X: np.ndarray,\n    k_range: range = range(2, 11)\n) -> dict:\n    \"\"\"Find optimal K using multiple methods.\n    \n    Args:\n        X: Feature matrix.\n        k_range: Range of K values to try.\n    \n    Returns:\n        Dictionary with analysis results.\n    \"\"\"\n    inertias = []\n    silhouettes = []\n    \n    for k in k_range:\n        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n        labels = kmeans.fit_predict(X)\n        inertias.append(kmeans.inertia_)\n        silhouettes.append(silhouette_score(X, labels))\n    \n    # Find elbow using second derivative\n    diffs = np.diff(inertias)\n    elbow_k = k_range[np.argmax(np.diff(diffs)) + 2]\n    \n    # Best silhouette\n    best_silhouette_k = k_range[np.argmax(silhouettes)]\n    \n    return {\n        'elbow_k': elbow_k,\n        'silhouette_k': best_silhouette_k,\n        'inertias': inertias,\n        'silhouettes': silhouettes\n    }\n\n\n# Test\nX_test, _ = make_blobs(n_samples=300, centers=4, random_state=42)\nresults = find_optimal_k(X_test)\nprint(f\"Elbow suggests K={results['elbow_k']}\")\nprint(f\"Silhouette suggests K={results['silhouette_k']}\")\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 3: PCA Explained Variance\n\nFind the number of components needed to explain at least 95% of variance."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\ndef find_n_components(\n    X: np.ndarray,\n    variance_threshold: float = 0.95\n) -> int:\n    \"\"\"Find number of PCA components for given variance threshold.\n    \n    Args:\n        X: Feature matrix.\n        variance_threshold: Minimum cumulative variance to explain.\n    \n    Returns:\n        Number of components needed.\n    \"\"\"\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    pca = PCA()\n    pca.fit(X_scaled)\n    \n    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n    n_components = np.argmax(cumulative_variance >= variance_threshold) + 1\n    \n    return n_components\n\n\n# Test\nfrom sklearn.datasets import load_iris\niris = load_iris()\nn = find_n_components(iris.data, 0.95)\nprint(f\"Components needed for 95% variance: {n}\")\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 4: DBSCAN Parameter Tuning\n\nImplement a function to find good eps value using k-distance graph."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nfrom sklearn.neighbors import NearestNeighbors\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_k_distance(\n    X: np.ndarray,\n    k: int = 5\n) -> np.ndarray:\n    \"\"\"Plot k-distance graph to find optimal eps for DBSCAN.\n    \n    Args:\n        X: Feature matrix.\n        k: Number of nearest neighbours (min_samples - 1).\n    \n    Returns:\n        Sorted k-distances.\n    \"\"\"\n    nn = NearestNeighbors(n_neighbors=k)\n    nn.fit(X)\n    distances, _ = nn.kneighbors(X)\n    \n    k_distances = np.sort(distances[:, -1])[::-1]\n    \n    plt.figure(figsize=(8, 5))\n    plt.plot(k_distances)\n    plt.xlabel('Points (sorted by distance)')\n    plt.ylabel(f'{k}-th Nearest Neighbour Distance')\n    plt.title('K-Distance Graph for DBSCAN eps Selection')\n    plt.show()\n    \n    return k_distances\n\n\n# Test\nX_test, _ = make_moons(n_samples=200, noise=0.05, random_state=42)\ndistances = plot_k_distance(X_test, k=5)\nprint(f\"Suggested eps (elbow): ~{distances[int(len(distances)*0.1)]:.3f}\")\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 5: Clustering Comparison\n\nCompare K-Means, Hierarchical, and DBSCAN on a dataset and return the best method."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score\nfrom typing import Dict\n\ndef compare_clustering_methods(\n    X: np.ndarray,\n    n_clusters: int = 3\n) -> Dict[str, float]:\n    \"\"\"Compare clustering methods using silhouette score.\n    \n    Args:\n        X: Feature matrix.\n        n_clusters: Number of clusters (for K-Means and Hierarchical).\n    \n    Returns:\n        Dictionary of method names and silhouette scores.\n    \"\"\"\n    results = {}\n    \n    # K-Means\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n    kmeans_labels = kmeans.fit_predict(X)\n    results['K-Means'] = silhouette_score(X, kmeans_labels)\n    \n    # Hierarchical\n    hier = AgglomerativeClustering(n_clusters=n_clusters)\n    hier_labels = hier.fit_predict(X)\n    results['Hierarchical'] = silhouette_score(X, hier_labels)\n    \n    # DBSCAN\n    dbscan = DBSCAN(eps=0.5, min_samples=5)\n    dbscan_labels = dbscan.fit_predict(X)\n    if len(set(dbscan_labels) - {-1}) >= 2:\n        valid_mask = dbscan_labels != -1\n        results['DBSCAN'] = silhouette_score(X[valid_mask], dbscan_labels[valid_mask])\n    else:\n        results['DBSCAN'] = -1\n    \n    return results\n\n\n# Test\nX_test, _ = make_blobs(n_samples=200, centers=3, random_state=42)\nscores = compare_clustering_methods(X_test, n_clusters=3)\nfor method, score in scores.items():\n    print(f\"{method}: {score:.4f}\")\nprint(f\"Best method: {max(scores, key=scores.get)}\")\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 6: Anomaly Detection Pipeline\n\nCreate a function that combines multiple anomaly detection methods and returns consensus anomalies."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nimport numpy as np\n\ndef ensemble_anomaly_detection(\n    X: np.ndarray,\n    contamination: float = 0.1,\n    consensus_threshold: int = 2\n) -> np.ndarray:\n    \"\"\"Detect anomalies using ensemble of methods.\n    \n    Args:\n        X: Feature matrix.\n        contamination: Expected anomaly proportion.\n        consensus_threshold: Minimum methods that must agree.\n    \n    Returns:\n        Boolean array (True = anomaly).\n    \"\"\"\n    # Isolation Forest\n    iso = IsolationForest(contamination=contamination, random_state=42)\n    iso_pred = iso.fit_predict(X) == -1\n    \n    # LOF\n    lof = LocalOutlierFactor(contamination=contamination)\n    lof_pred = lof.fit_predict(X) == -1\n    \n    # Z-score\n    z_scores = np.abs((X - X.mean(axis=0)) / X.std(axis=0))\n    zscore_pred = np.any(z_scores > 3, axis=1)\n    \n    # Consensus\n    votes = iso_pred.astype(int) + lof_pred.astype(int) + zscore_pred.astype(int)\n    \n    return votes >= consensus_threshold\n\n\n# Test\nnp.random.seed(42)\nX_normal = np.random.randn(100, 2)\nX_outliers = np.random.uniform(-5, 5, (10, 2))\nX_test = np.vstack([X_normal, X_outliers])\n\nanomalies = ensemble_anomaly_detection(X_test)\nprint(f\"Detected anomalies: {anomalies.sum()}\")\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 7: PCA Feature Importance\n\nDetermine which original features contribute most to the first principal component."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nfrom typing import List, Tuple\n\ndef pca_feature_importance(\n    X: np.ndarray,\n    feature_names: List[str],\n    n_components: int = 1\n) -> List[Tuple[str, float]]:\n    \"\"\"Get feature importance from PCA loadings.\n    \n    Args:\n        X: Feature matrix.\n        feature_names: Names of features.\n        n_components: Number of components to analyse.\n    \n    Returns:\n        List of (feature_name, importance) tuples sorted by importance.\n    \"\"\"\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    pca = PCA(n_components=n_components)\n    pca.fit(X_scaled)\n    \n    # Loadings for first component (absolute values)\n    loadings = np.abs(pca.components_[0])\n    \n    importance = list(zip(feature_names, loadings))\n    importance.sort(key=lambda x: x[1], reverse=True)\n    \n    return importance\n\n\n# Test\nfrom sklearn.datasets import load_iris\niris = load_iris()\nimportance = pca_feature_importance(iris.data, iris.feature_names)\nprint(\"Feature importance (PC1):\")\nfor name, score in importance:\n    print(f\"  {name}: {score:.4f}\")\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 8: Silhouette Score from Scratch\n\nImplement silhouette score calculation without using sklearn."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef silhouette_score_scratch(\n    X: np.ndarray,\n    labels: np.ndarray\n) -> float:\n    \"\"\"Compute silhouette score from scratch.\n    \n    Args:\n        X: Feature matrix.\n        labels: Cluster labels.\n    \n    Returns:\n        Mean silhouette score.\n    \"\"\"\n    n_samples = len(X)\n    unique_labels = np.unique(labels)\n    \n    distances = cdist(X, X, metric='euclidean')\n    silhouettes = []\n    \n    for i in range(n_samples):\n        # a(i): mean distance to same cluster\n        same_cluster = labels == labels[i]\n        same_cluster[i] = False\n        if same_cluster.sum() > 0:\n            a_i = distances[i, same_cluster].mean()\n        else:\n            a_i = 0\n        \n        # b(i): min mean distance to other clusters\n        b_i = float('inf')\n        for label in unique_labels:\n            if label != labels[i]:\n                other_cluster = labels == label\n                mean_dist = distances[i, other_cluster].mean()\n                b_i = min(b_i, mean_dist)\n        \n        # Silhouette for point i\n        s_i = (b_i - a_i) / max(a_i, b_i) if max(a_i, b_i) > 0 else 0\n        silhouettes.append(s_i)\n    \n    return np.mean(silhouettes)\n\n\n# Test and compare\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nX, _ = make_blobs(n_samples=100, centers=3, random_state=42)\nlabels = KMeans(n_clusters=3, random_state=42, n_init=10).fit_predict(X)\n\nscratch_score = silhouette_score_scratch(X, labels)\nsklearn_score = silhouette_score(X, labels)\n\nprint(f\"From scratch: {scratch_score:.4f}\")\nprint(f\"Sklearn: {sklearn_score:.4f}\")\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 9: Cluster Assignment for New Data\n\nGiven trained K-Means centroids, assign new data points to clusters."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nimport numpy as np\n\ndef assign_to_clusters(\n    X_new: np.ndarray,\n    centroids: np.ndarray\n) -> np.ndarray:\n    \"\"\"Assign new points to existing clusters.\n    \n    Args:\n        X_new: New data points.\n        centroids: Cluster centroids from training.\n    \n    Returns:\n        Cluster assignments for new points.\n    \"\"\"\n    distances = np.sqrt(((X_new[:, np.newaxis] - centroids) ** 2).sum(axis=2))\n    return np.argmin(distances, axis=1)\n\n\n# Test\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\nX_train, _ = make_blobs(n_samples=100, centers=3, random_state=42)\nkmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\nkmeans.fit(X_train)\n\nX_new = np.array([[0, 0], [5, 5], [-5, -5]])\nlabels_scratch = assign_to_clusters(X_new, kmeans.cluster_centers_)\nlabels_sklearn = kmeans.predict(X_new)\n\nprint(f\"From scratch: {labels_scratch}\")\nprint(f\"Sklearn: {labels_sklearn}\")\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 10: Hierarchical Clustering Cut\n\nGiven a dendrogram, determine the optimal number of clusters by finding the largest gap."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nfrom scipy.cluster.hierarchy import linkage, fcluster\nimport numpy as np\n\ndef find_optimal_clusters_hierarchical(\n    X: np.ndarray,\n    method: str = 'ward'\n) -> int:\n    \"\"\"Find optimal clusters by largest gap in dendrogram.\n    \n    Args:\n        X: Feature matrix.\n        method: Linkage method.\n    \n    Returns:\n        Optimal number of clusters.\n    \"\"\"\n    Z = linkage(X, method=method)\n    \n    # Distances at each merge (column 2)\n    distances = Z[:, 2]\n    \n    # Find largest gap\n    gaps = np.diff(distances)\n    largest_gap_idx = np.argmax(gaps)\n    \n    # Number of clusters = n - largest_gap_idx - 1\n    n_clusters = len(X) - largest_gap_idx - 1\n    \n    # Cap at reasonable range\n    n_clusters = min(max(n_clusters, 2), 10)\n    \n    return n_clusters\n\n\n# Test\nfrom sklearn.datasets import make_blobs\n\nX, _ = make_blobs(n_samples=100, centers=4, random_state=42)\nn_clusters = find_optimal_clusters_hierarchical(X)\nprint(f\"Optimal clusters: {n_clusters}\")\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 11: Dimensionality Reduction for Clustering\n\nCreate a pipeline that applies PCA before K-Means and evaluates the result."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\n\ndef pca_kmeans_pipeline(\n    X: np.ndarray,\n    n_clusters: int,\n    n_components: int = None,\n    variance_threshold: float = 0.95\n) -> dict:\n    \"\"\"Apply PCA then K-Means clustering.\n    \n    Args:\n        X: Feature matrix.\n        n_clusters: Number of clusters.\n        n_components: PCA components (auto if None).\n        variance_threshold: Variance threshold if n_components is None.\n    \n    Returns:\n        Dictionary with labels, scores, and component info.\n    \"\"\"\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    if n_components is None:\n        pca_full = PCA()\n        pca_full.fit(X_scaled)\n        cumvar = np.cumsum(pca_full.explained_variance_ratio_)\n        n_components = np.argmax(cumvar >= variance_threshold) + 1\n    \n    pca = PCA(n_components=n_components)\n    X_pca = pca.fit_transform(X_scaled)\n    \n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n    labels = kmeans.fit_predict(X_pca)\n    \n    return {\n        'labels': labels,\n        'n_components': n_components,\n        'variance_explained': sum(pca.explained_variance_ratio_),\n        'silhouette_score': silhouette_score(X_pca, labels)\n    }\n\n\n# Test\nfrom sklearn.datasets import load_wine\nwine = load_wine()\nresults = pca_kmeans_pipeline(wine.data, n_clusters=3)\nprint(f\"Components used: {results['n_components']}\")\nprint(f\"Variance explained: {results['variance_explained']:.2%}\")\nprint(f\"Silhouette score: {results['silhouette_score']:.4f}\")\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 12: Cluster Profiling\n\nCreate a function that profiles clusters by computing statistics for each feature."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom typing import List\n\ndef profile_clusters(\n    X: np.ndarray,\n    labels: np.ndarray,\n    feature_names: List[str]\n) -> pd.DataFrame:\n    \"\"\"Profile clusters by computing feature statistics.\n    \n    Args:\n        X: Feature matrix.\n        labels: Cluster labels.\n        feature_names: Names of features.\n    \n    Returns:\n        DataFrame with cluster profiles.\n    \"\"\"\n    df = pd.DataFrame(X, columns=feature_names)\n    df['cluster'] = labels\n    \n    # Compute mean and std for each cluster\n    profile = df.groupby('cluster').agg(['mean', 'std'])\n    \n    # Add cluster sizes\n    sizes = df['cluster'].value_counts().sort_index()\n    \n    # Flatten column names\n    profile.columns = ['_'.join(col) for col in profile.columns]\n    profile['size'] = sizes.values\n    \n    return profile\n\n\n# Test\nfrom sklearn.datasets import load_iris\nfrom sklearn.cluster import KMeans\n\niris = load_iris()\nlabels = KMeans(n_clusters=3, random_state=42, n_init=10).fit_predict(iris.data)\nprofile = profile_clusters(iris.data, labels, iris.feature_names)\nprint(profile[['sepal length (cm)_mean', 'petal length (cm)_mean', 'size']])\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary\n",
    "\n",
    "This notebook covered essential unsupervised learning concepts:\n",
    "\n",
    "1. **Clustering Overview**: Understanding when to use different clustering algorithms\n",
    "2. **K-Means**: Partition-based clustering with elbow and silhouette methods\n",
    "3. **Hierarchical Clustering**: Dendrogram-based clustering with different linkages\n",
    "4. **DBSCAN**: Density-based clustering that handles noise and arbitrary shapes\n",
    "5. **Clustering Metrics**: Internal (silhouette, Calinski-Harabasz) and external (ARI, NMI)\n",
    "6. **PCA**: Linear dimensionality reduction preserving variance\n",
    "7. **t-SNE**: Non-linear visualisation preserving local structure\n",
    "8. **UMAP**: Fast non-linear reduction preserving global and local structure\n",
    "9. **Anomaly Detection**: Isolation Forest, LOF, and statistical methods\n",
    "\n",
    "---\n",
    "\n",
    "### Key Interview Tips\n",
    "\n",
    "- **Know the trade-offs**: K-Means is fast but needs K; DBSCAN finds K but needs eps\n",
    "- **Always scale data**: Especially important for distance-based methods\n",
    "- **Evaluation matters**: Use silhouette score when no labels; ARI when labels exist\n",
    "- **PCA for preprocessing**: Often improves clustering by removing noise\n",
    "- **t-SNE for visualisation only**: Don't use for feature extraction or new data\n",
    "- **Anomaly detection context**: Choose method based on data characteristics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}