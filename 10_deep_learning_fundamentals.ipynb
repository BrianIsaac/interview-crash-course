{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Fundamentals Crash Course for Data Science Assessments\n",
    "\n",
    "**Last Updated:** 25 January 2026\n",
    "\n",
    "This notebook covers deep learning concepts commonly tested in data science interviews. We focus on fundamental understanding of neural networks, architectures, and key concepts rather than complex implementations.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction and Setup](#1-introduction-and-setup)\n",
    "2. [Neural Network Basics](#2-neural-network-basics)\n",
    "3. [Activation Functions](#3-activation-functions)\n",
    "4. [Forward and Backward Propagation](#4-forward-and-backward-propagation)\n",
    "5. [Loss Functions](#5-loss-functions)\n",
    "6. [Optimisers](#6-optimisers)\n",
    "7. [Regularisation Techniques](#7-regularisation-techniques)\n",
    "8. [Convolutional Neural Networks (CNNs)](#8-convolutional-neural-networks-cnns)\n",
    "9. [Recurrent Neural Networks (RNNs)](#9-recurrent-neural-networks-rnns)\n",
    "10. [Transfer Learning](#10-transfer-learning)\n",
    "11. [Practical Considerations](#11-practical-considerations)\n",
    "12. [Practice Questions](#12-practice-questions)\n",
    "13. [Summary](#13-summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction and Setup\n",
    "\n",
    "**Deep learning** is a subset of machine learning that uses neural networks with multiple layers to learn hierarchical representations from data.\n",
    "\n",
    "**Why Deep Learning?**\n",
    "- Automatic feature learning\n",
    "- Excellent for unstructured data (images, text, audio)\n",
    "- State-of-the-art performance on many tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "from sklearn.datasets import make_classification, make_moons\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Neural Network Basics\n",
    "\n",
    "A **neural network** consists of:\n",
    "- **Input layer**: Receives the features\n",
    "- **Hidden layers**: Learn representations\n",
    "- **Output layer**: Produces predictions\n",
    "\n",
    "### Perceptron (Single Neuron)\n",
    "\n",
    "$$y = f\\left(\\sum_{i=1}^{n} w_i x_i + b\\right) = f(\\mathbf{w}^T\\mathbf{x} + b)$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{x}$: Input features\n",
    "- $\\mathbf{w}$: Weights\n",
    "- $b$: Bias\n",
    "- $f$: Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"Simple perceptron implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_features: int, learning_rate: float = 0.01):\n",
    "        \"\"\"Initialise perceptron.\n",
    "        \n",
    "        Args:\n",
    "            n_features: Number of input features.\n",
    "            learning_rate: Learning rate for weight updates.\n",
    "        \"\"\"\n",
    "        self.weights = np.random.randn(n_features) * 0.01\n",
    "        self.bias = 0.0\n",
    "        self.lr = learning_rate\n",
    "    \n",
    "    def _sigmoid(self, z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Sigmoid activation function.\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return self._sigmoid(z)\n",
    "    \n",
    "    def train(self, X: np.ndarray, y: np.ndarray, epochs: int = 100) -> List[float]:\n",
    "        \"\"\"Train the perceptron.\n",
    "        \n",
    "        Args:\n",
    "            X: Training features.\n",
    "            y: Training labels.\n",
    "            epochs: Number of training epochs.\n",
    "        \n",
    "        Returns:\n",
    "            List of loss values per epoch.\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        for _ in range(epochs):\n",
    "            y_pred = self.forward(X)\n",
    "            \n",
    "            error = y_pred - y\n",
    "            loss = np.mean(error ** 2)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            d_weights = np.dot(X.T, error * y_pred * (1 - y_pred)) / len(y)\n",
    "            d_bias = np.mean(error * y_pred * (1 - y_pred))\n",
    "            \n",
    "            self.weights -= self.lr * d_weights\n",
    "            self.bias -= self.lr * d_bias\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        return (self.forward(X) >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=200, n_features=2, n_informative=2,\n",
    "                           n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "perceptron = Perceptron(n_features=2, learning_rate=1.0)\n",
    "losses = perceptron.train(X, y, epochs=100)\n",
    "\n",
    "predictions = perceptron.predict(X)\n",
    "accuracy = accuracy_score(y, predictions)\n",
    "\n",
    "print(f\"Final accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(losses)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "\n",
    "axes[1].scatter(X[:, 0], X[:, 1], c=predictions, cmap='viridis', alpha=0.7)\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "axes[1].set_title('Perceptron Decision')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Activation Functions\n",
    "\n",
    "**Activation functions** introduce non-linearity, allowing networks to learn complex patterns.\n",
    "\n",
    "| Function | Formula | Range | Use Case | Issues |\n",
    "|----------|---------|-------|----------|--------|\n",
    "| Sigmoid | $\\frac{1}{1+e^{-x}}$ | (0, 1) | Binary output | Vanishing gradients |\n",
    "| Tanh | $\\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | (-1, 1) | Hidden layers | Vanishing gradients |\n",
    "| ReLU | $\\max(0, x)$ | [0, inf) | Hidden layers | Dead neurons |\n",
    "| Leaky ReLU | $\\max(0.01x, x)$ | (-inf, inf) | Hidden layers | - |\n",
    "| Softmax | $\\frac{e^{x_i}}{\\sum e^{x_j}}$ | (0, 1), sum=1 | Multi-class output | - |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Derivative of sigmoid.\"\"\"\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "\n",
    "def tanh(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Hyperbolic tangent activation.\"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def tanh_derivative(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Derivative of tanh.\"\"\"\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "\n",
    "def relu(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Rectified Linear Unit.\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def relu_derivative(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Derivative of ReLU.\"\"\"\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "\n",
    "def leaky_relu(x: np.ndarray, alpha: float = 0.01) -> np.ndarray:\n",
    "    \"\"\"Leaky ReLU activation.\"\"\"\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "\n",
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Softmax activation for multi-class.\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "activations = [\n",
    "    ('Sigmoid', sigmoid(x)),\n",
    "    ('Tanh', tanh(x)),\n",
    "    ('ReLU', relu(x)),\n",
    "    ('Leaky ReLU', leaky_relu(x)),\n",
    "    ('Sigmoid Derivative', sigmoid_derivative(x)),\n",
    "    ('ReLU Derivative', relu_derivative(x))\n",
    "]\n",
    "\n",
    "for ax, (name, values) in zip(axes.flat, activations):\n",
    "    ax.plot(x, values, linewidth=2)\n",
    "    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('f(x)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing Gradient Problem\n",
    "\n",
    "**Problem**: Sigmoid/tanh gradients are small for large |x|, causing slow learning in deep networks.\n",
    "\n",
    "**Solution**: Use ReLU or its variants for hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gradient magnitudes at different x values:\")\n",
    "print(f\"{'x':>6} {'Sigmoid':>12} {'ReLU':>12}\")\n",
    "print(\"-\" * 32)\n",
    "for x_val in [-5, -2, 0, 2, 5]:\n",
    "    sig_grad = sigmoid_derivative(x_val)\n",
    "    relu_grad = relu_derivative(x_val)\n",
    "    print(f\"{x_val:>6} {sig_grad:>12.6f} {relu_grad:>12.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Forward and Backward Propagation\n",
    "\n",
    "### Forward Propagation\n",
    "Pass input through network to get predictions:\n",
    "$$\\mathbf{a}^{[l]} = f(\\mathbf{W}^{[l]} \\mathbf{a}^{[l-1]} + \\mathbf{b}^{[l]})$$\n",
    "\n",
    "### Backward Propagation (Backprop)\n",
    "Compute gradients using chain rule to update weights:\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{W}^{[l]}} = \\frac{\\partial L}{\\partial \\mathbf{a}^{[l]}} \\cdot \\frac{\\partial \\mathbf{a}^{[l]}}{\\partial \\mathbf{z}^{[l]}} \\cdot \\frac{\\partial \\mathbf{z}^{[l]}}{\\partial \\mathbf{W}^{[l]}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    \"\"\"Simple 2-layer neural network.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        output_size: int,\n",
    "        learning_rate: float = 0.01\n",
    "    ):\n",
    "        \"\"\"Initialise neural network.\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of input features.\n",
    "            hidden_size: Number of hidden neurons.\n",
    "            output_size: Number of output neurons.\n",
    "            learning_rate: Learning rate.\n",
    "        \"\"\"\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward propagation.\n",
    "        \n",
    "        Args:\n",
    "            X: Input data.\n",
    "        \n",
    "        Returns:\n",
    "            Network output.\n",
    "        \"\"\"\n",
    "        self.cache['X'] = X\n",
    "        \n",
    "        self.cache['Z1'] = np.dot(X, self.W1) + self.b1\n",
    "        self.cache['A1'] = relu(self.cache['Z1'])\n",
    "        \n",
    "        self.cache['Z2'] = np.dot(self.cache['A1'], self.W2) + self.b2\n",
    "        self.cache['A2'] = sigmoid(self.cache['Z2'])\n",
    "        \n",
    "        return self.cache['A2']\n",
    "    \n",
    "    def backward(self, y: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Backward propagation.\n",
    "        \n",
    "        Args:\n",
    "            y: True labels.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of gradients.\n",
    "        \"\"\"\n",
    "        m = y.shape[0]\n",
    "        y = y.reshape(-1, 1)\n",
    "        \n",
    "        dZ2 = self.cache['A2'] - y\n",
    "        dW2 = np.dot(self.cache['A1'].T, dZ2) / m\n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        dA1 = np.dot(dZ2, self.W2.T)\n",
    "        dZ1 = dA1 * relu_derivative(self.cache['Z1'])\n",
    "        dW1 = np.dot(self.cache['X'].T, dZ1) / m\n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        return {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2}\n",
    "    \n",
    "    def update_weights(self, gradients: Dict[str, np.ndarray]) -> None:\n",
    "        \"\"\"Update weights using gradients.\"\"\"\n",
    "        self.W1 -= self.lr * gradients['dW1']\n",
    "        self.b1 -= self.lr * gradients['db1']\n",
    "        self.W2 -= self.lr * gradients['dW2']\n",
    "        self.b2 -= self.lr * gradients['db2']\n",
    "    \n",
    "    def train(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        epochs: int = 1000\n",
    "    ) -> List[float]:\n",
    "        \"\"\"Train the network.\n",
    "        \n",
    "        Args:\n",
    "            X: Training features.\n",
    "            y: Training labels.\n",
    "            epochs: Number of epochs.\n",
    "        \n",
    "        Returns:\n",
    "            List of losses per epoch.\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        for _ in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            \n",
    "            y_reshaped = y.reshape(-1, 1)\n",
    "            loss = -np.mean(y_reshaped * np.log(output + 1e-8) + \n",
    "                           (1 - y_reshaped) * np.log(1 - output + 1e-8))\n",
    "            losses.append(loss)\n",
    "            \n",
    "            gradients = self.backward(y)\n",
    "            self.update_weights(gradients)\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        return (self.forward(X) >= 0.5).astype(int).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_moons, y_moons = make_moons(n_samples=500, noise=0.1, random_state=42)\n",
    "\n",
    "nn = SimpleNeuralNetwork(input_size=2, hidden_size=16, output_size=1, learning_rate=1.0)\n",
    "losses = nn.train(X_moons, y_moons, epochs=1000)\n",
    "\n",
    "predictions = nn.predict(X_moons)\n",
    "accuracy = accuracy_score(y_moons, predictions)\n",
    "\n",
    "print(f\"Final accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(losses)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "\n",
    "axes[1].scatter(X_moons[:, 0], X_moons[:, 1], c=predictions, cmap='viridis', alpha=0.7)\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "axes[1].set_title('Neural Network Decision (Non-linear Boundary)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Loss Functions\n",
    "\n",
    "**Loss functions** measure how wrong predictions are.\n",
    "\n",
    "| Loss | Formula | Use Case |\n",
    "|------|---------|----------|\n",
    "| MSE | $\\frac{1}{n}\\sum(y - \\hat{y})^2$ | Regression |\n",
    "| Binary Cross-Entropy | $-\\frac{1}{n}\\sum[y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y})]$ | Binary classification |\n",
    "| Categorical Cross-Entropy | $-\\sum y_i \\log(\\hat{y}_i)$ | Multi-class classification |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Mean Squared Error loss.\"\"\"\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "\n",
    "def binary_cross_entropy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Binary Cross-Entropy loss.\"\"\"\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "\n",
    "def categorical_cross_entropy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Categorical Cross-Entropy loss.\"\"\"\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.sum(y_true * np.log(y_pred)) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([1, 0, 1, 1])\n",
    "y_pred_good = np.array([0.9, 0.1, 0.8, 0.95])\n",
    "y_pred_bad = np.array([0.2, 0.8, 0.3, 0.4])\n",
    "\n",
    "print(\"Loss Comparison:\")\n",
    "print(f\"{'Predictions':<15} {'BCE':>10} {'MSE':>10}\")\n",
    "print(\"-\" * 37)\n",
    "print(f\"{'Good':.<15} {binary_cross_entropy(y_true, y_pred_good):>10.4f} {mse_loss(y_true, y_pred_good):>10.4f}\")\n",
    "print(f\"{'Bad':.<15} {binary_cross_entropy(y_true, y_pred_bad):>10.4f} {mse_loss(y_true, y_pred_bad):>10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Optimisers\n",
    "\n",
    "**Optimisers** update weights to minimise the loss function.\n",
    "\n",
    "| Optimiser | Update Rule | Key Feature |\n",
    "|-----------|-------------|-------------|\n",
    "| SGD | $w = w - \\eta \\nabla L$ | Simple, may oscillate |\n",
    "| Momentum | $v = \\beta v + \\nabla L$; $w = w - \\eta v$ | Smooths updates |\n",
    "| RMSprop | Adapts learning rate per parameter | Good for RNNs |\n",
    "| Adam | Combines Momentum + RMSprop | Default choice |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"Stochastic Gradient Descent optimiser.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 0.01):\n",
    "        self.lr = learning_rate\n",
    "    \n",
    "    def update(self, param: np.ndarray, grad: np.ndarray) -> np.ndarray:\n",
    "        return param - self.lr * grad\n",
    "\n",
    "\n",
    "class SGDMomentum:\n",
    "    \"\"\"SGD with Momentum optimiser.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 0.01, momentum: float = 0.9):\n",
    "        self.lr = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.velocity = None\n",
    "    \n",
    "    def update(self, param: np.ndarray, grad: np.ndarray) -> np.ndarray:\n",
    "        if self.velocity is None:\n",
    "            self.velocity = np.zeros_like(param)\n",
    "        \n",
    "        self.velocity = self.momentum * self.velocity + grad\n",
    "        return param - self.lr * self.velocity\n",
    "\n",
    "\n",
    "class Adam:\n",
    "    \"\"\"Adam optimiser.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float = 0.001,\n",
    "        beta1: float = 0.9,\n",
    "        beta2: float = 0.999,\n",
    "        epsilon: float = 1e-8\n",
    "    ):\n",
    "        self.lr = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "    \n",
    "    def update(self, param: np.ndarray, grad: np.ndarray) -> np.ndarray:\n",
    "        if self.m is None:\n",
    "            self.m = np.zeros_like(param)\n",
    "            self.v = np.zeros_like(param)\n",
    "        \n",
    "        self.t += 1\n",
    "        \n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * grad\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (grad ** 2)\n",
    "        \n",
    "        m_hat = self.m / (1 - self.beta1 ** self.t)\n",
    "        v_hat = self.v / (1 - self.beta2 ** self.t)\n",
    "        \n",
    "        return param - self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Regularisation Techniques\n",
    "\n",
    "**Regularisation** prevents overfitting.\n",
    "\n",
    "| Technique | Description | Implementation |\n",
    "|-----------|-------------|----------------|\n",
    "| L1 (Lasso) | Adds $\\lambda\\sum|w|$ to loss | Sparse weights |\n",
    "| L2 (Ridge) | Adds $\\lambda\\sum w^2$ to loss | Small weights |\n",
    "| Dropout | Randomly set neurons to 0 during training | Ensemble effect |\n",
    "| Batch Normalisation | Normalise layer inputs | Faster training |\n",
    "| Early Stopping | Stop when validation loss increases | Simple, effective |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(x: np.ndarray, rate: float = 0.5, training: bool = True) -> np.ndarray:\n",
    "    \"\"\"Apply dropout regularisation.\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor.\n",
    "        rate: Dropout rate (fraction to drop).\n",
    "        training: Whether in training mode.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor with dropout applied.\n",
    "    \"\"\"\n",
    "    if not training:\n",
    "        return x\n",
    "    \n",
    "    mask = np.random.binomial(1, 1 - rate, size=x.shape) / (1 - rate)\n",
    "    return x * mask\n",
    "\n",
    "\n",
    "def batch_normalisation(\n",
    "    x: np.ndarray,\n",
    "    gamma: float = 1.0,\n",
    "    beta: float = 0.0,\n",
    "    epsilon: float = 1e-8\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Apply batch normalisation.\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor.\n",
    "        gamma: Scale parameter.\n",
    "        beta: Shift parameter.\n",
    "        epsilon: Small constant for numerical stability.\n",
    "    \n",
    "    Returns:\n",
    "        Normalised tensor.\n",
    "    \"\"\"\n",
    "    mean = np.mean(x, axis=0)\n",
    "    var = np.var(x, axis=0)\n",
    "    x_norm = (x - mean) / np.sqrt(var + epsilon)\n",
    "    return gamma * x_norm + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(5, 4)\n",
    "print(\"Original:\")\n",
    "print(x.round(2))\n",
    "\n",
    "print(\"\\nWith Dropout (rate=0.5):\")\n",
    "print(dropout(x, rate=0.5).round(2))\n",
    "\n",
    "print(\"\\nBatch Normalised:\")\n",
    "print(batch_normalisation(x).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Convolutional Neural Networks (CNNs)\n",
    "\n",
    "**CNNs** are specialised for grid-like data (images).\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "| Layer | Purpose | Parameters |\n",
    "|-------|---------|------------|\n",
    "| Convolution | Extract local features | Filters, kernel size, stride, padding |\n",
    "| Pooling | Reduce spatial dimensions | Pool size, stride |\n",
    "| Flatten | Convert to 1D | None |\n",
    "| Dense | Final classification | Neurons |\n",
    "\n",
    "### Convolution Operation\n",
    "Slides a kernel (filter) over the input, computing dot products.\n",
    "\n",
    "**Output size**: $\\frac{n + 2p - f}{s} + 1$\n",
    "- $n$: input size\n",
    "- $p$: padding\n",
    "- $f$: filter size\n",
    "- $s$: stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(\n",
    "    image: np.ndarray,\n",
    "    kernel: np.ndarray,\n",
    "    stride: int = 1,\n",
    "    padding: int = 0\n",
    ") -> np.ndarray:\n",
    "    \"\"\"2D convolution operation.\n",
    "    \n",
    "    Args:\n",
    "        image: Input image (H x W).\n",
    "        kernel: Convolution kernel (kH x kW).\n",
    "        stride: Step size.\n",
    "        padding: Zero padding.\n",
    "    \n",
    "    Returns:\n",
    "        Convolved output.\n",
    "    \"\"\"\n",
    "    if padding > 0:\n",
    "        image = np.pad(image, padding, mode='constant', constant_values=0)\n",
    "    \n",
    "    h, w = image.shape\n",
    "    kh, kw = kernel.shape\n",
    "    \n",
    "    out_h = (h - kh) // stride + 1\n",
    "    out_w = (w - kw) // stride + 1\n",
    "    \n",
    "    output = np.zeros((out_h, out_w))\n",
    "    \n",
    "    for i in range(out_h):\n",
    "        for j in range(out_w):\n",
    "            region = image[i*stride:i*stride+kh, j*stride:j*stride+kw]\n",
    "            output[i, j] = np.sum(region * kernel)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def max_pool2d(\n",
    "    image: np.ndarray,\n",
    "    pool_size: int = 2,\n",
    "    stride: int = 2\n",
    ") -> np.ndarray:\n",
    "    \"\"\"2D max pooling operation.\n",
    "    \n",
    "    Args:\n",
    "        image: Input image.\n",
    "        pool_size: Size of pooling window.\n",
    "        stride: Step size.\n",
    "    \n",
    "    Returns:\n",
    "        Pooled output.\n",
    "    \"\"\"\n",
    "    h, w = image.shape\n",
    "    out_h = (h - pool_size) // stride + 1\n",
    "    out_w = (w - pool_size) // stride + 1\n",
    "    \n",
    "    output = np.zeros((out_h, out_w))\n",
    "    \n",
    "    for i in range(out_h):\n",
    "        for j in range(out_w):\n",
    "            region = image[i*stride:i*stride+pool_size, j*stride:j*stride+pool_size]\n",
    "            output[i, j] = np.max(region)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.random.rand(8, 8)\n",
    "\n",
    "edge_kernel = np.array([[-1, -1, -1],\n",
    "                        [-1,  8, -1],\n",
    "                        [-1, -1, -1]])\n",
    "\n",
    "convolved = conv2d(image, edge_kernel, stride=1, padding=1)\n",
    "pooled = max_pool2d(convolved, pool_size=2, stride=2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "axes[0].imshow(image, cmap='gray')\n",
    "axes[0].set_title(f'Original ({image.shape})')\n",
    "\n",
    "axes[1].imshow(convolved, cmap='gray')\n",
    "axes[1].set_title(f'After Convolution ({convolved.shape})')\n",
    "\n",
    "axes[2].imshow(pooled, cmap='gray')\n",
    "axes[2].set_title(f'After Max Pool ({pooled.shape})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common CNN Architectures\n",
    "\n",
    "| Architecture | Year | Key Innovation |\n",
    "|--------------|------|----------------|\n",
    "| LeNet | 1998 | First successful CNN |\n",
    "| AlexNet | 2012 | ReLU, Dropout, GPU training |\n",
    "| VGG | 2014 | Small filters (3x3), deeper |\n",
    "| ResNet | 2015 | Skip connections (residual) |\n",
    "| EfficientNet | 2019 | Compound scaling |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Recurrent Neural Networks (RNNs)\n",
    "\n",
    "**RNNs** are designed for sequential data (text, time series).\n",
    "\n",
    "### Basic RNN\n",
    "$$h_t = \\tanh(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$$\n",
    "$$y_t = W_{hy}h_t + b_y$$\n",
    "\n",
    "**Problem**: Vanishing/exploding gradients for long sequences.\n",
    "\n",
    "### LSTM (Long Short-Term Memory)\n",
    "Uses **gates** to control information flow:\n",
    "- **Forget gate**: What to discard from cell state\n",
    "- **Input gate**: What new information to store\n",
    "- **Output gate**: What to output\n",
    "\n",
    "### GRU (Gated Recurrent Unit)\n",
    "Simplified LSTM with:\n",
    "- **Reset gate**: Controls how much past to forget\n",
    "- **Update gate**: Controls how much to update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    \"\"\"Simple RNN cell implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        \"\"\"Initialise RNN cell.\n",
    "        \n",
    "        Args:\n",
    "            input_size: Input dimension.\n",
    "            hidden_size: Hidden state dimension.\n",
    "        \"\"\"\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.Wxh = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.bh = np.zeros((1, hidden_size))\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        h_prev: np.ndarray = None\n",
    "    ) -> Tuple[np.ndarray, List[np.ndarray]]:\n",
    "        \"\"\"Forward pass through sequence.\n",
    "        \n",
    "        Args:\n",
    "            x: Input sequence (seq_len, input_size).\n",
    "            h_prev: Initial hidden state.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (final hidden state, all hidden states).\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[0]\n",
    "        \n",
    "        if h_prev is None:\n",
    "            h_prev = np.zeros((1, self.hidden_size))\n",
    "        \n",
    "        hidden_states = []\n",
    "        h = h_prev\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            x_t = x[t:t+1]\n",
    "            h = np.tanh(np.dot(x_t, self.Wxh) + np.dot(h, self.Whh) + self.bh)\n",
    "            hidden_states.append(h)\n",
    "        \n",
    "        return h, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = SimpleRNN(input_size=10, hidden_size=20)\n",
    "sequence = np.random.randn(5, 10)\n",
    "\n",
    "final_h, all_h = rnn.forward(sequence)\n",
    "\n",
    "print(f\"Input sequence shape: {sequence.shape}\")\n",
    "print(f\"Final hidden state shape: {final_h.shape}\")\n",
    "print(f\"Number of hidden states: {len(all_h)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell:\n",
    "    \"\"\"LSTM cell implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        \"\"\"Initialise LSTM cell.\n",
    "        \n",
    "        Args:\n",
    "            input_size: Input dimension.\n",
    "            hidden_size: Hidden state dimension.\n",
    "        \"\"\"\n",
    "        self.hidden_size = hidden_size\n",
    "        scale = np.sqrt(2.0 / (input_size + hidden_size))\n",
    "        \n",
    "        # Forget gate\n",
    "        self.Wf = np.random.randn(input_size + hidden_size, hidden_size) * scale\n",
    "        self.bf = np.zeros((1, hidden_size))\n",
    "        \n",
    "        # Input gate\n",
    "        self.Wi = np.random.randn(input_size + hidden_size, hidden_size) * scale\n",
    "        self.bi = np.zeros((1, hidden_size))\n",
    "        \n",
    "        # Cell state\n",
    "        self.Wc = np.random.randn(input_size + hidden_size, hidden_size) * scale\n",
    "        self.bc = np.zeros((1, hidden_size))\n",
    "        \n",
    "        # Output gate\n",
    "        self.Wo = np.random.randn(input_size + hidden_size, hidden_size) * scale\n",
    "        self.bo = np.zeros((1, hidden_size))\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x_t: np.ndarray,\n",
    "        h_prev: np.ndarray,\n",
    "        c_prev: np.ndarray\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Forward pass for single timestep.\n",
    "        \n",
    "        Args:\n",
    "            x_t: Input at time t.\n",
    "            h_prev: Previous hidden state.\n",
    "            c_prev: Previous cell state.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (new hidden state, new cell state).\n",
    "        \"\"\"\n",
    "        combined = np.concatenate([x_t, h_prev], axis=1)\n",
    "        \n",
    "        f_t = sigmoid(np.dot(combined, self.Wf) + self.bf)\n",
    "        i_t = sigmoid(np.dot(combined, self.Wi) + self.bi)\n",
    "        c_tilde = np.tanh(np.dot(combined, self.Wc) + self.bc)\n",
    "        o_t = sigmoid(np.dot(combined, self.Wo) + self.bo)\n",
    "        \n",
    "        c_t = f_t * c_prev + i_t * c_tilde\n",
    "        h_t = o_t * np.tanh(c_t)\n",
    "        \n",
    "        return h_t, c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTMCell(input_size=10, hidden_size=20)\n",
    "\n",
    "x_t = np.random.randn(1, 10)\n",
    "h_prev = np.zeros((1, 20))\n",
    "c_prev = np.zeros((1, 20))\n",
    "\n",
    "h_t, c_t = lstm.forward(x_t, h_prev, c_prev)\n",
    "\n",
    "print(f\"Input shape: {x_t.shape}\")\n",
    "print(f\"Hidden state shape: {h_t.shape}\")\n",
    "print(f\"Cell state shape: {c_t.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Transfer Learning\n",
    "\n",
    "**Transfer learning** uses a model trained on one task as starting point for another task.\n",
    "\n",
    "### Approaches:\n",
    "\n",
    "| Strategy | When to Use | Implementation |\n",
    "|----------|-------------|----------------|\n",
    "| Feature Extraction | Small dataset, similar domain | Freeze all layers, train only classifier |\n",
    "| Fine-tuning | Medium dataset | Unfreeze some layers, train with low LR |\n",
    "| Full fine-tuning | Large dataset | Unfreeze all, use pretrained weights as init |\n",
    "\n",
    "### Common Pretrained Models:\n",
    "- **Images**: ResNet, VGG, EfficientNet (trained on ImageNet)\n",
    "- **Text**: BERT, GPT (trained on large text corpora)\n",
    "- **Audio**: Wav2Vec (trained on speech data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transfer Learning Decision Guide:\\n\")\n",
    "print(\"Dataset Size vs Domain Similarity Matrix:\")\n",
    "print()\n",
    "print(f\"{'':20} | {'Similar Domain':20} | {'Different Domain':20}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Small Dataset':20} | {'Feature Extraction':20} | {'Feature Extraction*':20}\")\n",
    "print(f\"{'Medium Dataset':20} | {'Fine-tune top layers':20} | {'Fine-tune more layers':20}\")\n",
    "print(f\"{'Large Dataset':20} | {'Full fine-tuning':20} | {'Train from scratch':20}\")\n",
    "print()\n",
    "print(\"* May not work well if domains are very different\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Practical Considerations\n",
    "\n",
    "### Hyperparameters to Tune\n",
    "\n",
    "| Hyperparameter | Typical Range | Effect |\n",
    "|----------------|---------------|--------|\n",
    "| Learning rate | 1e-5 to 1e-1 | Too high: diverge, too low: slow |\n",
    "| Batch size | 16 to 512 | Larger: faster, may generalise worse |\n",
    "| Hidden layers | 1 to 10+ | More: more capacity, overfitting risk |\n",
    "| Neurons per layer | 32 to 1024 | More: more capacity |\n",
    "| Dropout rate | 0.1 to 0.5 | Higher: more regularisation |\n",
    "\n",
    "### Training Tips\n",
    "1. Start with proven architectures\n",
    "2. Use batch normalisation\n",
    "3. Start with Adam optimiser\n",
    "4. Use learning rate scheduling\n",
    "5. Monitor for overfitting with validation set\n",
    "6. Use early stopping\n",
    "7. Data augmentation for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_schedule(\n",
    "    initial_lr: float,\n",
    "    epoch: int,\n",
    "    schedule_type: str = 'step'\n",
    ") -> float:\n",
    "    \"\"\"Compute learning rate based on schedule.\n",
    "    \n",
    "    Args:\n",
    "        initial_lr: Initial learning rate.\n",
    "        epoch: Current epoch.\n",
    "        schedule_type: Type of schedule ('step', 'exponential', 'cosine').\n",
    "    \n",
    "    Returns:\n",
    "        Adjusted learning rate.\n",
    "    \"\"\"\n",
    "    if schedule_type == 'step':\n",
    "        return initial_lr * (0.1 ** (epoch // 30))\n",
    "    \n",
    "    elif schedule_type == 'exponential':\n",
    "        return initial_lr * (0.95 ** epoch)\n",
    "    \n",
    "    elif schedule_type == 'cosine':\n",
    "        return initial_lr * (1 + np.cos(np.pi * epoch / 100)) / 2\n",
    "    \n",
    "    return initial_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(100)\n",
    "initial_lr = 0.1\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for schedule in ['step', 'exponential', 'cosine']:\n",
    "    lrs = [learning_rate_schedule(initial_lr, e, schedule) for e in epochs]\n",
    "    plt.plot(epochs, lrs, label=schedule.capitalize())\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedules')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 12. Practice Questions\n\nTest your understanding with these interview-style questions. Try to solve each question in the empty code cell before revealing the answer."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 1: Implement ReLU and Its Derivative\n\nImplement ReLU activation and its derivative from scratch."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nimport numpy as np\n\ndef relu(x: np.ndarray) -> np.ndarray:\n    \"\"\"ReLU activation function.\n    \n    Args:\n        x: Input array.\n    \n    Returns:\n        ReLU applied element-wise.\n    \"\"\"\n    return np.maximum(0, x)\n\n\ndef relu_derivative(x: np.ndarray) -> np.ndarray:\n    \"\"\"Derivative of ReLU.\n    \n    Args:\n        x: Input array.\n    \n    Returns:\n        Derivative (1 where x > 0, else 0).\n    \"\"\"\n    return (x > 0).astype(float)\n\n\n# Test\nx = np.array([-2, -1, 0, 1, 2])\nprint(f\"Input: {x}\")\nprint(f\"ReLU: {relu(x)}\")\nprint(f\"Derivative: {relu_derivative(x)}\")\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 2: Softmax Implementation\n\nImplement numerically stable softmax."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nimport numpy as np\n\ndef softmax(x: np.ndarray) -> np.ndarray:\n    \"\"\"Numerically stable softmax.\n    \n    Args:\n        x: Input array.\n    \n    Returns:\n        Softmax probabilities.\n    \"\"\"\n    # Subtract max for numerical stability\n    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n\n\n# Test\nx = np.array([1.0, 2.0, 3.0])\nprobs = softmax(x)\nprint(f\"Input: {x}\")\nprint(f\"Softmax: {probs}\")\nprint(f\"Sum: {np.sum(probs):.4f}\")\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 3: Cross-Entropy Loss\n\nImplement binary cross-entropy loss."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nimport numpy as np\n\ndef binary_cross_entropy(\n    y_true: np.ndarray,\n    y_pred: np.ndarray,\n    epsilon: float = 1e-15\n) -> float:\n    \"\"\"Binary cross-entropy loss.\n    \n    Args:\n        y_true: True labels (0 or 1).\n        y_pred: Predicted probabilities.\n        epsilon: Small value to prevent log(0).\n    \n    Returns:\n        Mean binary cross-entropy.\n    \"\"\"\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n    return -np.mean(\n        y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)\n    )\n\n\n# Test\ny_true = np.array([1, 0, 1, 1])\ny_pred = np.array([0.9, 0.1, 0.8, 0.7])\nloss = binary_cross_entropy(y_true, y_pred)\nprint(f\"BCE Loss: {loss:.4f}\")\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 4: Xavier Weight Initialisation\n\nImplement Xavier (Glorot) weight initialisation."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nimport numpy as np\n\ndef xavier_init(\n    fan_in: int,\n    fan_out: int,\n    uniform: bool = True\n) -> np.ndarray:\n    \"\"\"Xavier (Glorot) weight initialisation.\n    \n    Args:\n        fan_in: Number of input units.\n        fan_out: Number of output units.\n        uniform: Use uniform (True) or normal (False).\n    \n    Returns:\n        Initialised weight matrix.\n    \"\"\"\n    if uniform:\n        limit = np.sqrt(6 / (fan_in + fan_out))\n        return np.random.uniform(-limit, limit, (fan_in, fan_out))\n    else:\n        std = np.sqrt(2 / (fan_in + fan_out))\n        return np.random.randn(fan_in, fan_out) * std\n\n\n# Test\nweights = xavier_init(100, 50)\nprint(f\"Shape: {weights.shape}\")\nprint(f\"Mean: {weights.mean():.4f}\")\nprint(f\"Std: {weights.std():.4f}\")\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 5: Dropout Implementation\n\nImplement dropout with inverted scaling."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nimport numpy as np\n\ndef dropout(\n    x: np.ndarray,\n    rate: float = 0.5,\n    training: bool = True\n) -> np.ndarray:\n    \"\"\"Apply inverted dropout.\n    \n    Args:\n        x: Input tensor.\n        rate: Fraction of units to drop.\n        training: Whether in training mode.\n    \n    Returns:\n        Tensor with dropout applied.\n    \"\"\"\n    if not training or rate == 0:\n        return x\n    \n    # Create mask and scale by keep probability\n    keep_prob = 1 - rate\n    mask = np.random.binomial(1, keep_prob, size=x.shape) / keep_prob\n    \n    return x * mask\n\n\n# Test\nnp.random.seed(42)\nx = np.ones((3, 4))\nprint(f\"Original:\\n{x}\")\nprint(f\"\\nWith dropout (rate=0.5):\\n{dropout(x, rate=0.5)}\")\nprint(f\"\\nInference (no dropout):\\n{dropout(x, rate=0.5, training=False)}\")\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 6: Batch Normalisation\n\nImplement batch normalisation for training."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nimport numpy as np\n\ndef batch_norm(\n    x: np.ndarray,\n    gamma: np.ndarray = None,\n    beta: np.ndarray = None,\n    epsilon: float = 1e-5\n) -> tuple:\n    \"\"\"Batch normalisation.\n    \n    Args:\n        x: Input (batch_size, features).\n        gamma: Scale parameter.\n        beta: Shift parameter.\n        epsilon: Numerical stability.\n    \n    Returns:\n        Tuple of (normalised output, mean, variance).\n    \"\"\"\n    mean = np.mean(x, axis=0, keepdims=True)\n    var = np.var(x, axis=0, keepdims=True)\n    \n    x_norm = (x - mean) / np.sqrt(var + epsilon)\n    \n    if gamma is None:\n        gamma = np.ones_like(mean)\n    if beta is None:\n        beta = np.zeros_like(mean)\n    \n    out = gamma * x_norm + beta\n    \n    return out, mean, var\n\n\n# Test\nx = np.random.randn(32, 10) * 5 + 3  # Mean ~3, std ~5\nx_norm, mean, var = batch_norm(x)\nprint(f\"Original - Mean: {x.mean():.2f}, Std: {x.std():.2f}\")\nprint(f\"Normalised - Mean: {x_norm.mean():.2f}, Std: {x_norm.std():.2f}\")\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 7: Convolution Output Size\n\nCalculate the output size of a convolutional layer."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\ndef conv_output_size(\n    input_size: int,\n    kernel_size: int,\n    stride: int = 1,\n    padding: int = 0\n) -> int:\n    \"\"\"Calculate convolution output size.\n    \n    Args:\n        input_size: Input dimension.\n        kernel_size: Filter size.\n        stride: Step size.\n        padding: Zero padding.\n    \n    Returns:\n        Output dimension.\n    \"\"\"\n    return (input_size + 2 * padding - kernel_size) // stride + 1\n\n\n# Test\nprint(\"Conv output sizes:\")\nprint(f\"  32x32 input, 3x3 kernel, stride 1, no padding: {conv_output_size(32, 3)}\")\nprint(f\"  32x32 input, 3x3 kernel, stride 1, padding 1: {conv_output_size(32, 3, padding=1)}\")\nprint(f\"  32x32 input, 5x5 kernel, stride 2, padding 2: {conv_output_size(32, 5, stride=2, padding=2)}\")\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 8: Gradient Descent Step\n\nImplement a single gradient descent update step."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nimport numpy as np\nfrom typing import Dict\n\ndef gradient_descent_step(\n    parameters: Dict[str, np.ndarray],\n    gradients: Dict[str, np.ndarray],\n    learning_rate: float\n) -> Dict[str, np.ndarray]:\n    \"\"\"Perform one gradient descent step.\n    \n    Args:\n        parameters: Dictionary of parameters.\n        gradients: Dictionary of gradients.\n        learning_rate: Step size.\n    \n    Returns:\n        Updated parameters.\n    \"\"\"\n    updated = {}\n    for key in parameters:\n        updated[key] = parameters[key] - learning_rate * gradients['d' + key]\n    return updated\n\n\n# Test\nparams = {'W': np.array([1.0, 2.0]), 'b': np.array([0.5])}\ngrads = {'dW': np.array([0.1, 0.2]), 'db': np.array([0.05])}\n\nnew_params = gradient_descent_step(params, grads, learning_rate=0.1)\nprint(f\"Old W: {params['W']}\")\nprint(f\"New W: {new_params['W']}\")\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 9: LSTM Gates\n\nExplain and implement the forget gate of an LSTM."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nimport numpy as np\n\ndef forget_gate(\n    x_t: np.ndarray,\n    h_prev: np.ndarray,\n    Wf: np.ndarray,\n    bf: np.ndarray\n) -> np.ndarray:\n    \"\"\"LSTM forget gate computation.\n    \n    The forget gate decides what information to discard from the cell state.\n    Output is between 0 (forget everything) and 1 (keep everything).\n    \n    Args:\n        x_t: Current input.\n        h_prev: Previous hidden state.\n        Wf: Forget gate weights.\n        bf: Forget gate bias.\n    \n    Returns:\n        Forget gate activation.\n    \"\"\"\n    combined = np.concatenate([h_prev, x_t], axis=1)\n    return 1 / (1 + np.exp(-(np.dot(combined, Wf) + bf)))\n\n\n# Test\nx_t = np.random.randn(1, 10)\nh_prev = np.random.randn(1, 20)\nWf = np.random.randn(30, 20) * 0.01\nbf = np.zeros((1, 20))\n\nf = forget_gate(x_t, h_prev, Wf, bf)\nprint(f\"Forget gate shape: {f.shape}\")\nprint(f\"Values range: [{f.min():.4f}, {f.max():.4f}]\")\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 10: Model Parameters Count\n\nCalculate total parameters in a neural network."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nfrom typing import List, Tuple\n\ndef count_parameters(layers: List[Tuple[int, int]]) -> int:\n    \"\"\"Count total parameters in a fully-connected network.\n    \n    Args:\n        layers: List of (input_size, output_size) for each layer.\n    \n    Returns:\n        Total number of trainable parameters.\n    \"\"\"\n    total = 0\n    for input_size, output_size in layers:\n        weights = input_size * output_size\n        biases = output_size\n        total += weights + biases\n    return total\n\n\n# Test: Network with input=784, hidden=256, hidden=128, output=10\nlayers = [(784, 256), (256, 128), (128, 10)]\ntotal_params = count_parameters(layers)\n\nprint(\"Network architecture:\")\nfor i, (inp, out) in enumerate(layers):\n    params = inp * out + out\n    print(f\"  Layer {i+1}: {inp} -> {out}, params: {params:,}\")\nprint(f\"\\nTotal parameters: {total_params:,}\")\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 11: Early Stopping\n\nImplement early stopping logic."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nclass EarlyStopping:\n    \"\"\"Early stopping to prevent overfitting.\"\"\"\n    \n    def __init__(self, patience: int = 5, min_delta: float = 0.0):\n        \"\"\"Initialise early stopping.\n        \n        Args:\n            patience: Epochs to wait for improvement.\n            min_delta: Minimum change to qualify as improvement.\n        \"\"\"\n        self.patience = patience\n        self.min_delta = min_delta\n        self.best_loss = float('inf')\n        self.counter = 0\n        self.should_stop = False\n    \n    def __call__(self, val_loss: float) -> bool:\n        \"\"\"Check if training should stop.\n        \n        Args:\n            val_loss: Current validation loss.\n        \n        Returns:\n            True if should stop, False otherwise.\n        \"\"\"\n        if val_loss < self.best_loss - self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.should_stop = True\n        \n        return self.should_stop\n\n\n# Test\nearly_stop = EarlyStopping(patience=3)\nval_losses = [0.5, 0.4, 0.35, 0.36, 0.37, 0.38, 0.39]\n\nfor epoch, loss in enumerate(val_losses):\n    stop = early_stop(loss)\n    print(f\"Epoch {epoch}: loss={loss}, counter={early_stop.counter}, stop={stop}\")\n    if stop:\n        print(\"Early stopping triggered!\")\n        break\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 12: CNN Architecture Design\n\nDesign a CNN architecture for MNIST classification."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\ndef design_mnist_cnn() -> dict:\n    \"\"\"Design a CNN for MNIST (28x28 grayscale images, 10 classes).\n    \n    Returns:\n        Dictionary describing the architecture.\n    \"\"\"\n    architecture = {\n        'input': '28x28x1',\n        'layers': [\n            {'type': 'Conv2D', 'filters': 32, 'kernel': 3, 'activation': 'relu',\n             'output': '26x26x32'},\n            {'type': 'MaxPool2D', 'pool_size': 2, 'output': '13x13x32'},\n            {'type': 'Conv2D', 'filters': 64, 'kernel': 3, 'activation': 'relu',\n             'output': '11x11x64'},\n            {'type': 'MaxPool2D', 'pool_size': 2, 'output': '5x5x64'},\n            {'type': 'Flatten', 'output': '1600'},\n            {'type': 'Dense', 'units': 128, 'activation': 'relu'},\n            {'type': 'Dropout', 'rate': 0.5},\n            {'type': 'Dense', 'units': 10, 'activation': 'softmax'}\n        ],\n        'total_params': {\n            'conv1': 32 * (3 * 3 * 1 + 1),  # 320\n            'conv2': 64 * (3 * 3 * 32 + 1),  # 18,496\n            'dense1': 1600 * 128 + 128,  # 204,928\n            'dense2': 128 * 10 + 10  # 1,290\n        }\n    }\n    \n    total = sum(architecture['total_params'].values())\n    architecture['total_params']['total'] = total\n    \n    return architecture\n\n\n# Test\narch = design_mnist_cnn()\nprint(\"MNIST CNN Architecture:\\n\")\nprint(f\"Input: {arch['input']}\")\nfor layer in arch['layers']:\n    print(f\"  {layer}\")\nprint(f\"\\nTotal parameters: {arch['total_params']['total']:,}\")\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary\n",
    "\n",
    "This notebook covered deep learning fundamentals:\n",
    "\n",
    "1. **Neural Network Basics**: Perceptrons, layers, forward propagation\n",
    "2. **Activation Functions**: Sigmoid, tanh, ReLU, softmax and their properties\n",
    "3. **Backpropagation**: Computing gradients using chain rule\n",
    "4. **Loss Functions**: MSE, binary/categorical cross-entropy\n",
    "5. **Optimisers**: SGD, Momentum, Adam\n",
    "6. **Regularisation**: L1/L2, dropout, batch normalisation, early stopping\n",
    "7. **CNNs**: Convolution, pooling, common architectures\n",
    "8. **RNNs**: Basic RNN, LSTM, GRU for sequential data\n",
    "9. **Transfer Learning**: Feature extraction vs fine-tuning\n",
    "10. **Practical Tips**: Hyperparameter tuning, learning rate schedules\n",
    "\n",
    "---\n",
    "\n",
    "### Key Interview Tips\n",
    "\n",
    "- **Know the intuition**: Be able to explain why ReLU helps with vanishing gradients\n",
    "- **Understand trade-offs**: Why use LSTM over simple RNN? When to use CNN vs RNN?\n",
    "- **Common architectures**: Know ResNet (skip connections), LSTM gates\n",
    "- **Practical knowledge**: How to deal with overfitting, when to use transfer learning\n",
    "- **Math foundations**: Be able to derive simple gradients, explain backprop\n",
    "- **Implementation details**: Xavier initialisation, batch normalisation, dropout"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}