{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering Fundamentals Crash Course for Data Science Assessments\n",
    "\n",
    "**Last Updated:** 25 January 2026\n",
    "\n",
    "This notebook covers data engineering concepts that data scientists should understand. We focus on ETL pipelines, data quality, orchestration, and the fundamentals of building reliable data systems.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction and Setup](#1-introduction-and-setup)\n",
    "2. [ETL vs ELT](#2-etl-vs-elt)\n",
    "3. [Data Pipeline Concepts](#3-data-pipeline-concepts)\n",
    "4. [Batch vs Streaming Processing](#4-batch-vs-streaming-processing)\n",
    "5. [Data Quality and Validation](#5-data-quality-and-validation)\n",
    "6. [Data Formats and Storage](#6-data-formats-and-storage)\n",
    "7. [Orchestration Concepts](#7-orchestration-concepts)\n",
    "8. [Data Modelling for Analytics](#8-data-modelling-for-analytics)\n",
    "9. [Error Handling and Monitoring](#9-error-handling-and-monitoring)\n",
    "10. [Scalability Concepts](#10-scalability-concepts)\n",
    "11. [Practice Questions](#11-practice-questions)\n",
    "12. [Summary](#12-summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction and Setup\n",
    "\n",
    "**Data engineering** builds the infrastructure and pipelines that enable data science. Understanding these concepts helps data scientists:\n",
    "\n",
    "- Work effectively with data engineers\n",
    "- Build reliable data pipelines\n",
    "- Debug data quality issues\n",
    "- Understand data system trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Callable, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import hashlib\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. ETL vs ELT\n",
    "\n",
    "### ETL (Extract, Transform, Load)\n",
    "\n",
    "**Traditional approach**: Transform data before loading into target.\n",
    "\n",
    "```\n",
    "Source -> [Extract] -> [Transform] -> [Load] -> Data Warehouse\n",
    "```\n",
    "\n",
    "### ELT (Extract, Load, Transform)\n",
    "\n",
    "**Modern approach**: Load raw data first, transform in the warehouse.\n",
    "\n",
    "```\n",
    "Source -> [Extract] -> [Load] -> Data Lake -> [Transform] -> Data Warehouse\n",
    "```\n",
    "\n",
    "### Comparison\n",
    "\n",
    "| Aspect | ETL | ELT |\n",
    "|--------|-----|-----|\n",
    "| Transform location | Staging area | Target warehouse |\n",
    "| Raw data preserved | No | Yes |\n",
    "| Compute power | ETL tool | Warehouse (e.g., BigQuery) |\n",
    "| Schema | Defined upfront | Schema-on-read |\n",
    "| Best for | Structured, known requirements | Flexible, exploratory |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETLPipeline:\n",
    "    \"\"\"Simple ETL pipeline demonstration.\"\"\"\n",
    "    \n",
    "    def extract(self, source: Dict[str, Any]) -> pd.DataFrame:\n",
    "        \"\"\"Extract data from source.\n",
    "        \n",
    "        Args:\n",
    "            source: Source configuration.\n",
    "        \n",
    "        Returns:\n",
    "            Raw data as DataFrame.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Extracting from {source['type']}\")\n",
    "        return pd.DataFrame(source['data'])\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame, transformations: List[Callable]) -> pd.DataFrame:\n",
    "        \"\"\"Apply transformations to data.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame.\n",
    "            transformations: List of transformation functions.\n",
    "        \n",
    "        Returns:\n",
    "            Transformed DataFrame.\n",
    "        \"\"\"\n",
    "        result = df.copy()\n",
    "        for transform_func in transformations:\n",
    "            logger.info(f\"Applying transformation: {transform_func.__name__}\")\n",
    "            result = transform_func(result)\n",
    "        return result\n",
    "    \n",
    "    def load(self, df: pd.DataFrame, target: Dict[str, Any]) -> bool:\n",
    "        \"\"\"Load data to target.\n",
    "        \n",
    "        Args:\n",
    "            df: Data to load.\n",
    "            target: Target configuration.\n",
    "        \n",
    "        Returns:\n",
    "            Success status.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Loading {len(df)} rows to {target['type']}\")\n",
    "        return True\n",
    "    \n",
    "    def run(self, source: Dict, transformations: List[Callable], target: Dict) -> pd.DataFrame:\n",
    "        \"\"\"Run the complete ETL pipeline.\n",
    "        \n",
    "        Args:\n",
    "            source: Source configuration.\n",
    "            transformations: List of transformations.\n",
    "            target: Target configuration.\n",
    "        \n",
    "        Returns:\n",
    "            Transformed data.\n",
    "        \"\"\"\n",
    "        raw_data = self.extract(source)\n",
    "        transformed_data = self.transform(raw_data, transformations)\n",
    "        self.load(transformed_data, target)\n",
    "        return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_names(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Standardise column names.\"\"\"\n",
    "    df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_timestamp(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add processing timestamp.\"\"\"\n",
    "    df['processed_at'] = datetime.now()\n",
    "    return df\n",
    "\n",
    "\n",
    "source = {\n",
    "    'type': 'csv',\n",
    "    'data': {\n",
    "        'Customer Name': ['Alice', 'Bob', 'Charlie'],\n",
    "        'Order Total': [100.50, 200.75, 150.25]\n",
    "    }\n",
    "}\n",
    "\n",
    "target = {'type': 'database', 'table': 'orders'}\n",
    "\n",
    "pipeline = ETLPipeline()\n",
    "result = pipeline.run(source, [clean_names, add_timestamp], target)\n",
    "print(\"\\nTransformed data:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Data Pipeline Concepts\n",
    "\n",
    "### Pipeline Characteristics\n",
    "\n",
    "| Property | Description | Example |\n",
    "|----------|-------------|--------|\n",
    "| Idempotent | Same input always produces same output | Re-running doesn't create duplicates |\n",
    "| Reproducible | Can recreate any historical state | Versioned code + data |\n",
    "| Atomic | All or nothing execution | Transaction commits |\n",
    "| Incremental | Process only new/changed data | WHERE date > last_run |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass PipelineConfig:\n    \"\"\"Configuration for a data pipeline.\"\"\"\n    name: str\n    source: str\n    target: str\n    schedule: str\n    incremental: bool = True\n    watermark_column: Optional[str] = None\n\n\nclass IncrementalPipeline:\n    \"\"\"Pipeline that processes only new data.\"\"\"\n    \n    def __init__(self, config: PipelineConfig):\n        \"\"\"Initialise pipeline with configuration.\n        \n        Args:\n            config: Pipeline configuration.\n        \"\"\"\n        self.config = config\n        self.last_watermark: Optional[datetime] = None\n    \n    def get_watermark(self) -> Optional[datetime]:\n        \"\"\"Get the last processed timestamp.\n        \n        Returns:\n            Last watermark or None if first run.\n        \"\"\"\n        return self.last_watermark\n    \n    def set_watermark(self, value: datetime) -> None:\n        \"\"\"Update the watermark after successful processing.\n        \n        Args:\n            value: New watermark value.\n        \"\"\"\n        self.last_watermark = value\n        logger.info(f\"Watermark updated to {value}\")\n    \n    def extract_incremental(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Extract only records after watermark.\n        \n        Args:\n            df: Full source data.\n        \n        Returns:\n            Filtered DataFrame with only new records.\n        \"\"\"\n        if self.last_watermark is None:\n            logger.info(\"First run - extracting all data\")\n            return df\n        \n        col = self.config.watermark_column\n        filtered = df[df[col] > self.last_watermark]  # type: ignore[index]\n        logger.info(f\"Incremental: {len(filtered)} new records (after {self.last_watermark})\")\n        return filtered  # type: ignore[return-value]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "config = PipelineConfig(\n    name='daily_orders',\n    source='orders_db',\n    target='warehouse',\n    schedule='0 2 * * *',\n    incremental=True,\n    watermark_column='created_at'\n)\n\npipeline = IncrementalPipeline(config)\n\ndata = pd.DataFrame({\n    'order_id': [1, 2, 3, 4, 5],\n    'created_at': pd.to_datetime([\n        '2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05'\n    ])\n})\n\nprint(\"Run 1 (full load):\")\nbatch1 = pipeline.extract_incremental(data)\nprint(f\"  Extracted {len(batch1)} records\")\npipeline.set_watermark(batch1['created_at'].max())  # type: ignore[arg-type]\n\ndata_new = pd.concat([data, pd.DataFrame({\n    'order_id': [6, 7],\n    'created_at': pd.to_datetime(['2024-01-06', '2024-01-07'])\n})])\n\nprint(\"\\nRun 2 (incremental):\")\nbatch2 = pipeline.extract_incremental(data_new)\nprint(f\"  Extracted {len(batch2)} records\")\nprint(batch2)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Batch vs Streaming Processing\n",
    "\n",
    "| Aspect | Batch | Streaming |\n",
    "|--------|-------|----------|\n",
    "| Processing | Scheduled intervals | Continuous |\n",
    "| Latency | Hours/days | Seconds/minutes |\n",
    "| Complexity | Lower | Higher |\n",
    "| Use case | Reports, ML training | Real-time dashboards, alerts |\n",
    "| Tools | Spark, Airflow | Kafka, Flink, Spark Streaming |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchProcessor:\n",
    "    \"\"\"Process data in batches.\"\"\"\n",
    "    \n",
    "    def process(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Process entire batch at once.\n",
    "        \n",
    "        Args:\n",
    "            data: Full batch of data.\n",
    "        \n",
    "        Returns:\n",
    "            Processed results.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Processing batch of {len(data)} records\")\n",
    "        result = data.groupby('category').agg({\n",
    "            'amount': ['sum', 'mean', 'count']\n",
    "        })\n",
    "        return result\n",
    "\n",
    "\n",
    "class StreamProcessor:\n",
    "    \"\"\"Process data record by record (simplified streaming simulation).\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.state = {}\n",
    "    \n",
    "    def process_record(self, record: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Process single record and update state.\n",
    "        \n",
    "        Args:\n",
    "            record: Single data record.\n",
    "        \n",
    "        Returns:\n",
    "            Current aggregated state.\n",
    "        \"\"\"\n",
    "        category = record['category']\n",
    "        amount = record['amount']\n",
    "        \n",
    "        if category not in self.state:\n",
    "            self.state[category] = {'sum': 0, 'count': 0}\n",
    "        \n",
    "        self.state[category]['sum'] += amount\n",
    "        self.state[category]['count'] += 1\n",
    "        self.state[category]['avg'] = (\n",
    "            self.state[category]['sum'] / self.state[category]['count']\n",
    "        )\n",
    "        \n",
    "        return self.state[category]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "    'category': ['A', 'B', 'A', 'B', 'A'],\n",
    "    'amount': [100, 200, 150, 250, 120]\n",
    "})\n",
    "\n",
    "print(\"Batch Processing:\")\n",
    "batch = BatchProcessor()\n",
    "print(batch.process(data))\n",
    "\n",
    "print(\"\\nStream Processing (record by record):\")\n",
    "stream = StreamProcessor()\n",
    "for _, row in data.iterrows():\n",
    "    result = stream.process_record(row.to_dict())\n",
    "    print(f\"  After {row['category']}:{row['amount']} -> {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda vs Kappa Architecture\n",
    "\n",
    "| Architecture | Description | Pros | Cons |\n",
    "|--------------|-------------|------|------|\n",
    "| Lambda | Batch + streaming layers | Accurate batch, fast streaming | Maintain two codebases |\n",
    "| Kappa | Streaming only | Single codebase | Reprocessing harder |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Data Quality and Validation\n",
    "\n",
    "### Data Quality Dimensions\n",
    "\n",
    "| Dimension | Description | Example Check |\n",
    "|-----------|-------------|---------------|\n",
    "| Completeness | No missing values | `df.isnull().sum()` |\n",
    "| Accuracy | Values are correct | Range checks, lookup validation |\n",
    "| Consistency | Same format across records | Date formats, units |\n",
    "| Timeliness | Data is up-to-date | Max date within expected range |\n",
    "| Uniqueness | No duplicates | `df.duplicated().sum()` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ValidationResult:\n",
    "    \"\"\"Result of a data validation check.\"\"\"\n",
    "    check_name: str\n",
    "    passed: bool\n",
    "    message: str\n",
    "    details: Optional[Dict[str, Any]] = None\n",
    "\n",
    "\n",
    "class DataValidator:\n",
    "    \"\"\"Validate data quality.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results: List[ValidationResult] = []\n",
    "    \n",
    "    def check_not_null(self, df: pd.DataFrame, columns: List[str]) -> ValidationResult:\n",
    "        \"\"\"Check that specified columns have no null values.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to validate.\n",
    "            columns: Columns to check.\n",
    "        \n",
    "        Returns:\n",
    "            Validation result.\n",
    "        \"\"\"\n",
    "        null_counts = df[columns].isnull().sum()\n",
    "        has_nulls = null_counts.sum() > 0\n",
    "        \n",
    "        result = ValidationResult(\n",
    "            check_name='not_null',\n",
    "            passed=not has_nulls,\n",
    "            message='No null values found' if not has_nulls else f'Found nulls: {null_counts.to_dict()}',\n",
    "            details={'null_counts': null_counts.to_dict()}\n",
    "        )\n",
    "        self.results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def check_unique(self, df: pd.DataFrame, columns: List[str]) -> ValidationResult:\n",
    "        \"\"\"Check that specified columns have unique values.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to validate.\n",
    "            columns: Columns to check for uniqueness.\n",
    "        \n",
    "        Returns:\n",
    "            Validation result.\n",
    "        \"\"\"\n",
    "        duplicates = df.duplicated(subset=columns, keep=False).sum()\n",
    "        \n",
    "        result = ValidationResult(\n",
    "            check_name='unique',\n",
    "            passed=duplicates == 0,\n",
    "            message=f'No duplicates' if duplicates == 0 else f'{duplicates} duplicate rows',\n",
    "            details={'duplicate_count': duplicates}\n",
    "        )\n",
    "        self.results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def check_range(self, df: pd.DataFrame, column: str, min_val: float, max_val: float) -> ValidationResult:\n",
    "        \"\"\"Check that values are within expected range.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to validate.\n",
    "            column: Column to check.\n",
    "            min_val: Minimum allowed value.\n",
    "            max_val: Maximum allowed value.\n",
    "        \n",
    "        Returns:\n",
    "            Validation result.\n",
    "        \"\"\"\n",
    "        out_of_range = ((df[column] < min_val) | (df[column] > max_val)).sum()\n",
    "        \n",
    "        result = ValidationResult(\n",
    "            check_name='range',\n",
    "            passed=out_of_range == 0,\n",
    "            message=f'All values in range [{min_val}, {max_val}]' if out_of_range == 0 else f'{out_of_range} values out of range',\n",
    "            details={'out_of_range_count': out_of_range, 'actual_min': df[column].min(), 'actual_max': df[column].max()}\n",
    "        )\n",
    "        self.results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def check_row_count(self, df: pd.DataFrame, min_rows: int) -> ValidationResult:\n",
    "        \"\"\"Check that DataFrame has minimum number of rows.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to validate.\n",
    "            min_rows: Minimum expected rows.\n",
    "        \n",
    "        Returns:\n",
    "            Validation result.\n",
    "        \"\"\"\n",
    "        row_count = len(df)\n",
    "        \n",
    "        result = ValidationResult(\n",
    "            check_name='row_count',\n",
    "            passed=row_count >= min_rows,\n",
    "            message=f'Row count {row_count} >= {min_rows}' if row_count >= min_rows else f'Only {row_count} rows (expected >= {min_rows})',\n",
    "            details={'actual_rows': row_count, 'expected_min': min_rows}\n",
    "        )\n",
    "        self.results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def summary(self) -> pd.DataFrame:\n",
    "        \"\"\"Get summary of all validation results.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with validation summary.\n",
    "        \"\"\"\n",
    "        return pd.DataFrame([\n",
    "            {'check': r.check_name, 'passed': r.passed, 'message': r.message}\n",
    "            for r in self.results\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 4, 4],\n",
    "    'name': ['Alice', 'Bob', None, 'David', 'Eve'],\n",
    "    'age': [25, 30, 35, 150, 28]\n",
    "})\n",
    "\n",
    "validator = DataValidator()\n",
    "\n",
    "validator.check_not_null(data, ['id', 'name'])\n",
    "validator.check_unique(data, ['id'])\n",
    "validator.check_range(data, 'age', 0, 120)\n",
    "validator.check_row_count(data, 5)\n",
    "\n",
    "print(\"Validation Results:\")\n",
    "print(validator.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Data Formats and Storage\n",
    "\n",
    "### Common Data Formats\n",
    "\n",
    "| Format | Type | Best For | Pros | Cons |\n",
    "|--------|------|----------|------|------|\n",
    "| CSV | Row-based | Small data, compatibility | Human readable | No types, slow |\n",
    "| JSON | Document | APIs, nested data | Flexible schema | Verbose |\n",
    "| Parquet | Columnar | Analytics | Fast queries, compression | Not human readable |\n",
    "| Avro | Row-based | Streaming | Schema evolution | Less common |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Row-based vs Columnar Storage:\")\n",
    "print()\n",
    "print(\"ROW-BASED (CSV, JSON, Avro):\")\n",
    "print(\"  Storage: [id:1, name:Alice, age:25], [id:2, name:Bob, age:30], ...\")\n",
    "print(\"  Good for: Inserting rows, reading full records\")\n",
    "print(\"  Bad for: Aggregating single column (must scan all data)\")\n",
    "print()\n",
    "print(\"COLUMNAR (Parquet, ORC):\")\n",
    "print(\"  Storage: [id: 1,2,3,...], [name: Alice,Bob,...], [age: 25,30,...]\")\n",
    "print(\"  Good for: Analytics (SUM, AVG on one column)\")\n",
    "print(\"  Bad for: Full record retrieval, frequent inserts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_storage_savings(df: pd.DataFrame, column: str) -> Dict[str, Any]:\n",
    "    \"\"\"Estimate storage savings from columnar compression.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame.\n",
    "        column: Column to analyse.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with compression estimates.\n",
    "    \"\"\"\n",
    "    unique_values = df[column].nunique()\n",
    "    total_values = len(df)\n",
    "    \n",
    "    cardinality_ratio = unique_values / total_values\n",
    "    \n",
    "    if cardinality_ratio < 0.1:\n",
    "        compression = 'Excellent (dictionary encoding effective)'\n",
    "    elif cardinality_ratio < 0.5:\n",
    "        compression = 'Good'\n",
    "    else:\n",
    "        compression = 'Limited (high cardinality)'\n",
    "    \n",
    "    return {\n",
    "        'column': column,\n",
    "        'unique_values': unique_values,\n",
    "        'total_values': total_values,\n",
    "        'cardinality_ratio': cardinality_ratio,\n",
    "        'compression_potential': compression\n",
    "    }\n",
    "\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'user_id': range(1000),\n",
    "    'country': np.random.choice(['US', 'UK', 'DE', 'FR'], 1000),\n",
    "    'timestamp': pd.date_range('2024-01-01', periods=1000, freq='h')\n",
    "})\n",
    "\n",
    "print(\"Compression Potential by Column:\")\n",
    "for col in ['user_id', 'country']:\n",
    "    result = estimate_storage_savings(data, col)\n",
    "    print(f\"  {col}: {result['compression_potential']} (cardinality: {result['cardinality_ratio']:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Orchestration Concepts\n",
    "\n",
    "**Orchestration** schedules and manages pipeline execution.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Description | Example |\n",
    "|---------|-------------|--------|\n",
    "| DAG | Directed Acyclic Graph of tasks | Task B depends on Task A |\n",
    "| Schedule | When to run | Cron: `0 2 * * *` (2 AM daily) |\n",
    "| Dependency | Task ordering | Extract -> Transform -> Load |\n",
    "| Backfill | Process historical data | Rerun for past dates |\n",
    "| Idempotency | Safe to re-run | Same result on retry |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Task:\n",
    "    \"\"\"A task in a DAG.\"\"\"\n",
    "    name: str\n",
    "    func: Callable\n",
    "    dependencies: List[str]\n",
    "    retries: int = 3\n",
    "\n",
    "\n",
    "class SimpleDAG:\n",
    "    \"\"\"Simple DAG executor (like a minimal Airflow).\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        \"\"\"Initialise DAG.\n",
    "        \n",
    "        Args:\n",
    "            name: DAG name.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.tasks: Dict[str, Task] = {}\n",
    "        self.results: Dict[str, Any] = {}\n",
    "    \n",
    "    def add_task(self, task: Task) -> None:\n",
    "        \"\"\"Add a task to the DAG.\n",
    "        \n",
    "        Args:\n",
    "            task: Task to add.\n",
    "        \"\"\"\n",
    "        self.tasks[task.name] = task\n",
    "    \n",
    "    def _get_execution_order(self) -> List[str]:\n",
    "        \"\"\"Topological sort to get execution order.\n",
    "        \n",
    "        Returns:\n",
    "            List of task names in execution order.\n",
    "        \"\"\"\n",
    "        in_degree = {name: 0 for name in self.tasks}\n",
    "        for task in self.tasks.values():\n",
    "            for dep in task.dependencies:\n",
    "                if dep in in_degree:\n",
    "                    in_degree[task.name] += 1\n",
    "        \n",
    "        queue = [name for name, degree in in_degree.items() if degree == 0]\n",
    "        result = []\n",
    "        \n",
    "        while queue:\n",
    "            current = queue.pop(0)\n",
    "            result.append(current)\n",
    "            \n",
    "            for name, task in self.tasks.items():\n",
    "                if current in task.dependencies:\n",
    "                    in_degree[name] -= 1\n",
    "                    if in_degree[name] == 0:\n",
    "                        queue.append(name)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def run(self) -> Dict[str, Any]:\n",
    "        \"\"\"Execute the DAG.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of task results.\n",
    "        \"\"\"\n",
    "        order = self._get_execution_order()\n",
    "        logger.info(f\"Execution order: {order}\")\n",
    "        \n",
    "        for task_name in order:\n",
    "            task = self.tasks[task_name]\n",
    "            logger.info(f\"Running task: {task_name}\")\n",
    "            \n",
    "            dep_results = {dep: self.results.get(dep) for dep in task.dependencies}\n",
    "            \n",
    "            for attempt in range(task.retries):\n",
    "                try:\n",
    "                    self.results[task_name] = task.func(dep_results)\n",
    "                    logger.info(f\"Task {task_name} completed\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Task {task_name} failed (attempt {attempt + 1}): {e}\")\n",
    "                    if attempt == task.retries - 1:\n",
    "                        raise\n",
    "        \n",
    "        return self.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_task(deps):\n",
    "    return {'data': [1, 2, 3, 4, 5]}\n",
    "\n",
    "\n",
    "def transform_task(deps):\n",
    "    data = deps['extract']['data']\n",
    "    return {'data': [x * 2 for x in data]}\n",
    "\n",
    "\n",
    "def load_task(deps):\n",
    "    data = deps['transform']['data']\n",
    "    return {'rows_loaded': len(data)}\n",
    "\n",
    "\n",
    "dag = SimpleDAG('etl_pipeline')\n",
    "dag.add_task(Task('extract', extract_task, []))\n",
    "dag.add_task(Task('transform', transform_task, ['extract']))\n",
    "dag.add_task(Task('load', load_task, ['transform']))\n",
    "\n",
    "results = dag.run()\n",
    "print(\"\\nFinal Results:\")\n",
    "for task, result in results.items():\n",
    "    print(f\"  {task}: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cron Schedule Syntax\n",
    "\n",
    "```\n",
    "* * * * *\n",
    "| | | | |\n",
    "| | | | +-- Day of week (0-7, Sunday=0 or 7)\n",
    "| | | +---- Month (1-12)\n",
    "| | +------ Day of month (1-31)\n",
    "| +-------- Hour (0-23)\n",
    "+---------- Minute (0-59)\n",
    "```\n",
    "\n",
    "| Schedule | Cron Expression |\n",
    "|----------|----------------|\n",
    "| Every hour | `0 * * * *` |\n",
    "| Daily at 2 AM | `0 2 * * *` |\n",
    "| Monday at 9 AM | `0 9 * * 1` |\n",
    "| First of month | `0 0 1 * *` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Data Modelling for Analytics\n",
    "\n",
    "### Slowly Changing Dimensions (SCD)\n",
    "\n",
    "How to handle changes to dimension data over time.\n",
    "\n",
    "| Type | Strategy | Example |\n",
    "|------|----------|--------|\n",
    "| SCD 0 | Never update | Static reference data |\n",
    "| SCD 1 | Overwrite | Only current value matters |\n",
    "| SCD 2 | Add new row | Full history with date ranges |\n",
    "| SCD 3 | Add column | Current + previous value |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SCD Type 2 Example (Customer Address Changes):\")\n",
    "print()\n",
    "\n",
    "scd2_example = pd.DataFrame({\n",
    "    'customer_key': [1, 2, 3],\n",
    "    'customer_id': [100, 100, 101],\n",
    "    'name': ['Alice', 'Alice', 'Bob'],\n",
    "    'city': ['London', 'Manchester', 'Leeds'],\n",
    "    'valid_from': ['2020-01-01', '2023-06-15', '2021-03-01'],\n",
    "    'valid_to': ['2023-06-14', '9999-12-31', '9999-12-31'],\n",
    "    'is_current': [False, True, True]\n",
    "})\n",
    "\n",
    "print(scd2_example)\n",
    "print()\n",
    "print(\"Note: customer_id 100 (Alice) has two rows - old address and current address\")\n",
    "print(\"      Use 'is_current = True' for current state, or date range for point-in-time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class SCD2Handler:\n    \"\"\"Handle SCD Type 2 updates.\"\"\"\n    \n    def __init__(self, key_column: str, tracked_columns: List[str]):\n        \"\"\"Initialise SCD2 handler.\n        \n        Args:\n            key_column: Natural key column.\n            tracked_columns: Columns to track for changes.\n        \"\"\"\n        self.key_column = key_column\n        self.tracked_columns = tracked_columns\n    \n    def detect_changes(\n        self,\n        current: pd.DataFrame,\n        incoming: pd.DataFrame\n    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n        \"\"\"Detect inserts, updates, and unchanged records.\n        \n        Args:\n            current: Current dimension table.\n            incoming: New data.\n        \n        Returns:\n            Tuple of (new records, changed records, unchanged records).\n        \"\"\"\n        current_keys = set(current[self.key_column])\n        incoming_keys = set(incoming[self.key_column])\n        \n        new_keys = incoming_keys - current_keys\n        new_records = incoming[incoming[self.key_column].isin(list(new_keys))]  # type: ignore[arg-type]\n        \n        existing_keys = incoming_keys & current_keys\n        \n        changed_records = []\n        unchanged_records = []\n        \n        for key in existing_keys:\n            current_row = current[current[self.key_column] == key][self.tracked_columns].iloc[0]  # type: ignore[union-attr]\n            incoming_row = incoming[incoming[self.key_column] == key][self.tracked_columns].iloc[0]  # type: ignore[union-attr]\n            \n            if not current_row.equals(incoming_row):\n                changed_records.append(incoming[incoming[self.key_column] == key])\n            else:\n                unchanged_records.append(incoming[incoming[self.key_column] == key])\n        \n        changed_df = pd.concat(changed_records) if changed_records else pd.DataFrame()\n        unchanged_df = pd.concat(unchanged_records) if unchanged_records else pd.DataFrame()\n        \n        return new_records, changed_df, unchanged_df  # type: ignore[return-value]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dim = pd.DataFrame({\n",
    "    'customer_id': [1, 2, 3],\n",
    "    'name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'city': ['London', 'Manchester', 'Leeds']\n",
    "})\n",
    "\n",
    "incoming_data = pd.DataFrame({\n",
    "    'customer_id': [1, 2, 4],\n",
    "    'name': ['Alice', 'Bob', 'David'],\n",
    "    'city': ['London', 'Liverpool', 'Bristol']\n",
    "})\n",
    "\n",
    "handler = SCD2Handler(key_column='customer_id', tracked_columns=['name', 'city'])\n",
    "new, changed, unchanged = handler.detect_changes(current_dim, incoming_data)\n",
    "\n",
    "print(\"Change Detection Results:\")\n",
    "print(f\"\\nNew records ({len(new)}):\")\n",
    "print(new if len(new) > 0 else \"  (none)\")\n",
    "print(f\"\\nChanged records ({len(changed)}):\")\n",
    "print(changed if len(changed) > 0 else \"  (none)\")\n",
    "print(f\"\\nUnchanged records ({len(unchanged)}):\")\n",
    "print(unchanged if len(unchanged) > 0 else \"  (none)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Error Handling and Monitoring\n",
    "\n",
    "### Pipeline Failure Modes\n",
    "\n",
    "| Failure Type | Cause | Handling |\n",
    "|--------------|-------|----------|\n",
    "| Source unavailable | Network, permissions | Retry with backoff |\n",
    "| Data quality | Invalid data | Quarantine bad records |\n",
    "| Transform error | Code bug, edge case | Log, alert, fail fast |\n",
    "| Target full | Disk space, limits | Alert, pause pipeline |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry_with_backoff(\n",
    "    func: Callable,\n",
    "    max_retries: int = 3,\n",
    "    base_delay: float = 1.0,\n",
    "    exponential: bool = True\n",
    ") -> Any:\n",
    "    \"\"\"Retry a function with exponential backoff.\n",
    "    \n",
    "    Args:\n",
    "        func: Function to retry.\n",
    "        max_retries: Maximum number of retries.\n",
    "        base_delay: Initial delay in seconds.\n",
    "        exponential: Use exponential backoff.\n",
    "    \n",
    "    Returns:\n",
    "        Function result.\n",
    "    \n",
    "    Raises:\n",
    "        Exception: If all retries fail.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            \n",
    "            delay = base_delay * (2 ** attempt) if exponential else base_delay\n",
    "            logger.warning(f\"Attempt {attempt + 1} failed: {e}. Retrying in {delay}s...\")\n",
    "            time.sleep(delay)\n",
    "\n",
    "\n",
    "class DeadLetterQueue:\n",
    "    \"\"\"Store failed records for later processing.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.failed_records: List[Dict[str, Any]] = []\n",
    "    \n",
    "    def add(self, record: Dict[str, Any], error: str) -> None:\n",
    "        \"\"\"Add a failed record.\n",
    "        \n",
    "        Args:\n",
    "            record: The failed record.\n",
    "            error: Error message.\n",
    "        \"\"\"\n",
    "        self.failed_records.append({\n",
    "            'record': record,\n",
    "            'error': error,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "    \n",
    "    def get_all(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get all failed records.\n",
    "        \n",
    "        Returns:\n",
    "            List of failed records with metadata.\n",
    "        \"\"\"\n",
    "        return self.failed_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_and_process(record: Dict, dlq: DeadLetterQueue) -> Optional[Dict]:\n",
    "    \"\"\"Validate and process a record, sending failures to DLQ.\n",
    "    \n",
    "    Args:\n",
    "        record: Record to process.\n",
    "        dlq: Dead letter queue for failures.\n",
    "    \n",
    "    Returns:\n",
    "        Processed record or None if failed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if record.get('amount', 0) < 0:\n",
    "            raise ValueError(\"Negative amount not allowed\")\n",
    "        \n",
    "        record['processed'] = True\n",
    "        return record\n",
    "        \n",
    "    except Exception as e:\n",
    "        dlq.add(record, str(e))\n",
    "        return None\n",
    "\n",
    "\n",
    "records = [\n",
    "    {'id': 1, 'amount': 100},\n",
    "    {'id': 2, 'amount': -50},\n",
    "    {'id': 3, 'amount': 200},\n",
    "]\n",
    "\n",
    "dlq = DeadLetterQueue()\n",
    "processed = []\n",
    "\n",
    "for record in records:\n",
    "    result = validate_and_process(record, dlq)\n",
    "    if result:\n",
    "        processed.append(result)\n",
    "\n",
    "print(f\"Successfully processed: {len(processed)} records\")\n",
    "print(f\"Failed (in DLQ): {len(dlq.get_all())} records\")\n",
    "print(f\"\\nDLQ contents:\")\n",
    "for item in dlq.get_all():\n",
    "    print(f\"  Record {item['record']['id']}: {item['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Scalability Concepts\n",
    "\n",
    "### Scaling Strategies\n",
    "\n",
    "| Strategy | Description | Example |\n",
    "|----------|-------------|--------|\n",
    "| Vertical | Bigger machine | More RAM, CPUs |\n",
    "| Horizontal | More machines | Distribute across nodes |\n",
    "| Partitioning | Split data | By date, region |\n",
    "| Caching | Store computed results | Redis, Memcached |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_by_date(df: pd.DataFrame, date_column: str) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Partition DataFrame by date.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame.\n",
    "        date_column: Column containing dates.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping date strings to DataFrames.\n",
    "    \"\"\"\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "    df['partition_key'] = df[date_column].dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    partitions = {}\n",
    "    for key, group in df.groupby('partition_key'):\n",
    "        partitions[key] = group.drop(columns=['partition_key'])\n",
    "    \n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "    'event_id': range(10),\n",
    "    'event_date': pd.date_range('2024-01-01', periods=10, freq='12h'),\n",
    "    'value': np.random.randint(1, 100, 10)\n",
    "})\n",
    "\n",
    "partitions = partition_by_date(data, 'event_date')\n",
    "\n",
    "print(\"Partitioned Data:\")\n",
    "for partition_key, partition_data in partitions.items():\n",
    "    print(f\"\\nPartition: {partition_key} ({len(partition_data)} rows)\")\n",
    "    print(partition_data.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCache:\n",
    "    \"\"\"Simple in-memory cache with TTL.\"\"\"\n",
    "    \n",
    "    def __init__(self, ttl_seconds: int = 300):\n",
    "        \"\"\"Initialise cache.\n",
    "        \n",
    "        Args:\n",
    "            ttl_seconds: Time to live in seconds.\n",
    "        \"\"\"\n",
    "        self.cache: Dict[str, Tuple[Any, datetime]] = {}\n",
    "        self.ttl = timedelta(seconds=ttl_seconds)\n",
    "    \n",
    "    def _make_key(self, *args, **kwargs) -> str:\n",
    "        \"\"\"Create cache key from arguments.\"\"\"\n",
    "        key_data = json.dumps({'args': args, 'kwargs': kwargs}, sort_keys=True)\n",
    "        return hashlib.md5(key_data.encode()).hexdigest()\n",
    "    \n",
    "    def get(self, key: str) -> Optional[Any]:\n",
    "        \"\"\"Get value from cache.\n",
    "        \n",
    "        Args:\n",
    "            key: Cache key.\n",
    "        \n",
    "        Returns:\n",
    "            Cached value or None if not found/expired.\n",
    "        \"\"\"\n",
    "        if key in self.cache:\n",
    "            value, timestamp = self.cache[key]\n",
    "            if datetime.now() - timestamp < self.ttl:\n",
    "                return value\n",
    "            del self.cache[key]\n",
    "        return None\n",
    "    \n",
    "    def set(self, key: str, value: Any) -> None:\n",
    "        \"\"\"Set value in cache.\n",
    "        \n",
    "        Args:\n",
    "            key: Cache key.\n",
    "            value: Value to cache.\n",
    "        \"\"\"\n",
    "        self.cache[key] = (value, datetime.now())\n",
    "    \n",
    "    def cached(self, func: Callable) -> Callable:\n",
    "        \"\"\"Decorator to cache function results.\n",
    "        \n",
    "        Args:\n",
    "            func: Function to cache.\n",
    "        \n",
    "        Returns:\n",
    "            Wrapped function.\n",
    "        \"\"\"\n",
    "        def wrapper(*args, **kwargs):\n",
    "            key = self._make_key(*args, **kwargs)\n",
    "            cached_value = self.get(key)\n",
    "            if cached_value is not None:\n",
    "                logger.info(f\"Cache hit for {func.__name__}\")\n",
    "                return cached_value\n",
    "            \n",
    "            logger.info(f\"Cache miss for {func.__name__}\")\n",
    "            result = func(*args, **kwargs)\n",
    "            self.set(key, result)\n",
    "            return result\n",
    "        return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = SimpleCache(ttl_seconds=60)\n",
    "\n",
    "\n",
    "@cache.cached\n",
    "def expensive_computation(x: int) -> int:\n",
    "    \"\"\"Simulate expensive computation.\"\"\"\n",
    "    time.sleep(0.1)\n",
    "    return x * x\n",
    "\n",
    "\n",
    "print(\"First call (cache miss):\")\n",
    "result1 = expensive_computation(5)\n",
    "print(f\"Result: {result1}\")\n",
    "\n",
    "print(\"\\nSecond call (cache hit):\")\n",
    "result2 = expensive_computation(5)\n",
    "print(f\"Result: {result2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 12. Practice Questions\n\nTest your understanding with these interview-style questions. Try to solve each question in the empty code cell before revealing the answer."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 1: ETL vs ELT Decision\n\nWhen would you choose ETL over ELT, and vice versa?"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n**Choose ETL when:**\n- Data needs cleaning before storage (compliance, PII removal)\n- Target system has limited compute (traditional data warehouse)\n- Schema must be strictly defined upfront\n- Data volume is manageable on ETL server\n\n**Choose ELT when:**\n- Using cloud data warehouse (BigQuery, Snowflake, Redshift)\n- Want to preserve raw data for future analysis\n- Schema may evolve (schema-on-read)\n- Transformations are complex and benefit from warehouse compute\n- Need flexibility to re-transform historical data\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 2: Incremental Load Strategy\n\nDesign an incremental load strategy for a table with `updated_at` timestamp."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\ndef incremental_load(source_table: str, target_table: str, conn):\n    \"\"\"Incremental load using watermark.\"\"\"\n    \n    # 1. Get current watermark (max updated_at in target)\n    watermark_query = f\"SELECT MAX(updated_at) FROM {target_table}\"\n    watermark = conn.execute(watermark_query).fetchone()[0]\n    \n    # 2. Extract only new/updated records\n    if watermark:\n        extract_query = f\"\"\"\n            SELECT * FROM {source_table}\n            WHERE updated_at > '{watermark}'\n        \"\"\"\n    else:\n        # First run - full load\n        extract_query = f\"SELECT * FROM {source_table}\"\n    \n    new_data = pd.read_sql(extract_query, conn)\n    \n    # 3. Upsert to target (MERGE or INSERT ON CONFLICT)\n    # Handle both inserts and updates\n    for _, row in new_data.iterrows():\n        upsert_query = f\"\"\"\n            INSERT INTO {target_table} VALUES (...)\n            ON CONFLICT (id) DO UPDATE SET ...\n        \"\"\"\n        conn.execute(upsert_query)\n    \n    return len(new_data)\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 3: Data Quality Checks\n\nWhat data quality checks would you implement for a daily sales pipeline?"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\ndef validate_sales_data(df: pd.DataFrame) -> List[str]:\n    \"\"\"Validate daily sales data.\"\"\"\n    errors = []\n    \n    # 1. Completeness\n    required_cols = ['order_id', 'customer_id', 'amount', 'order_date']\n    for col in required_cols:\n        null_count = df[col].isnull().sum()\n        if null_count > 0:\n            errors.append(f\"{col} has {null_count} null values\")\n    \n    # 2. Uniqueness\n    dup_orders = df['order_id'].duplicated().sum()\n    if dup_orders > 0:\n        errors.append(f\"{dup_orders} duplicate order_ids\")\n    \n    # 3. Accuracy (range checks)\n    negative_amounts = (df['amount'] < 0).sum()\n    if negative_amounts > 0:\n        errors.append(f\"{negative_amounts} negative amounts\")\n    \n    # 4. Timeliness\n    max_date = df['order_date'].max()\n    expected_date = datetime.now().date()\n    if max_date < expected_date - timedelta(days=1):\n        errors.append(f\"Data is stale (max date: {max_date})\")\n    \n    # 5. Volume check\n    if len(df) < 100:  # Expected minimum\n        errors.append(f\"Low volume: only {len(df)} records\")\n    \n    return errors\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 4: Batch vs Streaming\n\nFor each use case, would you use batch or streaming processing?\n\na) Daily sales report\nb) Fraud detection for credit cards\nc) ML model training\nd) Real-time recommendation engine"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n**a) Daily sales report: BATCH**\n- No real-time requirement\n- Aggregates historical data\n- Simpler, cheaper\n\n**b) Fraud detection: STREAMING**\n- Must detect fraud in real-time to block transactions\n- Latency of minutes is too slow\n\n**c) ML model training: BATCH**\n- Needs historical data\n- Training is computationally intensive\n- Doesn't need to be real-time\n\n**d) Real-time recommendations: STREAMING**\n- Must respond to user actions immediately\n- \"Users who viewed this also viewed...\"\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 5: SCD Type 2 Implementation\n\nWrite SQL to update an SCD Type 2 dimension when a customer changes address."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```sql\n-- Step 1: Close the current record\nUPDATE dim_customer\nSET \n    valid_to = CURRENT_DATE - INTERVAL '1 day',\n    is_current = FALSE\nWHERE \n    customer_id = 123\n    AND is_current = TRUE;\n\n-- Step 2: Insert new record\nINSERT INTO dim_customer (\n    customer_id,\n    name,\n    address,\n    city,\n    valid_from,\n    valid_to,\n    is_current\n)\nVALUES (\n    123,\n    'Alice Smith',\n    '456 New Street',\n    'Manchester',\n    CURRENT_DATE,\n    '9999-12-31',\n    TRUE\n);\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 6: Pipeline Idempotency\n\nHow would you make a pipeline idempotent (safe to re-run)?"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n**Strategies for idempotency:**\n\n1. **Delete and replace:** Delete existing data for the time period, then insert\n```sql\nDELETE FROM fact_sales WHERE date = '2024-01-15';\nINSERT INTO fact_sales SELECT ... WHERE date = '2024-01-15';\n```\n\n2. **Upsert (MERGE):** Insert or update based on key\n```sql\nMERGE INTO target USING source\nON target.id = source.id\nWHEN MATCHED THEN UPDATE SET ...\nWHEN NOT MATCHED THEN INSERT ...;\n```\n\n3. **Partitioned overwrite:** Replace entire partition\n```sql\nINSERT OVERWRITE TABLE sales PARTITION (date='2024-01-15')\nSELECT ... FROM staging;\n```\n\n4. **Deduplication:** Include dedup logic\n```sql\nINSERT INTO target\nSELECT * FROM source\nWHERE NOT EXISTS (SELECT 1 FROM target WHERE target.id = source.id);\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 7: Error Handling Strategy\n\nDesign an error handling strategy for a pipeline processing customer orders."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nclass OrderPipeline:\n    def __init__(self):\n        self.dlq = DeadLetterQueue()  # For failed records\n        self.metrics = PipelineMetrics()  # For monitoring\n    \n    def process_order(self, order: dict) -> Optional[dict]:\n        try:\n            # Validate\n            self.validate(order)\n            \n            # Transform\n            transformed = self.transform(order)\n            \n            # Load with retry\n            retry_with_backoff(\n                lambda: self.load(transformed),\n                max_retries=3\n            )\n            \n            self.metrics.record_success()\n            return transformed\n            \n        except ValidationError as e:\n            # Bad data - send to DLQ, continue processing\n            self.dlq.add(order, str(e))\n            self.metrics.record_validation_error()\n            return None\n            \n        except (ConnectionError, TimeoutError) as e:\n            # Transient error - retry handled above, now fail\n            self.metrics.record_system_error()\n            raise PipelineError(f\"Failed after retries: {e}\")\n            \n        except Exception as e:\n            # Unexpected error - alert and stop\n            self.alert_team(f\"Unexpected error: {e}\")\n            raise\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 8: Partitioning Strategy\n\nDesign a partitioning strategy for a fact table with 1 billion rows per year."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n**Strategy: Partition by date + cluster by frequently filtered columns**\n\n```sql\nCREATE TABLE fact_events (\n    event_id BIGINT,\n    event_date DATE,\n    user_id BIGINT,\n    event_type STRING,\n    amount DECIMAL(10, 2)\n)\nPARTITIONED BY (event_date)\nCLUSTERED BY (user_id) INTO 256 BUCKETS;\n```\n\n**Reasoning:**\n1. **Partition by date:** Most queries filter by date range\n   - ~3M rows per day (1B / 365)\n   - Queries only scan relevant partitions\n\n2. **Cluster by user_id:** Common to filter/join on user\n   - 256 buckets balances parallelism and file count\n\n3. **Partition pruning:** `WHERE event_date = '2024-01-15'` only reads one partition\n\n4. **Data retention:** Easy to drop old partitions\n   ```sql\n   ALTER TABLE fact_events DROP PARTITION (event_date < '2022-01-01');\n   ```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 9: Data Format Selection\n\nWhich data format would you choose for each scenario?\n\na) Archive for compliance (rarely accessed)\nb) Analytics queries (aggregate columns)\nc) API responses\nd) Streaming with schema evolution"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n**a) Archive: Parquet with compression**\n- Excellent compression ratio\n- Columnar for efficient storage\n- Self-describing schema\n\n**b) Analytics: Parquet**\n- Columnar format perfect for aggregations\n- Only reads columns needed\n- Predicate pushdown for filtering\n\n**c) API responses: JSON**\n- Human readable\n- Widely supported\n- Flexible schema\n\n**d) Streaming with schema evolution: Avro**\n- Schema stored with data\n- Forward/backward compatibility\n- Compact binary format\n- Kafka's preferred format\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 10: DAG Design\n\nDesign a DAG for an ETL pipeline that:\n1. Extracts from 3 sources in parallel\n2. Validates each source\n3. Joins the validated data\n4. Loads to warehouse"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```\nDAG Structure:\n\nextract_source_a  validate_a \n                                \nextract_source_b  validate_b  join_data  load_warehouse\n                                \nextract_source_c  validate_c \n```\n\n```python\ndag = SimpleDAG('multi_source_etl')\n\n# Parallel extraction (no dependencies)\ndag.add_task(Task('extract_a', extract_source_a, []))\ndag.add_task(Task('extract_b', extract_source_b, []))\ndag.add_task(Task('extract_c', extract_source_c, []))\n\n# Validation (depends on respective extraction)\ndag.add_task(Task('validate_a', validate_data, ['extract_a']))\ndag.add_task(Task('validate_b', validate_data, ['extract_b']))\ndag.add_task(Task('validate_c', validate_data, ['extract_c']))\n\n# Join (depends on all validations)\ndag.add_task(Task('join_data', join_sources, \n                  ['validate_a', 'validate_b', 'validate_c']))\n\n# Load (depends on join)\ndag.add_task(Task('load_warehouse', load_to_warehouse, ['join_data']))\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 11: Monitoring Metrics\n\nWhat metrics would you monitor for a data pipeline?"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n**Pipeline Health Metrics:**\n1. **Run status:** Success/failure rate\n2. **Duration:** Execution time, SLA compliance\n3. **Lag:** Time since last successful run\n\n**Data Quality Metrics:**\n1. **Row counts:** Expected vs actual\n2. **Null rates:** Per column\n3. **Duplicate rates:** Primary key violations\n4. **Schema changes:** Column additions/removals\n\n**Volume Metrics:**\n1. **Records processed:** Per run\n2. **Data size:** GB processed\n3. **Throughput:** Records per second\n\n**Error Metrics:**\n1. **Error rate:** Failed records / total\n2. **DLQ size:** Unprocessed failures\n3. **Error types:** Categorised failures\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 12: Pipeline Backfill\n\nHow would you backfill data for a new pipeline that needs historical data from the past year?"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n**Strategy:**\n\n1. **Partition the backfill:** Process by date chunks\n```python\ndef backfill_pipeline(start_date, end_date, chunk_days=7):\n    current = start_date\n    while current < end_date:\n        chunk_end = min(current + timedelta(days=chunk_days), end_date)\n        process_date_range(current, chunk_end)\n        current = chunk_end\n```\n\n2. **Process oldest first:** Maintains chronological order\n\n3. **Idempotent design:** Safe to re-run if failures occur\n\n4. **Monitor and throttle:** Don't overload source systems\n\n5. **Progress tracking:** Log completed chunks\n```python\ndef mark_chunk_complete(date):\n    db.execute(\n        \"INSERT INTO backfill_progress (date, completed_at) VALUES (?, ?)\",\n        (date, datetime.now())\n    )\n```\n\n6. **Parallel processing:** If source can handle it\n```python\nfrom concurrent.futures import ThreadPoolExecutor\n\nwith ThreadPoolExecutor(max_workers=4) as executor:\n    executor.map(process_date, date_chunks)\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary\n",
    "\n",
    "This notebook covered data engineering fundamentals:\n",
    "\n",
    "1. **ETL vs ELT**: Transform before or after loading; ELT for cloud warehouses\n",
    "2. **Pipeline Concepts**: Idempotency, incremental loads, watermarks\n",
    "3. **Batch vs Streaming**: Scheduled vs continuous processing\n",
    "4. **Data Quality**: Validation checks, completeness, accuracy\n",
    "5. **Data Formats**: Row-based (CSV, JSON) vs columnar (Parquet)\n",
    "6. **Orchestration**: DAGs, dependencies, scheduling (cron)\n",
    "7. **Data Modelling**: SCD Type 2 for historical tracking\n",
    "8. **Error Handling**: Retries, dead letter queues, monitoring\n",
    "9. **Scalability**: Partitioning, caching, horizontal scaling\n",
    "\n",
    "---\n",
    "\n",
    "### Key Interview Tips\n",
    "\n",
    "- **Know ETL vs ELT trade-offs**: Cloud warehouses favour ELT\n",
    "- **Idempotency is essential**: Pipelines must be safe to re-run\n",
    "- **Data quality first**: Validate before loading, use DLQs for failures\n",
    "- **Understand partitioning**: Key for performance at scale\n",
    "- **Monitor everything**: Track row counts, latency, error rates\n",
    "- **Design for failure**: Retries, backfill capability, alerting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}