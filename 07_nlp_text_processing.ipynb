{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP and Text Processing Crash Course for Data Science Assessments\n",
    "\n",
    "**Last Updated:** 25 January 2026\n",
    "\n",
    "This notebook covers essential Natural Language Processing (NLP) concepts commonly tested in data science interviews. We focus on practical text processing techniques, from basic preprocessing to TF-IDF and text classification.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction and Setup](#1-introduction-and-setup)\n",
    "2. [Text Preprocessing Fundamentals](#2-text-preprocessing-fundamentals)\n",
    "3. [Tokenisation](#3-tokenisation)\n",
    "4. [Stopword Removal](#4-stopword-removal)\n",
    "5. [Stemming and Lemmatisation](#5-stemming-and-lemmatisation)\n",
    "6. [Bag of Words (BoW)](#6-bag-of-words-bow)\n",
    "7. [TF-IDF (Term Frequency-Inverse Document Frequency)](#7-tf-idf-term-frequency-inverse-document-frequency)\n",
    "8. [N-grams](#8-n-grams)\n",
    "9. [Word Embeddings Concepts](#9-word-embeddings-concepts)\n",
    "10. [Text Classification Pipeline](#10-text-classification-pipeline)\n",
    "11. [Sentiment Analysis](#11-sentiment-analysis)\n",
    "12. [Practice Questions](#12-practice-questions)\n",
    "13. [Summary](#13-summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction and Setup\n",
    "\n",
    "NLP enables machines to understand, interpret, and generate human language. In data science interviews, you'll encounter questions about text preprocessing, feature extraction, and building text classification models.\n",
    "\n",
    "**Key Libraries:**\n",
    "- **NLTK**: Classic NLP library with comprehensive tools\n",
    "- **scikit-learn**: TF-IDF vectorisation and text classification\n",
    "- **re**: Regular expressions for text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Text Data\n",
    "\n",
    "We'll use sample text data throughout this notebook to demonstrate NLP concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Natural language processing enables computers to understand text.\",\n",
    "    \"Data science combines statistics, programming, and domain knowledge.\",\n",
    "    \"Python is widely used for data analysis and machine learning.\"\n",
    "]\n",
    "\n",
    "sample_text = \"\"\"\n",
    "Natural Language Processing (NLP) is a field of artificial intelligence \n",
    "that focuses on the interaction between computers and humans using natural language.\n",
    "The ultimate objective of NLP is to read, decipher, understand, and make sense \n",
    "of human languages in a valuable manner.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Sample documents loaded:\")\n",
    "for i, doc in enumerate(sample_documents):\n",
    "    print(f\"  {i+1}. {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Text Preprocessing Fundamentals\n",
    "\n",
    "Text preprocessing transforms raw text into a clean, standardised format suitable for analysis. This is often the most time-consuming but critical step in any NLP pipeline.\n",
    "\n",
    "**Common Preprocessing Steps:**\n",
    "\n",
    "| Step | Description | Example |\n",
    "|------|-------------|--------|\n",
    "| Lowercasing | Convert to lowercase | \"Hello\" → \"hello\" |\n",
    "| Punctuation Removal | Remove special characters | \"Hello!\" → \"Hello\" |\n",
    "| Number Handling | Remove or normalise numbers | \"2024\" → \"\" or \"NUM\" |\n",
    "| Whitespace Normalisation | Remove extra spaces | \"hello  world\" → \"hello world\" |\n",
    "| HTML/URL Removal | Strip web artefacts | \"<p>text</p>\" → \"text\" |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(\n",
    "    text: str,\n",
    "    lowercase: bool = True,\n",
    "    remove_punctuation: bool = True,\n",
    "    remove_numbers: bool = False,\n",
    "    remove_urls: bool = True,\n",
    "    remove_html: bool = True\n",
    ") -> str:\n",
    "    \"\"\"Clean and preprocess text for NLP tasks.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string.\n",
    "        lowercase: Convert text to lowercase.\n",
    "        remove_punctuation: Remove punctuation marks.\n",
    "        remove_numbers: Remove numeric characters.\n",
    "        remove_urls: Remove URLs from text.\n",
    "        remove_html: Remove HTML tags.\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned text string.\n",
    "    \"\"\"\n",
    "    if remove_html:\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    if remove_urls:\n",
    "        text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    if remove_punctuation:\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    if remove_numbers:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messy_text = \"<p>Check out https://example.com for MORE info!!! Price: $99.99</p>\"\n",
    "\n",
    "print(f\"Original: {messy_text}\")\n",
    "print(f\"Cleaned:  {preprocess_text(messy_text)}\")\n",
    "print(f\"Keep numbers: {preprocess_text(messy_text, remove_numbers=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Tokenisation\n",
    "\n",
    "**Tokenisation** is the process of breaking text into smaller units called tokens. Tokens can be words, sentences, or subwords.\n",
    "\n",
    "**Types of Tokenisation:**\n",
    "- **Word tokenisation**: Split text into words\n",
    "- **Sentence tokenisation**: Split text into sentences\n",
    "- **Subword tokenisation**: Split into subword units (used in modern models like BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(\n",
    "    text: str,\n",
    "    method: str = 'word'\n",
    ") -> List[str]:\n",
    "    \"\"\"Tokenise text into words or sentences.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string.\n",
    "        method: Tokenisation method ('word' or 'sentence').\n",
    "    \n",
    "    Returns:\n",
    "        List of tokens.\n",
    "    \"\"\"\n",
    "    if method == 'word':\n",
    "        return word_tokenize(text)\n",
    "    elif method == 'sentence':\n",
    "        return sent_tokenize(text)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Mr. Smith bought cheapsite.com for 1.5 million dollars. It's a great deal!\"\n",
    "\n",
    "print(\"Word tokens:\")\n",
    "print(tokenize_text(text, 'word'))\n",
    "\n",
    "print(\"\\nSentence tokens:\")\n",
    "print(tokenize_text(text, 'sentence'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Tokenisation with Regular Expressions\n",
    "\n",
    "For interview questions, you may need to implement tokenisation without NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Simple word tokenisation using regex.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string.\n",
    "    \n",
    "    Returns:\n",
    "        List of word tokens.\n",
    "    \"\"\"\n",
    "    return re.findall(r'[a-zA-Z]+', text.lower())\n",
    "\n",
    "\n",
    "print(\"Simple tokenisation:\")\n",
    "print(simple_tokenize(\"Hello, World! This is NLP 101.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Stopword Removal\n",
    "\n",
    "**Stopwords** are common words that carry little meaningful information (e.g., \"the\", \"is\", \"at\"). Removing them reduces noise and dimensionality.\n",
    "\n",
    "**When to remove stopwords:**\n",
    "- Bag of Words / TF-IDF models\n",
    "- Topic modelling\n",
    "- Keyword extraction\n",
    "\n",
    "**When to keep stopwords:**\n",
    "- Sentiment analysis (\"not good\" vs \"good\")\n",
    "- Named entity recognition\n",
    "- Language models / deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "print(f\"Number of English stopwords: {len(english_stopwords)}\")\n",
    "print(f\"\\nSample stopwords: {list(english_stopwords)[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(\n",
    "    tokens: List[str],\n",
    "    stop_words: set = None\n",
    ") -> List[str]:\n",
    "    \"\"\"Remove stopwords from a list of tokens.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of word tokens.\n",
    "        stop_words: Set of stopwords to remove.\n",
    "    \n",
    "    Returns:\n",
    "        Filtered list of tokens.\n",
    "    \"\"\"\n",
    "    if stop_words is None:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    return [token for token in tokens if token.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "tokens = simple_tokenize(text)\n",
    "\n",
    "print(f\"Original tokens: {tokens}\")\n",
    "print(f\"After stopword removal: {remove_stopwords(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Stemming and Lemmatisation\n",
    "\n",
    "Both techniques reduce words to their base form, but they work differently:\n",
    "\n",
    "| Technique | Method | Example | Result |\n",
    "|-----------|--------|---------|--------|\n",
    "| **Stemming** | Rule-based suffix stripping | \"running\", \"runs\", \"ran\" | \"run\", \"run\", \"ran\" |\n",
    "| **Lemmatisation** | Dictionary-based, considers POS | \"running\", \"runs\", \"ran\" | \"run\", \"run\", \"run\" |\n",
    "\n",
    "**Stemming**: Faster but can produce non-words (\"studies\" → \"studi\")\n",
    "\n",
    "**Lemmatisation**: More accurate but slower, requires POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def stem_tokens(tokens: List[str]) -> List[str]:\n",
    "    \"\"\"Apply Porter stemming to tokens.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of word tokens.\n",
    "    \n",
    "    Returns:\n",
    "        List of stemmed tokens.\n",
    "    \"\"\"\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "\n",
    "def lemmatize_tokens(tokens: List[str]) -> List[str]:\n",
    "    \"\"\"Apply lemmatisation to tokens.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of word tokens.\n",
    "    \n",
    "    Returns:\n",
    "        List of lemmatised tokens.\n",
    "    \"\"\"\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['running', 'runs', 'ran', 'studies', 'studying', 'better', 'wolves']\n",
    "\n",
    "print(f\"{'Original':<12} {'Stemmed':<12} {'Lemmatised':<12}\")\n",
    "print(\"-\" * 36)\n",
    "for word in words:\n",
    "    print(f\"{word:<12} {stemmer.stem(word):<12} {lemmatizer.lemmatize(word):<12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Text Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipeline(\n",
    "    text: str,\n",
    "    remove_stops: bool = True,\n",
    "    use_lemmatisation: bool = True\n",
    ") -> List[str]:\n",
    "    \"\"\"Complete text preprocessing pipeline.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string.\n",
    "        remove_stops: Whether to remove stopwords.\n",
    "        use_lemmatisation: Use lemmatisation (True) or stemming (False).\n",
    "    \n",
    "    Returns:\n",
    "        List of processed tokens.\n",
    "    \"\"\"\n",
    "    text = preprocess_text(text)\n",
    "    tokens = simple_tokenize(text)\n",
    "    \n",
    "    if remove_stops:\n",
    "        tokens = remove_stopwords(tokens)\n",
    "    \n",
    "    if use_lemmatisation:\n",
    "        tokens = lemmatize_tokens(tokens)\n",
    "    else:\n",
    "        tokens = stem_tokens(tokens)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The researchers are studying various machine learning algorithms!\"\n",
    "\n",
    "print(f\"Original: {text}\")\n",
    "print(f\"Processed: {preprocess_pipeline(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Bag of Words (BoW)\n",
    "\n",
    "**Bag of Words** represents text as a vector of word counts, ignoring grammar and word order.\n",
    "\n",
    "**How it works:**\n",
    "1. Build vocabulary from all documents\n",
    "2. For each document, count occurrences of each vocabulary word\n",
    "3. Result: Document-term matrix\n",
    "\n",
    "**Limitations:**\n",
    "- Ignores word order (\"dog bites man\" = \"man bites dog\")\n",
    "- High dimensionality with large vocabularies\n",
    "- Sparse matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bow_manually(documents: List[str]) -> Tuple[Dict[str, int], np.ndarray]:\n",
    "    \"\"\"Create Bag of Words representation manually.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of text documents.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (vocabulary dict, document-term matrix).\n",
    "    \"\"\"\n",
    "    all_tokens = []\n",
    "    doc_tokens = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        tokens = preprocess_pipeline(doc)\n",
    "        doc_tokens.append(tokens)\n",
    "        all_tokens.extend(tokens)\n",
    "    \n",
    "    vocabulary = {word: idx for idx, word in enumerate(sorted(set(all_tokens)))}\n",
    "    \n",
    "    matrix = np.zeros((len(documents), len(vocabulary)))\n",
    "    for doc_idx, tokens in enumerate(doc_tokens):\n",
    "        for token in tokens:\n",
    "            if token in vocabulary:\n",
    "                matrix[doc_idx, vocabulary[token]] += 1\n",
    "    \n",
    "    return vocabulary, matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"I love machine learning\",\n",
    "    \"Machine learning is great\",\n",
    "    \"I love data science\"\n",
    "]\n",
    "\n",
    "vocab, bow_matrix = create_bow_manually(docs)\n",
    "\n",
    "print(\"Vocabulary:\")\n",
    "print(vocab)\n",
    "print(\"\\nBoW Matrix:\")\n",
    "print(bow_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using scikit-learn's CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "bow_sklearn = vectorizer.fit_transform(docs)\n",
    "\n",
    "print(\"Feature names:\")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(\"\\nBoW Matrix (sklearn):\")\n",
    "print(bow_sklearn.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "**TF-IDF** weighs terms by their importance in a document relative to the entire corpus. Words that appear frequently in one document but rarely across all documents get higher scores.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)$$\n",
    "\n",
    "Where:\n",
    "- **TF (Term Frequency)**: $\\frac{\\text{count of term } t \\text{ in document } d}{\\text{total terms in document } d}$\n",
    "- **IDF (Inverse Document Frequency)**: $\\log\\frac{\\text{total documents}}{\\text{documents containing term } t}$\n",
    "\n",
    "**Why TF-IDF?**\n",
    "- Reduces weight of common words\n",
    "- Increases weight of distinctive words\n",
    "- Better for information retrieval and document similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(document: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"Compute term frequency for a document.\n",
    "    \n",
    "    Args:\n",
    "        document: List of tokens.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping terms to their frequencies.\n",
    "    \"\"\"\n",
    "    word_counts = Counter(document)\n",
    "    total_words = len(document)\n",
    "    return {word: count / total_words for word, count in word_counts.items()}\n",
    "\n",
    "\n",
    "def compute_idf(documents: List[List[str]]) -> Dict[str, float]:\n",
    "    \"\"\"Compute inverse document frequency for a corpus.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of tokenised documents.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping terms to their IDF scores.\n",
    "    \"\"\"\n",
    "    n_docs = len(documents)\n",
    "    all_words = set(word for doc in documents for word in doc)\n",
    "    \n",
    "    idf = {}\n",
    "    for word in all_words:\n",
    "        doc_count = sum(1 for doc in documents if word in doc)\n",
    "        idf[word] = np.log(n_docs / doc_count) + 1\n",
    "    \n",
    "    return idf\n",
    "\n",
    "\n",
    "def compute_tfidf(\n",
    "    documents: List[str]\n",
    ") -> Tuple[List[Dict[str, float]], Dict[str, float]]:\n",
    "    \"\"\"Compute TF-IDF for a corpus.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of text documents.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (list of TF-IDF dicts per document, IDF dict).\n",
    "    \"\"\"\n",
    "    tokenized_docs = [preprocess_pipeline(doc) for doc in documents]\n",
    "    idf = compute_idf(tokenized_docs)\n",
    "    \n",
    "    tfidf_docs = []\n",
    "    for doc in tokenized_docs:\n",
    "        tf = compute_tf(doc)\n",
    "        tfidf = {word: tf[word] * idf[word] for word in tf}\n",
    "        tfidf_docs.append(tfidf)\n",
    "    \n",
    "    return tfidf_docs, idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog sat on the log\",\n",
    "    \"Cats and dogs are pets\"\n",
    "]\n",
    "\n",
    "tfidf_results, idf_scores = compute_tfidf(documents)\n",
    "\n",
    "print(\"TF-IDF scores for each document:\\n\")\n",
    "for i, tfidf in enumerate(tfidf_results):\n",
    "    print(f\"Document {i+1}: {documents[i]}\")\n",
    "    sorted_terms = sorted(tfidf.items(), key=lambda x: x[1], reverse=True)\n",
    "    for term, score in sorted_terms:\n",
    "        print(f\"  {term}: {score:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using scikit-learn's TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(),\n",
    "    columns=feature_names,\n",
    "    index=[f'Doc {i+1}' for i in range(len(documents))]\n",
    ")\n",
    "\n",
    "print(\"TF-IDF Matrix (sklearn):\")\n",
    "print(tfidf_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. N-grams\n",
    "\n",
    "**N-grams** are contiguous sequences of n items from text. They capture local word order and context that BoW misses.\n",
    "\n",
    "| Type | n | Example (\"I love data science\") |\n",
    "|------|---|-------------------------------|\n",
    "| Unigram | 1 | \"I\", \"love\", \"data\", \"science\" |\n",
    "| Bigram | 2 | \"I love\", \"love data\", \"data science\" |\n",
    "| Trigram | 3 | \"I love data\", \"love data science\" |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(tokens: List[str], n: int) -> List[Tuple[str, ...]]:\n",
    "    \"\"\"Generate n-grams from a list of tokens.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of word tokens.\n",
    "        n: Size of n-gram.\n",
    "    \n",
    "    Returns:\n",
    "        List of n-gram tuples.\n",
    "    \"\"\"\n",
    "    return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I love machine learning and data science\"\n",
    "tokens = text.lower().split()\n",
    "\n",
    "print(f\"Unigrams: {generate_ngrams(tokens, 1)}\")\n",
    "print(f\"Bigrams:  {generate_ngrams(tokens, 2)}\")\n",
    "print(f\"Trigrams: {generate_ngrams(tokens, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "bigram_matrix = bigram_vectorizer.fit_transform(sample_documents)\n",
    "\n",
    "print(\"Features with bigrams:\")\n",
    "print(bigram_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Word Embeddings Concepts\n",
    "\n",
    "**Word embeddings** represent words as dense vectors in a continuous vector space, capturing semantic relationships.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Method | Description | Key Property |\n",
    "|--------|-------------|-------------|\n",
    "| **Word2Vec** | Neural network-based (CBOW, Skip-gram) | \"king - man + woman = queen\" |\n",
    "| **GloVe** | Global word co-occurrence statistics | Captures global corpus statistics |\n",
    "| **FastText** | Subword embeddings | Handles out-of-vocabulary words |\n",
    "\n",
    "### Word2Vec Architectures\n",
    "\n",
    "1. **CBOW (Continuous Bag of Words)**: Predicts target word from context\n",
    "2. **Skip-gram**: Predicts context words from target word\n",
    "\n",
    "**Interview Tip**: Be prepared to explain the difference between sparse (BoW/TF-IDF) and dense (embeddings) representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sparse vs Dense Representations:\\n\")\n",
    "print(\"Sparse (BoW/TF-IDF):\")\n",
    "print(\"  - High dimensional (vocabulary size)\")\n",
    "print(\"  - Mostly zeros\")\n",
    "print(\"  - No semantic relationships\")\n",
    "print(\"  - Example: [0, 0, 1, 0, 0, 2, 0, 0, 1, 0, ...]\")\n",
    "print(\"\\nDense (Word Embeddings):\")\n",
    "print(\"  - Low dimensional (typically 50-300)\")\n",
    "print(\"  - All values non-zero\")\n",
    "print(\"  - Captures semantic similarity\")\n",
    "print(\"  - Example: [0.25, -0.13, 0.89, 0.02, -0.45, ...]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "Cosine similarity measures the angle between two vectors, commonly used to compare document or word similarity.\n",
    "\n",
    "$$\\text{cosine\\_similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"Compute cosine similarity between two vectors.\n",
    "    \n",
    "    Args:\n",
    "        vec1: First vector.\n",
    "        vec2: Second vector.\n",
    "    \n",
    "    Returns:\n",
    "        Cosine similarity score between -1 and 1.\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return dot_product / (norm1 * norm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_for_similarity = [\n",
    "    \"I love machine learning\",\n",
    "    \"I enjoy deep learning\",\n",
    "    \"The weather is sunny today\"\n",
    "]\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "tfidf_sim = tfidf_vec.fit_transform(docs_for_similarity).toarray()\n",
    "\n",
    "print(\"Document Similarity (Cosine):\")\n",
    "print(f\"Doc 1 vs Doc 2: {cosine_similarity(tfidf_sim[0], tfidf_sim[1]):.4f}\")\n",
    "print(f\"Doc 1 vs Doc 3: {cosine_similarity(tfidf_sim[0], tfidf_sim[2]):.4f}\")\n",
    "print(f\"Doc 2 vs Doc 3: {cosine_similarity(tfidf_sim[1], tfidf_sim[2]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Text Classification Pipeline\n",
    "\n",
    "Text classification assigns predefined categories to text documents. Common applications include spam detection, sentiment analysis, and topic classification.\n",
    "\n",
    "**Pipeline Steps:**\n",
    "1. Text preprocessing\n",
    "2. Feature extraction (TF-IDF)\n",
    "3. Model training\n",
    "4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Great product, highly recommend!\",\n",
    "    \"Terrible quality, waste of money\",\n",
    "    \"Amazing experience, will buy again\",\n",
    "    \"Disappointed with the purchase\",\n",
    "    \"Excellent value for the price\",\n",
    "    \"Poor customer service\",\n",
    "    \"Love this item, perfect!\",\n",
    "    \"Not worth the money\",\n",
    "    \"Best purchase I've made\",\n",
    "    \"Awful product, don't buy\",\n",
    "    \"Fantastic quality and fast shipping\",\n",
    "    \"Broken on arrival, very unhappy\",\n",
    "    \"Exceeded my expectations\",\n",
    "    \"Complete waste of time\",\n",
    "    \"Highly satisfied customer\",\n",
    "    \"Worst experience ever\"\n",
    "]\n",
    "\n",
    "labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_text_classifier(\n",
    "    model_type: str = 'naive_bayes'\n",
    ") -> Pipeline:\n",
    "    \"\"\"Build a text classification pipeline.\n",
    "    \n",
    "    Args:\n",
    "        model_type: Type of classifier ('naive_bayes' or 'logistic').\n",
    "    \n",
    "    Returns:\n",
    "        Scikit-learn Pipeline object.\n",
    "    \"\"\"\n",
    "    if model_type == 'naive_bayes':\n",
    "        classifier = MultinomialNB()\n",
    "    elif model_type == 'logistic':\n",
    "        classifier = LogisticRegression(max_iter=1000)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(stop_words='english', ngram_range=(1, 2))),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pipeline = build_text_classifier('naive_bayes')\n",
    "nb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb_pipeline.predict(X_test)\n",
    "\n",
    "print(\"Naive Bayes Classification Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reviews = [\n",
    "    \"This product is absolutely wonderful!\",\n",
    "    \"Terrible experience, never again\",\n",
    "    \"It's okay, nothing special\"\n",
    "]\n",
    "\n",
    "predictions = nb_pipeline.predict(new_reviews)\n",
    "print(\"Predictions on new reviews:\")\n",
    "for review, pred in zip(new_reviews, predictions):\n",
    "    sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
    "    print(f\"  '{review}' -> {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Sentiment Analysis\n",
    "\n",
    "**Sentiment analysis** determines the emotional tone of text (positive, negative, neutral). It's a specific application of text classification.\n",
    "\n",
    "**Approaches:**\n",
    "1. **Lexicon-based**: Use predefined sentiment dictionaries\n",
    "2. **Machine learning**: Train classifiers on labelled data\n",
    "3. **Deep learning**: Use neural networks (LSTM, BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = {'good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic',\n",
    "                  'love', 'best', 'happy', 'perfect', 'recommend', 'satisfied'}\n",
    "negative_words = {'bad', 'terrible', 'awful', 'horrible', 'worst', 'hate',\n",
    "                  'disappointed', 'poor', 'waste', 'broken', 'unhappy', 'never'}\n",
    "\n",
    "\n",
    "def lexicon_sentiment(text: str) -> Tuple[str, float]:\n",
    "    \"\"\"Simple lexicon-based sentiment analysis.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (sentiment label, score).\n",
    "    \"\"\"\n",
    "    tokens = simple_tokenize(text)\n",
    "    \n",
    "    pos_count = sum(1 for t in tokens if t in positive_words)\n",
    "    neg_count = sum(1 for t in tokens if t in negative_words)\n",
    "    \n",
    "    total = pos_count + neg_count\n",
    "    if total == 0:\n",
    "        return 'Neutral', 0.0\n",
    "    \n",
    "    score = (pos_count - neg_count) / total\n",
    "    \n",
    "    if score > 0:\n",
    "        return 'Positive', score\n",
    "    elif score < 0:\n",
    "        return 'Negative', score\n",
    "    else:\n",
    "        return 'Neutral', score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"This is a great product, I love it!\",\n",
    "    \"Terrible experience, worst purchase ever\",\n",
    "    \"The product arrived on time\",\n",
    "    \"Good quality but poor customer service\"\n",
    "]\n",
    "\n",
    "print(\"Lexicon-based Sentiment Analysis:\\n\")\n",
    "for sentence in test_sentences:\n",
    "    sentiment, score = lexicon_sentiment(sentence)\n",
    "    print(f\"Text: '{sentence}'\")\n",
    "    print(f\"Sentiment: {sentiment} (score: {score:.2f})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 12. Practice Questions\n\nTest your understanding with these interview-style questions. Try to solve each question in the empty code cell before revealing the answer."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 1: Word Frequency Counter\n\nWrite a function that takes a sentence and returns a list of tuples containing each word and its frequency (TF). Remove punctuation, convert to lowercase, and remove English stopwords."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nimport re\nfrom collections import Counter\nfrom nltk.corpus import stopwords\n\ndef word_frequencies(sentence: str) -> List[Tuple[str, float]]:\n    \"\"\"Compute word frequencies (TF) for a sentence.\n    \n    Args:\n        sentence: Input text string.\n    \n    Returns:\n        List of (word, frequency) tuples.\n    \"\"\"\n    stop_words = set(stopwords.words('english'))\n    \n    tokens = re.findall(r'[a-z]+', sentence.lower())\n    tokens = [t for t in tokens if t not in stop_words]\n    \n    counts = Counter(tokens)\n    total = sum(counts.values())\n    \n    return [(word, count / total) for word, count in counts.items()]\n\n\n# Test\nresult = word_frequencies(\"The quick brown fox jumps over the lazy dog\")\nprint(result)\n# [('quick', 0.2), ('brown', 0.2), ('fox', 0.2), ('jumps', 0.2), ('lazy', 0.2)]\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 2: Document Similarity\n\nWrite a function that computes the cosine similarity between two documents using TF-IDF vectors."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\n\ndef document_similarity(doc1: str, doc2: str) -> float:\n    \"\"\"Compute cosine similarity between two documents.\n    \n    Args:\n        doc1: First document.\n        doc2: Second document.\n    \n    Returns:\n        Cosine similarity score.\n    \"\"\"\n    vectorizer = TfidfVectorizer()\n    tfidf = vectorizer.fit_transform([doc1, doc2]).toarray()\n    \n    dot_product = np.dot(tfidf[0], tfidf[1])\n    norm1 = np.linalg.norm(tfidf[0])\n    norm2 = np.linalg.norm(tfidf[1])\n    \n    return dot_product / (norm1 * norm2)\n\n\n# Test\nsim = document_similarity(\n    \"Machine learning is fascinating\",\n    \"Deep learning is a subset of machine learning\"\n)\nprint(f\"Similarity: {sim:.4f}\")\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 3: N-gram Generator\n\nImplement a function that generates all n-grams from a given text and returns them with their frequencies."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nfrom collections import Counter\nfrom typing import Dict, Tuple\n\ndef ngram_frequencies(text: str, n: int) -> Dict[Tuple[str, ...], int]:\n    \"\"\"Generate n-grams and their frequencies.\n    \n    Args:\n        text: Input text string.\n        n: Size of n-gram.\n    \n    Returns:\n        Dictionary mapping n-grams to counts.\n    \"\"\"\n    tokens = text.lower().split()\n    ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n    return dict(Counter(ngrams))\n\n\n# Test\ntext = \"I love data science and I love machine learning\"\nprint(ngram_frequencies(text, 2))\n# {('i', 'love'): 2, ('love', 'data'): 1, ...}\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 4: Custom Stopwords\n\nWrite a function that removes stopwords from text but allows adding custom stopwords to the default list."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nfrom nltk.corpus import stopwords\nfrom typing import List, Set\n\ndef remove_custom_stopwords(\n    text: str,\n    custom_stopwords: List[str] = None\n) -> str:\n    \"\"\"Remove stopwords including custom ones.\n    \n    Args:\n        text: Input text string.\n        custom_stopwords: Additional stopwords to remove.\n    \n    Returns:\n        Text with stopwords removed.\n    \"\"\"\n    stop_words = set(stopwords.words('english'))\n    \n    if custom_stopwords:\n        stop_words.update(word.lower() for word in custom_stopwords)\n    \n    tokens = text.lower().split()\n    filtered = [t for t in tokens if t not in stop_words]\n    \n    return ' '.join(filtered)\n\n\n# Test\ntext = \"The data science course is very interesting\"\nprint(remove_custom_stopwords(text, ['data', 'course']))\n# \"science interesting\"\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 5: Text Classification Pipeline\n\nBuild a complete text classification pipeline that preprocesses text, extracts TF-IDF features, and trains a Naive Bayes classifier."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\ndef build_and_train_classifier(\n    texts: List[str],\n    labels: List[int],\n    test_size: float = 0.2\n) -> Tuple[Pipeline, float]:\n    \"\"\"Build and train a text classifier.\n    \n    Args:\n        texts: List of text documents.\n        labels: List of labels.\n        test_size: Proportion for test set.\n    \n    Returns:\n        Tuple of (trained pipeline, accuracy).\n    \"\"\"\n    X_train, X_test, y_train, y_test = train_test_split(\n        texts, labels, test_size=test_size, random_state=42\n    )\n    \n    pipeline = Pipeline([\n        ('tfidf', TfidfVectorizer(stop_words='english')),\n        ('clf', MultinomialNB())\n    ])\n    \n    pipeline.fit(X_train, y_train)\n    accuracy = accuracy_score(y_test, pipeline.predict(X_test))\n    \n    return pipeline, accuracy\n\n\n# Test\ntexts = [\"great product\", \"bad quality\", \"love it\", \"hate it\"] * 10\nlabels = [1, 0, 1, 0] * 10\nmodel, acc = build_and_train_classifier(texts, labels)\nprint(f\"Accuracy: {acc:.2f}\")\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 6: IDF Calculation\n\nImplement a function that computes IDF scores for all unique words in a corpus."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nimport numpy as np\nfrom typing import Dict, List\n\ndef compute_idf(documents: List[str]) -> Dict[str, float]:\n    \"\"\"Compute IDF scores for a corpus.\n    \n    Args:\n        documents: List of text documents.\n    \n    Returns:\n        Dictionary mapping words to IDF scores.\n    \"\"\"\n    n_docs = len(documents)\n    doc_words = [set(doc.lower().split()) for doc in documents]\n    \n    all_words = set().union(*doc_words)\n    \n    idf = {}\n    for word in all_words:\n        doc_count = sum(1 for doc in doc_words if word in doc)\n        idf[word] = np.log(n_docs / doc_count) + 1\n    \n    return idf\n\n\n# Test\ndocs = [\"cat sat mat\", \"dog sat log\", \"cat dog pet\"]\nidf_scores = compute_idf(docs)\nfor word, score in sorted(idf_scores.items()):\n    print(f\"{word}: {score:.3f}\")\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 7: Stemming vs Lemmatisation Comparison\n\nWrite a function that compares stemming and lemmatisation outputs for a list of words."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom typing import List, Tuple\n\ndef compare_normalisation(words: List[str]) -> List[Tuple[str, str, str]]:\n    \"\"\"Compare stemming and lemmatisation for words.\n    \n    Args:\n        words: List of words to process.\n    \n    Returns:\n        List of (original, stemmed, lemmatised) tuples.\n    \"\"\"\n    stemmer = PorterStemmer()\n    lemmatizer = WordNetLemmatizer()\n    \n    results = []\n    for word in words:\n        stemmed = stemmer.stem(word)\n        lemmatised = lemmatizer.lemmatize(word)\n        results.append((word, stemmed, lemmatised))\n    \n    return results\n\n\n# Test\nwords = ['running', 'runs', 'ran', 'studies', 'wolves']\nfor orig, stem, lemma in compare_normalisation(words):\n    print(f\"{orig:12} -> stem: {stem:10} lemma: {lemma}\")\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 8: Most Important Words\n\nGiven a document and a TF-IDF matrix, find the top N most important words."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\n\ndef top_tfidf_words(\n    documents: List[str],\n    doc_index: int,\n    n: int = 5\n) -> List[Tuple[str, float]]:\n    \"\"\"Find top N words by TF-IDF score in a document.\n    \n    Args:\n        documents: List of text documents.\n        doc_index: Index of document to analyse.\n        n: Number of top words to return.\n    \n    Returns:\n        List of (word, score) tuples.\n    \"\"\"\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform(documents)\n    \n    feature_names = vectorizer.get_feature_names_out()\n    doc_vector = tfidf_matrix[doc_index].toarray()[0]\n    \n    top_indices = np.argsort(doc_vector)[-n:][::-1]\n    \n    return [(feature_names[i], doc_vector[i]) for i in top_indices]\n\n\n# Test\ndocs = [\n    \"Machine learning is transforming data science\",\n    \"Deep learning uses neural networks\",\n    \"Data science requires statistics knowledge\"\n]\ntop_words = top_tfidf_words(docs, 0, 3)\nprint(\"Top words in Doc 0:\")\nfor word, score in top_words:\n    print(f\"  {word}: {score:.4f}\")\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 9: Email Spam Detector\n\nBuild a simple rule-based spam detector that checks for common spam indicators."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nimport re\n\ndef detect_spam(email_text: str) -> Tuple[bool, List[str]]:\n    \"\"\"Detect spam using rule-based approach.\n    \n    Args:\n        email_text: Email content to check.\n    \n    Returns:\n        Tuple of (is_spam, list of triggered rules).\n    \"\"\"\n    spam_indicators = [\n        (r'\\bfree\\b', 'Contains \"free\"'),\n        (r'\\bwinner\\b', 'Contains \"winner\"'),\n        (r'\\bcongratulations\\b', 'Contains \"congratulations\"'),\n        (r'\\$\\d+', 'Contains money amount'),\n        (r'!{2,}', 'Multiple exclamation marks'),\n        (r'URGENT', 'Contains \"URGENT\"'),\n        (r'click here', 'Contains \"click here\"'),\n    ]\n    \n    triggered = []\n    text_lower = email_text.lower()\n    \n    for pattern, description in spam_indicators:\n        if re.search(pattern, text_lower) or re.search(pattern, email_text):\n            triggered.append(description)\n    \n    is_spam = len(triggered) >= 2\n    return is_spam, triggered\n\n\n# Test\nemail = \"CONGRATULATIONS! You're a WINNER!! Click here to claim $1000 FREE!\"\nis_spam, rules = detect_spam(email)\nprint(f\"Is spam: {is_spam}\")\nprint(f\"Triggered rules: {rules}\")\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 10: Vocabulary Builder\n\nCreate a function that builds a vocabulary from a corpus with minimum frequency threshold."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nfrom collections import Counter\nfrom typing import Dict, List\n\ndef build_vocabulary(\n    documents: List[str],\n    min_freq: int = 1,\n    max_vocab_size: int = None\n) -> Dict[str, int]:\n    \"\"\"Build vocabulary with frequency threshold.\n    \n    Args:\n        documents: List of text documents.\n        min_freq: Minimum word frequency.\n        max_vocab_size: Maximum vocabulary size.\n    \n    Returns:\n        Dictionary mapping words to indices.\n    \"\"\"\n    all_words = []\n    for doc in documents:\n        tokens = doc.lower().split()\n        all_words.extend(tokens)\n    \n    word_counts = Counter(all_words)\n    \n    filtered = [(w, c) for w, c in word_counts.items() if c >= min_freq]\n    filtered.sort(key=lambda x: (-x[1], x[0]))\n    \n    if max_vocab_size:\n        filtered = filtered[:max_vocab_size]\n    \n    return {word: idx for idx, (word, _) in enumerate(filtered)}\n\n\n# Test\ndocs = [\"I love data\", \"I love science\", \"data science is great\"]\nvocab = build_vocabulary(docs, min_freq=2)\nprint(vocab)\n# {'i': 0, 'love': 1, 'data': 2, 'science': 3}\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 11: Text Normalisation Pipeline\n\nCreate a configurable text normalisation pipeline class."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nimport re\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nclass TextNormaliser:\n    \"\"\"Configurable text normalisation pipeline.\"\"\"\n    \n    def __init__(\n        self,\n        lowercase: bool = True,\n        remove_punctuation: bool = True,\n        remove_stopwords: bool = True,\n        use_stemming: bool = False,\n        use_lemmatisation: bool = True\n    ):\n        \"\"\"Initialise the normaliser.\n        \n        Args:\n            lowercase: Convert to lowercase.\n            remove_punctuation: Remove punctuation.\n            remove_stopwords: Remove stopwords.\n            use_stemming: Apply stemming.\n            use_lemmatisation: Apply lemmatisation.\n        \"\"\"\n        self.lowercase = lowercase\n        self.remove_punctuation = remove_punctuation\n        self.remove_stopwords = remove_stopwords\n        self.use_stemming = use_stemming\n        self.use_lemmatisation = use_lemmatisation\n        \n        self.stemmer = PorterStemmer()\n        self.lemmatizer = WordNetLemmatizer()\n        self.stop_words = set(stopwords.words('english'))\n    \n    def normalise(self, text: str) -> str:\n        \"\"\"Normalise text according to configuration.\n        \n        Args:\n            text: Input text string.\n        \n        Returns:\n            Normalised text string.\n        \"\"\"\n        if self.lowercase:\n            text = text.lower()\n        \n        if self.remove_punctuation:\n            text = re.sub(r'[^\\w\\s]', '', text)\n        \n        tokens = text.split()\n        \n        if self.remove_stopwords:\n            tokens = [t for t in tokens if t not in self.stop_words]\n        \n        if self.use_stemming:\n            tokens = [self.stemmer.stem(t) for t in tokens]\n        elif self.use_lemmatisation:\n            tokens = [self.lemmatizer.lemmatize(t) for t in tokens]\n        \n        return ' '.join(tokens)\n\n\n# Test\nnormaliser = TextNormaliser(use_lemmatisation=True)\ntext = \"The researchers were studying various algorithms!\"\nprint(normaliser.normalise(text))\n# \"researcher studying various algorithm\"\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 12: Keyword Extraction\n\nExtract the most important keywords from a document using TF-IDF against a background corpus."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write your solution here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<details>\n<summary>Click to reveal answer</summary>\n\n```python\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nfrom typing import List, Tuple\n\ndef extract_keywords(\n    target_doc: str,\n    background_corpus: List[str],\n    n_keywords: int = 5\n) -> List[Tuple[str, float]]:\n    \"\"\"Extract keywords from document using TF-IDF.\n    \n    Args:\n        target_doc: Document to extract keywords from.\n        background_corpus: Corpus for IDF calculation.\n        n_keywords: Number of keywords to extract.\n    \n    Returns:\n        List of (keyword, score) tuples.\n    \"\"\"\n    corpus = background_corpus + [target_doc]\n    \n    vectorizer = TfidfVectorizer(stop_words='english')\n    tfidf_matrix = vectorizer.fit_transform(corpus)\n    \n    feature_names = vectorizer.get_feature_names_out()\n    target_vector = tfidf_matrix[-1].toarray()[0]\n    \n    top_indices = np.argsort(target_vector)[-n_keywords:][::-1]\n    \n    keywords = [\n        (feature_names[i], target_vector[i])\n        for i in top_indices\n        if target_vector[i] > 0\n    ]\n    \n    return keywords\n\n\n# Test\nbackground = [\n    \"The economy is growing steadily\",\n    \"Stock markets reached new highs\",\n    \"Inflation rates remain stable\"\n]\ntarget = \"Machine learning revolutionises artificial intelligence applications\"\n\nkeywords = extract_keywords(target, background, 3)\nprint(\"Keywords:\")\nfor kw, score in keywords:\n    print(f\"  {kw}: {score:.4f}\")\n```\n\n</details>\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary\n",
    "\n",
    "This notebook covered essential NLP concepts for data science interviews:\n",
    "\n",
    "1. **Text Preprocessing**: Cleaning text by removing noise, normalising case, and handling special characters\n",
    "2. **Tokenisation**: Breaking text into words or sentences\n",
    "3. **Stopword Removal**: Filtering common words that carry little meaning\n",
    "4. **Stemming and Lemmatisation**: Reducing words to their base forms\n",
    "5. **Bag of Words**: Representing text as word count vectors\n",
    "6. **TF-IDF**: Weighing terms by importance relative to corpus\n",
    "7. **N-grams**: Capturing word sequences and local context\n",
    "8. **Word Embeddings**: Dense vector representations capturing semantics\n",
    "9. **Text Classification**: Building pipelines for categorising text\n",
    "10. **Sentiment Analysis**: Determining emotional tone of text\n",
    "\n",
    "---\n",
    "\n",
    "### Key Interview Tips\n",
    "\n",
    "- **Know when to use what**: TF-IDF for traditional ML, embeddings for deep learning\n",
    "- **Preprocessing matters**: Always clean and normalise text before feature extraction\n",
    "- **Understand the trade-offs**: Stemming is fast but crude; lemmatisation is accurate but slower\n",
    "- **Be able to implement from scratch**: Interviewers often ask for manual TF-IDF or tokenisation\n",
    "- **Consider the use case**: Sentiment analysis may need stopwords; topic modelling may not\n",
    "- **Know cosine similarity**: The standard metric for comparing text vectors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}