{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing Crash Course\n",
    "\n",
    "**Date Created:** 20 January 2026\n",
    "\n",
    "This notebook provides a comprehensive guide to data cleaning and preprocessing techniques commonly tested in data science interviews and assessments. Each section includes explanations, examples, and practice questions to solidify your understanding.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Identifying and Handling Missing Values](#1-identifying-and-handling-missing-values)\n",
    "2. [Detecting and Handling Outliers](#2-detecting-and-handling-outliers)\n",
    "3. [Data Type Conversions](#3-data-type-conversions)\n",
    "4. [Encoding Categorical Variables](#4-encoding-categorical-variables)\n",
    "5. [Feature Scaling and Normalisation](#5-feature-scaling-and-normalisation)\n",
    "6. [Handling Duplicates](#6-handling-duplicates)\n",
    "7. [String Cleaning and Text Preprocessing](#7-string-cleaning-and-text-preprocessing)\n",
    "8. [Date Parsing and Feature Extraction](#8-date-parsing-and-feature-extraction)\n",
    "9. [Binning and Discretisation](#9-binning-and-discretisation)\n",
    "10. [Feature Engineering Basics](#10-feature-engineering-basics)\n",
    "11. [Practice Questions](#11-practice-questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import (\n",
    "    LabelEncoder, \n",
    "    OneHotEncoder, \n",
    "    StandardScaler, \n",
    "    MinMaxScaler, \n",
    "    RobustScaler,\n",
    "    KBinsDiscretizer\n",
    ")\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Identifying and Handling Missing Values\n",
    "\n",
    "Missing values occur when no data is recorded for certain entries in a dataset. This can happen due to:\n",
    "- Data collection errors\n",
    "- Incomplete surveys or forms\n",
    "- System failures\n",
    "- Data integration issues\n",
    "\n",
    "### 1.1 Identifying Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data with missing values\n",
    "data = {\n",
    "    'name': ['Alice', 'Bob', None, 'David', 'Eve'],\n",
    "    'age': [25, np.nan, 35, 40, np.nan],\n",
    "    'salary': [50000, 60000, np.nan, 80000, 55000],\n",
    "    'department': ['HR', 'IT', 'IT', None, 'Finance'],\n",
    "    'years_experience': [2, 5, np.nan, 10, 3]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nPercentage of missing values:\")\n",
    "print((df.isnull().sum() / len(df) * 100).round(2))\n",
    "print(\"\\nTotal missing values:\", df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise missing values pattern\n",
    "print(\"Missing value pattern (True = Missing):\")\n",
    "print(df.isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Handling Missing Values\n",
    "\n",
    "#### Method 1: Dropping Missing Values (`dropna`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_dropna(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Demonstrate various dropna options.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with missing values.\n",
    "    \"\"\"\n",
    "    print(\"Original shape:\", df.shape)\n",
    "    \n",
    "    # Drop rows with ANY missing values\n",
    "    df_drop_any = df.dropna()\n",
    "    print(\"\\nAfter dropna() - drop rows with any NaN:\")\n",
    "    print(df_drop_any)\n",
    "    print(\"Shape:\", df_drop_any.shape)\n",
    "    \n",
    "    # Drop rows only if ALL values are missing\n",
    "    df_drop_all = df.dropna(how='all')\n",
    "    print(\"\\nAfter dropna(how='all') - drop rows where all values are NaN:\")\n",
    "    print(df_drop_all)\n",
    "    \n",
    "    # Drop rows based on specific columns\n",
    "    df_drop_subset = df.dropna(subset=['salary'])\n",
    "    print(\"\\nAfter dropna(subset=['salary']):\")\n",
    "    print(df_drop_subset)\n",
    "    \n",
    "    # Drop rows with threshold (keep rows with at least n non-null values)\n",
    "    df_thresh = df.dropna(thresh=4)\n",
    "    print(\"\\nAfter dropna(thresh=4) - keep rows with at least 4 non-null values:\")\n",
    "    print(df_thresh)\n",
    "\n",
    "demonstrate_dropna(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: Filling Missing Values (`fillna`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_fillna(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Demonstrate various fillna strategies.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with missing values.\n",
    "    \"\"\"\n",
    "    # Fill with a constant value\n",
    "    print(\"Fill with constant value (0):\")\n",
    "    print(df['age'].fillna(0))\n",
    "    \n",
    "    # Fill with mean (for numerical columns)\n",
    "    print(\"\\nFill age with mean:\")\n",
    "    print(df['age'].fillna(df['age'].mean()))\n",
    "    \n",
    "    # Fill with median (more robust to outliers)\n",
    "    print(\"\\nFill salary with median:\")\n",
    "    print(df['salary'].fillna(df['salary'].median()))\n",
    "    \n",
    "    # Fill with mode (for categorical columns)\n",
    "    print(\"\\nFill department with mode:\")\n",
    "    print(df['department'].fillna(df['department'].mode()[0]))\n",
    "    \n",
    "    # Forward fill (propagate last valid observation)\n",
    "    print(\"\\nForward fill (ffill):\")\n",
    "    print(df['age'].ffill())\n",
    "    \n",
    "    # Backward fill\n",
    "    print(\"\\nBackward fill (bfill):\")\n",
    "    print(df['age'].bfill())\n",
    "\n",
    "demonstrate_fillna(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 3: Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time series data for interpolation example\n",
    "ts_data = pd.DataFrame({\n",
    "    'date': pd.date_range('2025-01-01', periods=10),\n",
    "    'temperature': [20, 22, np.nan, np.nan, 28, 30, np.nan, 32, 31, 29]\n",
    "})\n",
    "print(\"Time series data with gaps:\")\n",
    "print(ts_data)\n",
    "\n",
    "# Linear interpolation\n",
    "ts_data['temp_linear'] = ts_data['temperature'].interpolate(method='linear')\n",
    "\n",
    "# Polynomial interpolation\n",
    "ts_data['temp_polynomial'] = ts_data['temperature'].interpolate(method='polynomial', order=2)\n",
    "\n",
    "print(\"\\nAfter interpolation:\")\n",
    "print(ts_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 4: Using sklearn Imputers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_with_sklearn(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Demonstrate sklearn imputation methods.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with missing values.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with imputed values.\n",
    "    \"\"\"\n",
    "    numerical_cols = ['age', 'salary', 'years_experience']\n",
    "    df_numeric = df[numerical_cols].copy()\n",
    "    \n",
    "    # SimpleImputer with mean strategy\n",
    "    imputer_mean = SimpleImputer(strategy='mean')\n",
    "    df_mean_imputed = pd.DataFrame(\n",
    "        imputer_mean.fit_transform(df_numeric),\n",
    "        columns=numerical_cols\n",
    "    )\n",
    "    print(\"SimpleImputer (mean strategy):\")\n",
    "    print(df_mean_imputed)\n",
    "    \n",
    "    # KNN Imputer (considers relationships between features)\n",
    "    knn_imputer = KNNImputer(n_neighbors=2)\n",
    "    df_knn_imputed = pd.DataFrame(\n",
    "        knn_imputer.fit_transform(df_numeric),\n",
    "        columns=numerical_cols\n",
    "    )\n",
    "    print(\"\\nKNN Imputer:\")\n",
    "    print(df_knn_imputed)\n",
    "    \n",
    "    return df_knn_imputed\n",
    "\n",
    "impute_with_sklearn(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Detecting and Handling Outliers\n",
    "\n",
    "An outlier is a data point that deviates significantly from other observations. Outliers can be caused by:\n",
    "- Data entry errors\n",
    "- Measurement errors\n",
    "- Genuine extreme values\n",
    "\n",
    "### 2.1 IQR (Interquartile Range) Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with outliers\n",
    "np.random.seed(42)\n",
    "data_with_outliers = np.concatenate([\n",
    "    np.random.normal(50, 10, 100),  # Normal data\n",
    "    [150, 160, -20, -30]  # Outliers\n",
    "])\n",
    "df_outliers = pd.DataFrame({'value': data_with_outliers})\n",
    "\n",
    "print(f\"Data statistics:\")\n",
    "print(df_outliers.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(data: pd.Series, multiplier: float = 1.5) -> tuple:\n",
    "    \"\"\"Detect outliers using the IQR method.\n",
    "    \n",
    "    Args:\n",
    "        data: Series containing numerical data.\n",
    "        multiplier: IQR multiplier for bounds (default 1.5).\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (lower_bound, upper_bound, outlier_mask).\n",
    "    \"\"\"\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - multiplier * IQR\n",
    "    upper_bound = Q3 + multiplier * IQR\n",
    "    \n",
    "    outlier_mask = (data < lower_bound) | (data > upper_bound)\n",
    "    \n",
    "    print(f\"Q1: {Q1:.2f}, Q3: {Q3:.2f}, IQR: {IQR:.2f}\")\n",
    "    print(f\"Lower bound: {lower_bound:.2f}, Upper bound: {upper_bound:.2f}\")\n",
    "    print(f\"Number of outliers: {outlier_mask.sum()}\")\n",
    "    \n",
    "    return lower_bound, upper_bound, outlier_mask\n",
    "\n",
    "lower, upper, outliers = detect_outliers_iqr(df_outliers['value'])\n",
    "print(\"\\nOutlier values:\")\n",
    "print(df_outliers[outliers]['value'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Z-Score Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_zscore(data: pd.Series, threshold: float = 3.0) -> pd.Series:\n",
    "    \"\"\"Detect outliers using the Z-score method.\n",
    "    \n",
    "    Args:\n",
    "        data: Series containing numerical data.\n",
    "        threshold: Z-score threshold (default 3.0).\n",
    "        \n",
    "    Returns:\n",
    "        Boolean Series indicating outliers.\n",
    "    \"\"\"\n",
    "    z_scores = np.abs(stats.zscore(data))\n",
    "    outlier_mask = z_scores > threshold\n",
    "    \n",
    "    print(f\"Z-score threshold: {threshold}\")\n",
    "    print(f\"Number of outliers: {outlier_mask.sum()}\")\n",
    "    \n",
    "    return outlier_mask\n",
    "\n",
    "zscore_outliers = detect_outliers_zscore(df_outliers['value'])\n",
    "print(\"\\nOutlier values (Z-score method):\")\n",
    "print(df_outliers[zscore_outliers]['value'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outliers(df: pd.DataFrame, column: str, method: str = 'cap') -> pd.DataFrame:\n",
    "    \"\"\"Handle outliers using different methods.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the data.\n",
    "        column: Column name to process.\n",
    "        method: Method to handle outliers ('cap', 'remove', 'impute').\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with handled outliers.\n",
    "    \"\"\"\n",
    "    df_result = df.copy()\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    \n",
    "    if method == 'cap':\n",
    "        # Cap/Winsorise: Replace outliers with boundary values\n",
    "        df_result[column] = df_result[column].clip(lower=lower, upper=upper)\n",
    "    elif method == 'remove':\n",
    "        # Remove outliers\n",
    "        mask = (df_result[column] >= lower) & (df_result[column] <= upper)\n",
    "        df_result = df_result[mask]\n",
    "    elif method == 'impute':\n",
    "        # Replace outliers with median\n",
    "        median = df_result[column].median()\n",
    "        outlier_mask = (df_result[column] < lower) | (df_result[column] > upper)\n",
    "        df_result.loc[outlier_mask, column] = median\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "# Demonstrate each method\n",
    "print(\"Original shape:\", df_outliers.shape)\n",
    "print(\"Original stats:\")\n",
    "print(df_outliers.describe().T[['min', 'max', 'mean']])\n",
    "\n",
    "df_capped = handle_outliers(df_outliers, 'value', method='cap')\n",
    "print(\"\\nAfter capping:\")\n",
    "print(df_capped.describe().T[['min', 'max', 'mean']])\n",
    "\n",
    "df_removed = handle_outliers(df_outliers, 'value', method='remove')\n",
    "print(f\"\\nAfter removal (shape: {df_removed.shape}):\")\n",
    "print(df_removed.describe().T[['min', 'max', 'mean']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Data Type Conversions\n",
    "\n",
    "Ensuring correct data types is crucial for proper analysis and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data with incorrect types\n",
    "df_types = pd.DataFrame({\n",
    "    'id': ['001', '002', '003', '004'],\n",
    "    'age': ['25', '30', '35', '28'],\n",
    "    'salary': ['50000.50', '60000.75', '55000.00', '70000.25'],\n",
    "    'is_active': ['True', 'False', 'True', 'True'],\n",
    "    'join_date': ['2023-01-15', '2022-06-20', '2024-03-10', '2023-09-05']\n",
    "})\n",
    "\n",
    "print(\"Original data types:\")\n",
    "print(df_types.dtypes)\n",
    "print(\"\\nSample data:\")\n",
    "print(df_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_types(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert columns to appropriate data types.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns to convert.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with converted data types.\n",
    "    \"\"\"\n",
    "    df_converted = df.copy()\n",
    "    \n",
    "    # Convert to integer\n",
    "    df_converted['age'] = df_converted['age'].astype(int)\n",
    "    \n",
    "    # Convert to float\n",
    "    df_converted['salary'] = df_converted['salary'].astype(float)\n",
    "    \n",
    "    # Convert to boolean\n",
    "    df_converted['is_active'] = df_converted['is_active'].map({'True': True, 'False': False})\n",
    "    \n",
    "    # Convert to datetime\n",
    "    df_converted['join_date'] = pd.to_datetime(df_converted['join_date'])\n",
    "    \n",
    "    # Convert to category (memory efficient for repeated values)\n",
    "    df_converted['id'] = df_converted['id'].astype('category')\n",
    "    \n",
    "    return df_converted\n",
    "\n",
    "df_converted = convert_data_types(df_types)\n",
    "print(\"Converted data types:\")\n",
    "print(df_converted.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling conversion errors\n",
    "messy_numbers = pd.Series(['100', '200', 'N/A', '400', 'missing', '600'])\n",
    "\n",
    "# Using errors='coerce' converts invalid values to NaN\n",
    "converted = pd.to_numeric(messy_numbers, errors='coerce')\n",
    "print(\"Converting messy data with errors='coerce':\")\n",
    "print(converted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Encoding Categorical Variables\n",
    "\n",
    "Machine learning models require numerical input, so categorical data must be encoded.\n",
    "\n",
    "### 4.1 Label Encoding\n",
    "\n",
    "Converts each category to a unique integer. Best for ordinal data or tree-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample categorical data\n",
    "df_cat = pd.DataFrame({\n",
    "    'colour': ['red', 'blue', 'green', 'blue', 'red', 'green'],\n",
    "    'size': ['small', 'medium', 'large', 'small', 'large', 'medium'],\n",
    "    'quality': ['low', 'medium', 'high', 'medium', 'high', 'low']\n",
    "})\n",
    "print(\"Original categorical data:\")\n",
    "print(df_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "df_label_encoded = df_cat.copy()\n",
    "df_label_encoded['colour_encoded'] = label_encoder.fit_transform(df_cat['colour'])\n",
    "\n",
    "print(\"Label Encoded (colour):\")\n",
    "print(df_label_encoded[['colour', 'colour_encoded']])\n",
    "print(\"\\nMapping:\", dict(zip(label_encoder.classes_, range(len(label_encoder.classes_)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 One-Hot Encoding\n",
    "\n",
    "Creates binary columns for each category. Best for nominal data (no inherent order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding with pandas\n",
    "df_onehot = pd.get_dummies(df_cat['colour'], prefix='colour')\n",
    "print(\"One-Hot Encoded (using pandas):\")\n",
    "print(df_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding with sklearn\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False, drop='first')  # drop='first' to avoid multicollinearity\n",
    "\n",
    "colour_encoded = onehot_encoder.fit_transform(df_cat[['colour']])\n",
    "feature_names = onehot_encoder.get_feature_names_out(['colour'])\n",
    "\n",
    "df_onehot_sklearn = pd.DataFrame(colour_encoded, columns=feature_names)\n",
    "print(\"One-Hot Encoded (sklearn with drop='first'):\")\n",
    "print(df_onehot_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Ordinal Encoding\n",
    "\n",
    "For categorical variables with a natural order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual ordinal encoding with custom order\n",
    "size_order = {'small': 0, 'medium': 1, 'large': 2}\n",
    "quality_order = {'low': 0, 'medium': 1, 'high': 2}\n",
    "\n",
    "df_ordinal = df_cat.copy()\n",
    "df_ordinal['size_encoded'] = df_cat['size'].map(size_order)\n",
    "df_ordinal['quality_encoded'] = df_cat['quality'].map(quality_order)\n",
    "\n",
    "print(\"Ordinal Encoded:\")\n",
    "print(df_ordinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Target Encoding (Mean Encoding)\n",
    "\n",
    "Replaces categories with the mean of the target variable. Useful for high-cardinality features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encode(df: pd.DataFrame, column: str, target: str) -> pd.Series:\n",
    "    \"\"\"Apply target encoding to a categorical column.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the data.\n",
    "        column: Categorical column to encode.\n",
    "        target: Target column for calculating means.\n",
    "        \n",
    "    Returns:\n",
    "        Series with target-encoded values.\n",
    "    \"\"\"\n",
    "    target_means = df.groupby(column)[target].mean()\n",
    "    return df[column].map(target_means)\n",
    "\n",
    "# Example with target encoding\n",
    "df_target = pd.DataFrame({\n",
    "    'city': ['London', 'Paris', 'London', 'Berlin', 'Paris', 'Berlin', 'London', 'Paris'],\n",
    "    'price': [500, 400, 550, 300, 420, 320, 480, 450]\n",
    "})\n",
    "\n",
    "df_target['city_encoded'] = target_encode(df_target, 'city', 'price')\n",
    "print(\"Target Encoded:\")\n",
    "print(df_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Feature Scaling and Normalisation\n",
    "\n",
    "Scaling ensures features are on a similar scale, which is important for distance-based algorithms and gradient descent.\n",
    "\n",
    "### 5.1 StandardScaler (Z-score Normalisation)\n",
    "\n",
    "Transforms data to have mean=0 and std=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for scaling\n",
    "df_scale = pd.DataFrame({\n",
    "    'age': [25, 30, 35, 40, 45, 50],\n",
    "    'income': [30000, 50000, 60000, 80000, 100000, 150000],\n",
    "    'score': [0.5, 0.6, 0.7, 0.8, 0.85, 0.9]\n",
    "})\n",
    "print(\"Original data:\")\n",
    "print(df_scale)\n",
    "print(\"\\nOriginal statistics:\")\n",
    "print(df_scale.describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_standard = pd.DataFrame(\n",
    "    scaler.fit_transform(df_scale),\n",
    "    columns=df_scale.columns\n",
    ")\n",
    "print(\"After StandardScaler:\")\n",
    "print(df_standard.round(2))\n",
    "print(\"\\nMean:\", df_standard.mean().round(4).values)\n",
    "print(\"Std:\", df_standard.std().round(4).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 MinMaxScaler (Min-Max Normalisation)\n",
    "\n",
    "Scales data to a fixed range, typically [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMaxScaler\n",
    "minmax_scaler = MinMaxScaler()\n",
    "df_minmax = pd.DataFrame(\n",
    "    minmax_scaler.fit_transform(df_scale),\n",
    "    columns=df_scale.columns\n",
    ")\n",
    "print(\"After MinMaxScaler:\")\n",
    "print(df_minmax.round(2))\n",
    "print(\"\\nMin:\", df_minmax.min().values)\n",
    "print(\"Max:\", df_minmax.max().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 RobustScaler\n",
    "\n",
    "Uses median and IQR, making it robust to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RobustScaler (robust to outliers)\n",
    "robust_scaler = RobustScaler()\n",
    "df_robust = pd.DataFrame(\n",
    "    robust_scaler.fit_transform(df_scale),\n",
    "    columns=df_scale.columns\n",
    ")\n",
    "print(\"After RobustScaler:\")\n",
    "print(df_robust.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Which Scaler\n",
    "\n",
    "| Scaler | Use When | Not Suitable For |\n",
    "|--------|----------|------------------|\n",
    "| StandardScaler | Data is approximately normally distributed | Data with significant outliers |\n",
    "| MinMaxScaler | Need bounded values (e.g., neural networks) | Data with outliers |\n",
    "| RobustScaler | Data contains outliers | When exact bounds are required |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Handling Duplicates\n",
    "\n",
    "Duplicate records can skew analysis and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with duplicates\n",
    "df_dup = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 2, 4, 3, 5],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Bob', 'David', 'Charlie', 'Eve'],\n",
    "    'value': [100, 200, 300, 200, 400, 300, 500]\n",
    "})\n",
    "print(\"Data with duplicates:\")\n",
    "print(df_dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify duplicates\n",
    "print(\"Duplicate rows (all columns):\")\n",
    "print(df_dup[df_dup.duplicated(keep=False)])\n",
    "\n",
    "print(\"\\nDuplicate rows (based on 'id'):\")\n",
    "print(df_dup[df_dup.duplicated(subset=['id'], keep=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_duplicates(df: pd.DataFrame, subset: list = None, keep: str = 'first') -> pd.DataFrame:\n",
    "    \"\"\"Remove duplicate rows from a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to process.\n",
    "        subset: Columns to consider for identifying duplicates.\n",
    "        keep: Which duplicate to keep ('first', 'last', False).\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with duplicates removed.\n",
    "    \"\"\"\n",
    "    n_before = len(df)\n",
    "    df_clean = df.drop_duplicates(subset=subset, keep=keep)\n",
    "    n_after = len(df_clean)\n",
    "    \n",
    "    print(f\"Removed {n_before - n_after} duplicate rows\")\n",
    "    return df_clean\n",
    "\n",
    "df_no_dup = handle_duplicates(df_dup, subset=['id'], keep='first')\n",
    "print(\"\\nAfter removing duplicates:\")\n",
    "print(df_no_dup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. String Cleaning and Text Preprocessing\n",
    "\n",
    "Text data often requires cleaning before analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample messy text data\n",
    "df_text = pd.DataFrame({\n",
    "    'name': ['  Alice Smith  ', 'BOB JONES', 'charlie brown', '  David Lee'],\n",
    "    'email': ['ALICE@EMAIL.COM', 'bob@email.com', 'Charlie@Email.Com', 'david@email.com'],\n",
    "    'phone': ['123-456-7890', '(234) 567-8901', '345.678.9012', '456 789 0123'],\n",
    "    'description': ['Good product!', 'Bad...    very bad!!!', 'OK  product', 'Great!!!   ']\n",
    "})\n",
    "print(\"Messy text data:\")\n",
    "print(df_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean text columns in a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with text columns.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with cleaned text.\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Strip whitespace\n",
    "    df_clean['name'] = df_clean['name'].str.strip()\n",
    "    \n",
    "    # Convert to title case\n",
    "    df_clean['name'] = df_clean['name'].str.title()\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    df_clean['email'] = df_clean['email'].str.lower()\n",
    "    \n",
    "    # Remove non-numeric characters from phone\n",
    "    df_clean['phone'] = df_clean['phone'].str.replace(r'[^0-9]', '', regex=True)\n",
    "    \n",
    "    # Clean description: remove extra spaces and punctuation\n",
    "    df_clean['description'] = (\n",
    "        df_clean['description']\n",
    "        .str.strip()\n",
    "        .str.replace(r'\\s+', ' ', regex=True)  # Multiple spaces to single\n",
    "        .str.replace(r'[!]+', '!', regex=True)  # Multiple ! to single\n",
    "        .str.replace(r'\\.+', '.', regex=True)  # Multiple . to single\n",
    "    )\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "df_text_clean = clean_text_data(df_text)\n",
    "print(\"Cleaned text data:\")\n",
    "print(df_text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional text operations\n",
    "sample_text = pd.Series(['Hello World', 'Python Programming', 'Data Science'])\n",
    "\n",
    "print(\"Original:\", sample_text.tolist())\n",
    "print(\"Upper:\", sample_text.str.upper().tolist())\n",
    "print(\"Lower:\", sample_text.str.lower().tolist())\n",
    "print(\"Length:\", sample_text.str.len().tolist())\n",
    "print(\"Contains 'Python':\", sample_text.str.contains('Python').tolist())\n",
    "print(\"Replace:\", sample_text.str.replace('World', 'Universe').tolist())\n",
    "print(\"Split (first word):\", sample_text.str.split().str[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Date Parsing and Feature Extraction\n",
    "\n",
    "Datetime features can provide valuable information for models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample date data in various formats\n",
    "df_dates = pd.DataFrame({\n",
    "    'date_str': ['2025-01-15', '15/02/2025', 'March 20, 2025', '2025-04-25 14:30:00'],\n",
    "    'timestamp': [1705312800, 1708041600, 1710892800, 1714052400]\n",
    "})\n",
    "print(\"Raw date data:\")\n",
    "print(df_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Parse date strings to datetime objects.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with date columns.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with parsed dates.\n",
    "    \"\"\"\n",
    "    df_parsed = df.copy()\n",
    "    \n",
    "    # Parse date strings (pandas infers format)\n",
    "    df_parsed['date_parsed'] = pd.to_datetime(df_parsed['date_str'], format='mixed')\n",
    "    \n",
    "    # Parse Unix timestamps\n",
    "    df_parsed['timestamp_parsed'] = pd.to_datetime(df_parsed['timestamp'], unit='s')\n",
    "    \n",
    "    return df_parsed\n",
    "\n",
    "df_dates_parsed = parse_dates(df_dates)\n",
    "print(\"Parsed dates:\")\n",
    "print(df_dates_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_features(df: pd.DataFrame, date_col: str) -> pd.DataFrame:\n",
    "    \"\"\"Extract features from a datetime column.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with datetime column.\n",
    "        date_col: Name of the datetime column.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with extracted date features.\n",
    "    \"\"\"\n",
    "    df_features = df.copy()\n",
    "    dt = df[date_col].dt\n",
    "    \n",
    "    # Basic components\n",
    "    df_features['year'] = dt.year\n",
    "    df_features['month'] = dt.month\n",
    "    df_features['day'] = dt.day\n",
    "    df_features['hour'] = dt.hour\n",
    "    \n",
    "    # Derived features\n",
    "    df_features['day_of_week'] = dt.dayofweek  # Monday=0, Sunday=6\n",
    "    df_features['day_name'] = dt.day_name()\n",
    "    df_features['is_weekend'] = dt.dayofweek >= 5\n",
    "    df_features['quarter'] = dt.quarter\n",
    "    df_features['week_of_year'] = dt.isocalendar().week\n",
    "    df_features['day_of_year'] = dt.dayofyear\n",
    "    \n",
    "    # Is it a month start/end?\n",
    "    df_features['is_month_start'] = dt.is_month_start\n",
    "    df_features['is_month_end'] = dt.is_month_end\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "df_date_features = extract_date_features(df_dates_parsed, 'date_parsed')\n",
    "print(\"Extracted date features:\")\n",
    "print(df_date_features[['date_parsed', 'year', 'month', 'day', 'day_name', 'is_weekend', 'quarter']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Binning and Discretisation\n",
    "\n",
    "Converting continuous variables into categorical bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample continuous data\n",
    "np.random.seed(42)\n",
    "df_bins = pd.DataFrame({\n",
    "    'age': np.random.randint(18, 80, 20),\n",
    "    'income': np.random.randint(20000, 150000, 20)\n",
    "})\n",
    "print(\"Continuous data:\")\n",
    "print(df_bins.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equal-width binning with pd.cut\n",
    "df_bins['age_bins_equal'] = pd.cut(df_bins['age'], bins=4, labels=['Young', 'Adult', 'Middle', 'Senior'])\n",
    "print(\"Equal-width binning (age):\")\n",
    "print(df_bins[['age', 'age_bins_equal']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equal-frequency binning with pd.qcut\n",
    "df_bins['income_quantile'] = pd.qcut(df_bins['income'], q=4, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "print(\"Equal-frequency binning (income):\")\n",
    "print(df_bins[['income', 'income_quantile']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom bins\n",
    "age_bins = [0, 25, 35, 50, 65, 100]\n",
    "age_labels = ['Gen Z', 'Millennial', 'Gen X', 'Boomer', 'Silent']\n",
    "df_bins['age_custom'] = pd.cut(df_bins['age'], bins=age_bins, labels=age_labels)\n",
    "print(\"Custom binning:\")\n",
    "print(df_bins[['age', 'age_custom']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn KBinsDiscretizer\n",
    "discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n",
    "df_bins['income_sklearn'] = discretizer.fit_transform(df_bins[['income']])\n",
    "print(\"KBinsDiscretizer (quantile strategy):\")\n",
    "print(df_bins[['income', 'income_sklearn']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Feature Engineering Basics\n",
    "\n",
    "Creating new features from existing data to improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for feature engineering\n",
    "df_eng = pd.DataFrame({\n",
    "    'length': [10, 20, 15, 25, 30],\n",
    "    'width': [5, 10, 8, 12, 15],\n",
    "    'height': [2, 4, 3, 5, 6],\n",
    "    'price': [100, 200, 150, 300, 400],\n",
    "    'quantity': [10, 5, 8, 3, 2]\n",
    "})\n",
    "print(\"Original data:\")\n",
    "print(df_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create new features from existing columns.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with source columns.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with engineered features.\n",
    "    \"\"\"\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Mathematical combinations\n",
    "    df_new['area'] = df['length'] * df['width']\n",
    "    df_new['volume'] = df['length'] * df['width'] * df['height']\n",
    "    \n",
    "    # Ratios\n",
    "    df_new['aspect_ratio'] = df['length'] / df['width']\n",
    "    df_new['price_per_unit'] = df['price'] / df['quantity']\n",
    "    \n",
    "    # Polynomial features\n",
    "    df_new['price_squared'] = df['price'] ** 2\n",
    "    df_new['price_sqrt'] = np.sqrt(df['price'])\n",
    "    \n",
    "    # Log transformation (useful for skewed data)\n",
    "    df_new['price_log'] = np.log1p(df['price'])  # log1p handles zero values\n",
    "    \n",
    "    # Aggregations\n",
    "    df_new['total_value'] = df['price'] * df['quantity']\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "df_engineered = engineer_features(df_eng)\n",
    "print(\"Engineered features:\")\n",
    "print(df_engineered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "features = df_eng[['length', 'width']]\n",
    "poly_features = poly.fit_transform(features)\n",
    "\n",
    "df_poly = pd.DataFrame(\n",
    "    poly_features,\n",
    "    columns=poly.get_feature_names_out(['length', 'width'])\n",
    ")\n",
    "print(\"Polynomial interaction features:\")\n",
    "print(df_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Practice Questions\n",
    "\n",
    "Test your understanding with the following practice questions. Try to solve each problem before revealing the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Missing Value Analysis\n",
    "\n",
    "Given the following DataFrame, write a function that:\n",
    "1. Calculates the percentage of missing values for each column\n",
    "2. Returns only columns with more than 20% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data for Question 1\n",
    "np.random.seed(42)\n",
    "q1_data = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan, 4, 5, np.nan, 7, np.nan, 9, 10],\n",
    "    'B': [np.nan, np.nan, np.nan, 4, 5, np.nan, np.nan, 8, np.nan, 10],\n",
    "    'C': [1, 2, 3, 4, 5, 6, 7, 8, 9, np.nan],\n",
    "    'D': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "})\n",
    "print(\"Question 1 Data:\")\n",
    "print(q1_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "def get_high_missing_columns(df: pd.DataFrame, threshold: float = 20.0) -> pd.Series:\n",
    "    \"\"\"Find columns with missing values above threshold.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to analyse.\n",
    "        threshold: Percentage threshold for missing values.\n",
    "        \n",
    "    Returns:\n",
    "        Series with missing percentages for qualifying columns.\n",
    "    \"\"\"\n",
    "    missing_pct = (df.isnull().sum() / len(df) * 100)\n",
    "    return missing_pct[missing_pct > threshold]\n",
    "\n",
    "result = get_high_missing_columns(q1_data, threshold=20.0)\n",
    "print(\"Columns with >20% missing values:\")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Outlier Detection and Treatment\n",
    "\n",
    "Write a function that:\n",
    "1. Detects outliers using the IQR method\n",
    "2. Replaces outliers with the median value\n",
    "3. Returns the cleaned DataFrame and the number of outliers replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data for Question 2\n",
    "np.random.seed(42)\n",
    "q2_data = pd.DataFrame({\n",
    "    'values': list(np.random.normal(50, 10, 20)) + [150, -30, 200]\n",
    "})\n",
    "print(\"Question 2 Data (with outliers):\")\n",
    "print(q2_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "def replace_outliers_with_median(df: pd.DataFrame, column: str) -> tuple:\n",
    "    \"\"\"Replace outliers with median using IQR method.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the data.\n",
    "        column: Column name to process.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (cleaned DataFrame, number of outliers replaced).\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outlier_mask = (df[column] < lower_bound) | (df[column] > upper_bound)\n",
    "    n_outliers = outlier_mask.sum()\n",
    "    \n",
    "    median = df[column].median()\n",
    "    df_clean.loc[outlier_mask, column] = median\n",
    "    \n",
    "    return df_clean, n_outliers\n",
    "\n",
    "cleaned_df, n_replaced = replace_outliers_with_median(q2_data, 'values')\n",
    "print(f\"Number of outliers replaced: {n_replaced}\")\n",
    "print(\"\\nCleaned data statistics:\")\n",
    "print(cleaned_df.describe())\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Data Type Validation\n",
    "\n",
    "Write a function that validates and converts data types:\n",
    "1. Convert 'age' to integer (handle invalid values)\n",
    "2. Convert 'date' to datetime\n",
    "3. Convert 'price' to float\n",
    "4. Return the number of conversion errors for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data for Question 3\n",
    "q3_data = pd.DataFrame({\n",
    "    'age': ['25', '30', 'invalid', '40', 'N/A'],\n",
    "    'date': ['2025-01-15', '2025-02-20', 'bad_date', '2025-04-10', '2025-05-25'],\n",
    "    'price': ['100.50', '200.75', '300.00', 'free', '500.25']\n",
    "})\n",
    "print(\"Question 3 Data:\")\n",
    "print(q3_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "def validate_and_convert(df: pd.DataFrame) -> tuple:\n",
    "    \"\"\"Validate and convert data types with error tracking.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns to convert.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (converted DataFrame, dictionary of error counts).\n",
    "    \"\"\"\n",
    "    df_converted = df.copy()\n",
    "    errors = {}\n",
    "    \n",
    "    # Age to integer\n",
    "    df_converted['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "    errors['age'] = df_converted['age'].isna().sum()\n",
    "    \n",
    "    # Date to datetime\n",
    "    df_converted['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    errors['date'] = df_converted['date'].isna().sum()\n",
    "    \n",
    "    # Price to float\n",
    "    df_converted['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "    errors['price'] = df_converted['price'].isna().sum()\n",
    "    \n",
    "    return df_converted, errors\n",
    "\n",
    "converted_df, error_counts = validate_and_convert(q3_data)\n",
    "print(\"Converted DataFrame:\")\n",
    "print(converted_df)\n",
    "print(\"\\nConversion errors per column:\")\n",
    "print(error_counts)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Categorical Encoding Pipeline\n",
    "\n",
    "Create a function that:\n",
    "1. Applies one-hot encoding to nominal columns\n",
    "2. Applies ordinal encoding to ordinal columns with specified order\n",
    "3. Returns the encoded DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data for Question 4\n",
    "q4_data = pd.DataFrame({\n",
    "    'city': ['London', 'Paris', 'Berlin', 'London', 'Paris'],\n",
    "    'education': ['Bachelor', 'Master', 'PhD', 'Bachelor', 'Master'],\n",
    "    'colour': ['red', 'blue', 'green', 'red', 'blue']\n",
    "})\n",
    "print(\"Question 4 Data:\")\n",
    "print(q4_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "def encode_categorical_columns(\n",
    "    df: pd.DataFrame,\n",
    "    nominal_cols: list,\n",
    "    ordinal_cols: dict\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Encode categorical columns with appropriate methods.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with categorical columns.\n",
    "        nominal_cols: List of columns for one-hot encoding.\n",
    "        ordinal_cols: Dict mapping column names to ordered categories.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with encoded columns.\n",
    "    \"\"\"\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    # One-hot encode nominal columns\n",
    "    for col in nominal_cols:\n",
    "        dummies = pd.get_dummies(df[col], prefix=col, drop_first=True)\n",
    "        df_encoded = pd.concat([df_encoded, dummies], axis=1)\n",
    "        df_encoded = df_encoded.drop(col, axis=1)\n",
    "    \n",
    "    # Ordinal encode with custom order\n",
    "    for col, order in ordinal_cols.items():\n",
    "        mapping = {cat: i for i, cat in enumerate(order)}\n",
    "        df_encoded[f'{col}_encoded'] = df[col].map(mapping)\n",
    "        df_encoded = df_encoded.drop(col, axis=1)\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "result = encode_categorical_columns(\n",
    "    q4_data,\n",
    "    nominal_cols=['city', 'colour'],\n",
    "    ordinal_cols={'education': ['Bachelor', 'Master', 'PhD']}\n",
    ")\n",
    "print(\"Encoded DataFrame:\")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Feature Scaling Comparison\n",
    "\n",
    "Write a function that:\n",
    "1. Applies StandardScaler, MinMaxScaler, and RobustScaler to the data\n",
    "2. Returns a comparison of the scaled values showing min, max, mean, and std for each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data for Question 5\n",
    "q5_data = pd.DataFrame({\n",
    "    'feature': [10, 20, 30, 100, 50, 60, 70, 80, 90, 1000]  # Note the outlier\n",
    "})\n",
    "print(\"Question 5 Data:\")\n",
    "print(q5_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "def compare_scalers(df: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    \"\"\"Compare different scaling methods.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with data to scale.\n",
    "        column: Column name to scale.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame comparing scaler statistics.\n",
    "    \"\"\"\n",
    "    data = df[[column]]\n",
    "    \n",
    "    scalers = {\n",
    "        'StandardScaler': StandardScaler(),\n",
    "        'MinMaxScaler': MinMaxScaler(),\n",
    "        'RobustScaler': RobustScaler()\n",
    "    }\n",
    "    \n",
    "    results = {'Original': data[column]}\n",
    "    \n",
    "    for name, scaler in scalers.items():\n",
    "        scaled = scaler.fit_transform(data)\n",
    "        results[name] = scaled.flatten()\n",
    "    \n",
    "    comparison = pd.DataFrame(results)\n",
    "    \n",
    "    stats = pd.DataFrame({\n",
    "        'Min': comparison.min(),\n",
    "        'Max': comparison.max(),\n",
    "        'Mean': comparison.mean(),\n",
    "        'Std': comparison.std()\n",
    "    }).round(3)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "comparison_result = compare_scalers(q5_data, 'feature')\n",
    "print(\"Scaler Comparison:\")\n",
    "print(comparison_result)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: Duplicate Detection and Aggregation\n",
    "\n",
    "Write a function that:\n",
    "1. Identifies duplicate entries based on a subset of columns\n",
    "2. Aggregates the duplicates by taking the mean of numerical columns\n",
    "3. Returns the deduplicated DataFrame with aggregated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data for Question 6\n",
    "q6_data = pd.DataFrame({\n",
    "    'customer_id': [1, 2, 1, 3, 2, 1],\n",
    "    'product': ['A', 'B', 'A', 'C', 'B', 'A'],\n",
    "    'quantity': [10, 5, 15, 8, 7, 20],\n",
    "    'price': [100, 200, 100, 150, 200, 100]\n",
    "})\n",
    "print(\"Question 6 Data:\")\n",
    "print(q6_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "def aggregate_duplicates(\n",
    "    df: pd.DataFrame,\n",
    "    group_cols: list,\n",
    "    agg_cols: list\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Aggregate duplicate rows by specified columns.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with potential duplicates.\n",
    "        group_cols: Columns to group by.\n",
    "        agg_cols: Columns to aggregate (mean).\n",
    "        \n",
    "    Returns:\n",
    "        Aggregated DataFrame without duplicates.\n",
    "    \"\"\"\n",
    "    agg_dict = {col: 'mean' for col in agg_cols}\n",
    "    \n",
    "    df_aggregated = df.groupby(group_cols, as_index=False).agg(agg_dict)\n",
    "    \n",
    "    return df_aggregated\n",
    "\n",
    "result = aggregate_duplicates(\n",
    "    q6_data,\n",
    "    group_cols=['customer_id', 'product'],\n",
    "    agg_cols=['quantity', 'price']\n",
    ")\n",
    "print(\"Aggregated DataFrame:\")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: Text Cleaning Pipeline\n",
    "\n",
    "Create a function that cleans text data by:\n",
    "1. Converting to lowercase\n",
    "2. Removing special characters (keep only alphanumeric and spaces)\n",
    "3. Removing extra whitespace\n",
    "4. Removing leading/trailing whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data for Question 7\n",
    "q7_data = pd.DataFrame({\n",
    "    'text': [\n",
    "        '  Hello WORLD!!!  ',\n",
    "        'This   is   a    TEST...',\n",
    "        '@User: Check #this out!!!',\n",
    "        'Email: test@email.com',\n",
    "        '   Multiple   Spaces   Here   '\n",
    "    ]\n",
    "})\n",
    "print(\"Question 7 Data:\")\n",
    "print(q7_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "def clean_text_column(df: pd.DataFrame, column: str) -> pd.Series:\n",
    "    \"\"\"Clean text data in a DataFrame column.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing text data.\n",
    "        column: Name of the text column to clean.\n",
    "        \n",
    "    Returns:\n",
    "        Series with cleaned text.\n",
    "    \"\"\"\n",
    "    cleaned = (\n",
    "        df[column]\n",
    "        .str.lower()  # Convert to lowercase\n",
    "        .str.replace(r'[^a-z0-9\\s]', '', regex=True)  # Remove special characters\n",
    "        .str.replace(r'\\s+', ' ', regex=True)  # Remove extra whitespace\n",
    "        .str.strip()  # Remove leading/trailing whitespace\n",
    "    )\n",
    "    return cleaned\n",
    "\n",
    "q7_data['cleaned_text'] = clean_text_column(q7_data, 'text')\n",
    "print(\"Cleaned text:\")\n",
    "print(q7_data)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8: Date Feature Engineering\n",
    "\n",
    "Write a function that extracts the following features from a date column:\n",
    "1. Year, month, day\n",
    "2. Day of week (as number and name)\n",
    "3. Is weekend (boolean)\n",
    "4. Days since a reference date (e.g., '2020-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data for Question 8\n",
    "q8_data = pd.DataFrame({\n",
    "    'date': pd.to_datetime(['2025-01-15', '2025-03-20', '2025-06-07', '2025-09-13', '2025-12-25'])\n",
    "})\n",
    "print(\"Question 8 Data:\")\n",
    "print(q8_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "def extract_date_features(\n",
    "    df: pd.DataFrame,\n",
    "    date_col: str,\n",
    "    reference_date: str = '2020-01-01'\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Extract features from a datetime column.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with datetime column.\n",
    "        date_col: Name of the datetime column.\n",
    "        reference_date: Reference date for calculating days since.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with extracted date features.\n",
    "    \"\"\"\n",
    "    df_features = df.copy()\n",
    "    dt = df[date_col].dt\n",
    "    ref = pd.to_datetime(reference_date)\n",
    "    \n",
    "    df_features['year'] = dt.year\n",
    "    df_features['month'] = dt.month\n",
    "    df_features['day'] = dt.day\n",
    "    df_features['day_of_week'] = dt.dayofweek\n",
    "    df_features['day_name'] = dt.day_name()\n",
    "    df_features['is_weekend'] = dt.dayofweek >= 5\n",
    "    df_features['days_since_ref'] = (df[date_col] - ref).dt.days\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "result = extract_date_features(q8_data, 'date', '2020-01-01')\n",
    "print(\"Date features:\")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9: Custom Binning with Labels\n",
    "\n",
    "Write a function that:\n",
    "1. Bins a continuous variable into custom ranges\n",
    "2. Assigns meaningful labels to each bin\n",
    "3. Returns the binned column and a summary of the bin distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data for Question 9\n",
    "np.random.seed(42)\n",
    "q9_data = pd.DataFrame({\n",
    "    'income': np.random.randint(15000, 200000, 50)\n",
    "})\n",
    "print(\"Question 9 Data:\")\n",
    "print(q9_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "def create_income_bins(\n",
    "    df: pd.DataFrame,\n",
    "    column: str,\n",
    "    bins: list = None,\n",
    "    labels: list = None\n",
    ") -> tuple:\n",
    "    \"\"\"Bin income data into categories.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with income data.\n",
    "        column: Name of the income column.\n",
    "        bins: List of bin edges.\n",
    "        labels: List of labels for bins.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (binned Series, distribution summary).\n",
    "    \"\"\"\n",
    "    if bins is None:\n",
    "        bins = [0, 30000, 50000, 75000, 100000, float('inf')]\n",
    "    if labels is None:\n",
    "        labels = ['Low', 'Lower-Middle', 'Middle', 'Upper-Middle', 'High']\n",
    "    \n",
    "    binned = pd.cut(df[column], bins=bins, labels=labels)\n",
    "    distribution = binned.value_counts().sort_index()\n",
    "    \n",
    "    return binned, distribution\n",
    "\n",
    "q9_data['income_bracket'], dist = create_income_bins(q9_data, 'income')\n",
    "print(\"Binned data sample:\")\n",
    "print(q9_data.head(10))\n",
    "print(\"\\nDistribution:\")\n",
    "print(dist)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10: Complete Data Preprocessing Pipeline\n",
    "\n",
    "Write a comprehensive preprocessing function that:\n",
    "1. Handles missing values (numeric: median, categorical: mode)\n",
    "2. Removes outliers using IQR method\n",
    "3. Encodes categorical variables (one-hot)\n",
    "4. Scales numeric features (StandardScaler)\n",
    "5. Returns the preprocessed DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data for Question 10\n",
    "np.random.seed(42)\n",
    "q10_data = pd.DataFrame({\n",
    "    'age': [25, np.nan, 35, 40, 150, 30, np.nan, 45, 50, 28],  # 150 is outlier\n",
    "    'income': [50000, 60000, np.nan, 80000, 70000, 55000, 65000, 90000, np.nan, 52000],\n",
    "    'category': ['A', 'B', 'A', None, 'C', 'B', 'A', 'C', 'B', 'A']\n",
    "})\n",
    "print(\"Question 10 Data:\")\n",
    "print(q10_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Complete data preprocessing pipeline.\n",
    "    \n",
    "    Args:\n",
    "        df: Raw DataFrame to preprocess.\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Identify column types\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Step 1: Handle missing values\n",
    "    for col in numeric_cols:\n",
    "        df_processed[col] = df_processed[col].fillna(df_processed[col].median())\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        mode_val = df_processed[col].mode()\n",
    "        if len(mode_val) > 0:\n",
    "            df_processed[col] = df_processed[col].fillna(mode_val[0])\n",
    "    \n",
    "    # Step 2: Remove outliers from numeric columns\n",
    "    for col in numeric_cols:\n",
    "        Q1 = df_processed[col].quantile(0.25)\n",
    "        Q3 = df_processed[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "        df_processed[col] = df_processed[col].clip(lower=lower, upper=upper)\n",
    "    \n",
    "    # Step 3: One-hot encode categorical columns\n",
    "    df_processed = pd.get_dummies(df_processed, columns=categorical_cols, drop_first=True)\n",
    "    \n",
    "    # Step 4: Scale numeric features\n",
    "    scaler = StandardScaler()\n",
    "    df_processed[numeric_cols] = scaler.fit_transform(df_processed[numeric_cols])\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "result = preprocess_data(q10_data)\n",
    "print(\"Preprocessed DataFrame:\")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11: Handling Imbalanced Missing Data\n",
    "\n",
    "Write a function that:\n",
    "1. Analyses the pattern of missing data (MCAR, MAR, MNAR)\n",
    "2. Returns a summary indicating whether missing values appear to be random or correlated with other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data for Question 11\n",
    "np.random.seed(42)\n",
    "n = 100\n",
    "q11_data = pd.DataFrame({\n",
    "    'income': np.random.normal(50000, 15000, n),\n",
    "    'age': np.random.randint(20, 70, n)\n",
    "})\n",
    "# Make missing values correlated with income (high income -> more likely missing age)\n",
    "missing_mask = q11_data['income'] > 60000\n",
    "q11_data.loc[missing_mask, 'age'] = np.where(\n",
    "    np.random.random(missing_mask.sum()) < 0.7,\n",
    "    np.nan,\n",
    "    q11_data.loc[missing_mask, 'age']\n",
    ")\n",
    "print(\"Question 11 Data sample:\")\n",
    "print(q11_data.head(20))\n",
    "print(f\"\\nMissing age values: {q11_data['age'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "def analyse_missing_pattern(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str\n",
    ") -> dict:\n",
    "    \"\"\"Analyse the pattern of missing data.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to analyse.\n",
    "        target_col: Column with missing values to analyse.\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with analysis results.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Create missing indicator\n",
    "    df_analysis = df.copy()\n",
    "    df_analysis['is_missing'] = df[target_col].isna().astype(int)\n",
    "    \n",
    "    # Get numeric columns (excluding target)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if target_col in numeric_cols:\n",
    "        numeric_cols.remove(target_col)\n",
    "    \n",
    "    # Check correlation between missingness and other features\n",
    "    correlations = {}\n",
    "    for col in numeric_cols:\n",
    "        corr = df_analysis[[col, 'is_missing']].corr().iloc[0, 1]\n",
    "        correlations[col] = corr\n",
    "    \n",
    "    results['correlations'] = correlations\n",
    "    \n",
    "    # Compare means of other features for missing vs non-missing\n",
    "    mean_comparison = {}\n",
    "    for col in numeric_cols:\n",
    "        mean_missing = df_analysis[df_analysis['is_missing'] == 1][col].mean()\n",
    "        mean_not_missing = df_analysis[df_analysis['is_missing'] == 0][col].mean()\n",
    "        mean_comparison[col] = {\n",
    "            'mean_when_missing': mean_missing,\n",
    "            'mean_when_present': mean_not_missing,\n",
    "            'difference': mean_missing - mean_not_missing\n",
    "        }\n",
    "    \n",
    "    results['mean_comparison'] = mean_comparison\n",
    "    \n",
    "    # Determine likely pattern\n",
    "    max_corr = max(abs(c) for c in correlations.values()) if correlations else 0\n",
    "    if max_corr < 0.1:\n",
    "        results['likely_pattern'] = 'MCAR (Missing Completely At Random)'\n",
    "    elif max_corr < 0.3:\n",
    "        results['likely_pattern'] = 'Possibly MAR (Missing At Random)'\n",
    "    else:\n",
    "        results['likely_pattern'] = 'Likely MAR or MNAR (Not Missing At Random)'\n",
    "    \n",
    "    return results\n",
    "\n",
    "analysis = analyse_missing_pattern(q11_data, 'age')\n",
    "print(\"Missing data analysis:\")\n",
    "print(f\"\\nCorrelations with missingness: {analysis['correlations']}\")\n",
    "print(f\"\\nMean comparison: {analysis['mean_comparison']}\")\n",
    "print(f\"\\nLikely pattern: {analysis['likely_pattern']}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12: Multi-Column Imputation Strategy\n",
    "\n",
    "Implement a function that:\n",
    "1. Uses KNN imputation for numerical columns\n",
    "2. Uses mode imputation for categorical columns\n",
    "3. Tracks which values were imputed by creating indicator columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data for Question 12\n",
    "q12_data = pd.DataFrame({\n",
    "    'age': [25, 30, np.nan, 40, 35, np.nan, 50, 45],\n",
    "    'income': [50000, np.nan, 70000, 80000, np.nan, 60000, 90000, 75000],\n",
    "    'education': ['Bachelor', 'Master', 'PhD', None, 'Bachelor', 'Master', None, 'PhD'],\n",
    "    'city': ['London', 'Paris', None, 'Berlin', 'London', None, 'Paris', 'Berlin']\n",
    "})\n",
    "print(\"Question 12 Data:\")\n",
    "print(q12_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "def impute_with_tracking(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Impute missing values with tracking indicators.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with missing values.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with imputed values and indicator columns.\n",
    "    \"\"\"\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Create missing indicators\n",
    "    for col in df.columns:\n",
    "        df_result[f'{col}_was_missing'] = df[col].isna().astype(int)\n",
    "    \n",
    "    # KNN imputation for numerical columns\n",
    "    if numeric_cols:\n",
    "        knn_imputer = KNNImputer(n_neighbors=3)\n",
    "        df_result[numeric_cols] = knn_imputer.fit_transform(df[numeric_cols])\n",
    "    \n",
    "    # Mode imputation for categorical columns\n",
    "    for col in categorical_cols:\n",
    "        mode_val = df[col].mode()\n",
    "        if len(mode_val) > 0:\n",
    "            df_result[col] = df_result[col].fillna(mode_val[0])\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "result = impute_with_tracking(q12_data)\n",
    "print(\"Imputed DataFrame with tracking:\")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook covered the essential data cleaning and preprocessing techniques:\n",
    "\n",
    "1. **Missing Values**: `dropna()`, `fillna()`, interpolation, sklearn imputers\n",
    "2. **Outliers**: IQR method, Z-score, capping/removal/imputation\n",
    "3. **Data Types**: Type conversion, handling conversion errors\n",
    "4. **Encoding**: Label encoding, one-hot encoding, ordinal encoding, target encoding\n",
    "5. **Scaling**: StandardScaler, MinMaxScaler, RobustScaler\n",
    "6. **Duplicates**: Detection and removal strategies\n",
    "7. **Text Cleaning**: String operations, regex cleaning\n",
    "8. **Date Features**: Parsing and feature extraction\n",
    "9. **Binning**: Equal-width, equal-frequency, custom bins\n",
    "10. **Feature Engineering**: Mathematical transformations, polynomial features\n",
    "\n",
    "### Key Interview Tips\n",
    "\n",
    "- Always explore your data first before deciding on preprocessing steps\n",
    "- Document your preprocessing decisions and rationale\n",
    "- Consider the impact of preprocessing on model interpretability\n",
    "- Use pipelines in production to ensure consistent preprocessing\n",
    "- Be prepared to explain the trade-offs of different approaches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
