{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Crash Course for Data Science Assessments\n",
    "\n",
    "**Date Created:** 20 January 2026\n",
    "\n",
    "This comprehensive notebook covers essential Pandas concepts commonly tested in data science interviews and assessments. It includes teaching sections with clear explanations, practical examples, and practice questions with solutions.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [DataFrame Creation and Basic Operations](#1-dataframe-creation-and-basic-operations)\n",
    "2. [Indexing and Selection](#2-indexing-and-selection)\n",
    "3. [Merging and Joining](#3-merging-and-joining)\n",
    "4. [GroupBy Operations and Aggregations](#4-groupby-operations-and-aggregations)\n",
    "5. [Pivot Tables and Reshaping](#5-pivot-tables-and-reshaping)\n",
    "6. [Apply, Map, and Lambda Functions](#6-apply-map-and-lambda-functions)\n",
    "7. [Handling Missing Data](#7-handling-missing-data)\n",
    "8. [String Operations](#8-string-operations)\n",
    "9. [Date/Time Operations](#9-datetime-operations)\n",
    "10. [Practice Questions](#10-practice-questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. DataFrame Creation and Basic Operations\n",
    "\n",
    "A **DataFrame** is a two-dimensional, size-mutable, heterogeneous tabular data structure with labelled axes (rows and columns). It is the primary data structure in Pandas.\n",
    "\n",
    "### Creating DataFrames\n",
    "\n",
    "There are multiple ways to create a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: From a dictionary\n",
    "data_dict = {\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'age': [25, 30, 35, 28, 32],\n",
    "    'city': ['London', 'Manchester', 'Birmingham', 'London', 'Leeds'],\n",
    "    'salary': [50000, 60000, 75000, 55000, 65000]\n",
    "}\n",
    "df_from_dict = pd.DataFrame(data_dict)\n",
    "print(\"DataFrame from dictionary:\")\n",
    "print(df_from_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: From a list of dictionaries\n",
    "data_list = [\n",
    "    {'product': 'Laptop', 'price': 999.99, 'quantity': 50},\n",
    "    {'product': 'Mouse', 'price': 29.99, 'quantity': 200},\n",
    "    {'product': 'Keyboard', 'price': 79.99, 'quantity': 150}\n",
    "]\n",
    "df_from_list = pd.DataFrame(data_list)\n",
    "print(\"DataFrame from list of dictionaries:\")\n",
    "print(df_from_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: From a NumPy array with custom index and columns\n",
    "np_array = np.random.randint(1, 100, size=(4, 3))\n",
    "df_from_numpy = pd.DataFrame(\n",
    "    np_array,\n",
    "    index=['row1', 'row2', 'row3', 'row4'],\n",
    "    columns=['A', 'B', 'C']\n",
    ")\n",
    "print(\"DataFrame from NumPy array:\")\n",
    "print(df_from_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample DataFrame for demonstrations\n",
    "employees = pd.DataFrame({\n",
    "    'employee_id': [101, 102, 103, 104, 105, 106, 107, 108],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank', 'Grace', 'Henry'],\n",
    "    'department': ['Engineering', 'Sales', 'Engineering', 'HR', 'Sales', 'Engineering', 'HR', 'Sales'],\n",
    "    'salary': [75000, 55000, 80000, 60000, 52000, 90000, 58000, 61000],\n",
    "    'years_experience': [5, 3, 7, 4, 2, 10, 3, 5],\n",
    "    'hire_date': pd.to_datetime(['2019-03-15', '2021-06-01', '2017-09-20', '2020-01-10', \n",
    "                                  '2022-05-25', '2014-11-08', '2021-08-15', '2019-12-01'])\n",
    "})\n",
    "print(\"Sample Employee DataFrame:\")\n",
    "print(employees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first n rows (default 5)\n",
    "print(\"First 3 rows:\")\n",
    "print(employees.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View last n rows (default 5)\n",
    "print(\"Last 2 rows:\")\n",
    "print(employees.tail(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get shape (rows, columns)\n",
    "print(f\"Shape: {employees.shape}\")\n",
    "print(f\"Number of rows: {employees.shape[0]}\")\n",
    "print(f\"Number of columns: {employees.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column names and data types\n",
    "print(\"Column names:\")\n",
    "print(employees.columns.tolist())\n",
    "print(\"\\nData types:\")\n",
    "print(employees.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary information\n",
    "print(\"DataFrame info:\")\n",
    "employees.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for numerical columns\n",
    "print(\"Descriptive statistics:\")\n",
    "print(employees.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values and value counts\n",
    "print(\"Unique departments:\")\n",
    "print(employees['department'].unique())\n",
    "\n",
    "print(\"\\nDepartment value counts:\")\n",
    "print(employees['department'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Indexing and Selection\n",
    "\n",
    "Pandas provides powerful ways to select and filter data using `.loc[]`, `.iloc[]`, and boolean indexing.\n",
    "\n",
    "### `.loc[]` - Label-based Selection\n",
    "\n",
    "Use `.loc[]` when you want to select by **row/column labels**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set employee_id as index for better demonstration\n",
    "df_indexed = employees.set_index('employee_id')\n",
    "print(\"DataFrame with employee_id as index:\")\n",
    "print(df_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a single row by label\n",
    "print(\"Row for employee 103:\")\n",
    "print(df_indexed.loc[103])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select multiple rows by labels\n",
    "print(\"Rows for employees 101, 103, 105:\")\n",
    "print(df_indexed.loc[[101, 103, 105]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific rows and columns\n",
    "print(\"Name and salary for employees 102-104:\")\n",
    "print(df_indexed.loc[102:104, ['name', 'salary']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.iloc[]` - Integer Position-based Selection\n",
    "\n",
    "Use `.iloc[]` when you want to select by **integer position** (0-indexed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select first row\n",
    "print(\"First row (position 0):\")\n",
    "print(employees.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows 1-3 and columns 0-2\n",
    "print(\"Rows 1-3, columns 0-2:\")\n",
    "print(employees.iloc[1:4, 0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific rows and columns by position\n",
    "print(\"Rows 0, 2, 4 and columns 1, 3:\")\n",
    "print(employees.iloc[[0, 2, 4], [1, 3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean Indexing\n",
    "\n",
    "Filter rows based on conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single condition\n",
    "print(\"Employees with salary > 60000:\")\n",
    "print(employees[employees['salary'] > 60000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple conditions with AND (&)\n",
    "print(\"Engineering employees with salary > 70000:\")\n",
    "print(employees[(employees['department'] == 'Engineering') & (employees['salary'] > 70000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple conditions with OR (|)\n",
    "print(\"Employees in HR or with experience > 5 years:\")\n",
    "print(employees[(employees['department'] == 'HR') | (employees['years_experience'] > 5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using isin() for multiple values\n",
    "print(\"Employees in Engineering or Sales:\")\n",
    "print(employees[employees['department'].isin(['Engineering', 'Sales'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using query() method - more readable for complex conditions\n",
    "print(\"Using query():\")\n",
    "print(employees.query('salary > 60000 and years_experience >= 4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Merging and Joining\n",
    "\n",
    "Pandas provides several methods to combine DataFrames: `merge()`, `join()`, and `concat()`.\n",
    "\n",
    "### Creating Sample DataFrames for Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left DataFrame - Orders\n",
    "orders = pd.DataFrame({\n",
    "    'order_id': [1001, 1002, 1003, 1004, 1005],\n",
    "    'customer_id': ['C001', 'C002', 'C001', 'C003', 'C004'],\n",
    "    'product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headphones'],\n",
    "    'amount': [999.99, 29.99, 79.99, 299.99, 149.99]\n",
    "})\n",
    "print(\"Orders DataFrame:\")\n",
    "print(orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right DataFrame - Customers\n",
    "customers = pd.DataFrame({\n",
    "    'customer_id': ['C001', 'C002', 'C003', 'C005'],\n",
    "    'customer_name': ['John Smith', 'Jane Doe', 'Bob Wilson', 'Alice Brown'],\n",
    "    'city': ['London', 'Manchester', 'Birmingham', 'Leeds']\n",
    "})\n",
    "print(\"Customers DataFrame:\")\n",
    "print(customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `merge()` - SQL-style Joins\n",
    "\n",
    "The `merge()` function provides flexibility with different join types: `inner`, `left`, `right`, and `outer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join - only matching rows\n",
    "inner_merged = pd.merge(orders, customers, on='customer_id', how='inner')\n",
    "print(\"Inner Join (only matching customer_ids):\")\n",
    "print(inner_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join - all rows from left DataFrame\n",
    "left_merged = pd.merge(orders, customers, on='customer_id', how='left')\n",
    "print(\"Left Join (all orders, matching customers):\")\n",
    "print(left_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right join - all rows from right DataFrame\n",
    "right_merged = pd.merge(orders, customers, on='customer_id', how='right')\n",
    "print(\"Right Join (all customers, matching orders):\")\n",
    "print(right_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer join - all rows from both DataFrames\n",
    "outer_merged = pd.merge(orders, customers, on='customer_id', how='outer')\n",
    "print(\"Outer Join (all rows from both):\")\n",
    "print(outer_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on different column names\n",
    "orders_renamed = orders.rename(columns={'customer_id': 'cust_id'})\n",
    "merged_diff_cols = pd.merge(orders_renamed, customers, left_on='cust_id', right_on='customer_id')\n",
    "print(\"Merge with different column names:\")\n",
    "print(merged_diff_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `concat()` - Concatenating DataFrames\n",
    "\n",
    "Use `concat()` to stack DataFrames vertically (axis=0) or horizontally (axis=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two DataFrames to concatenate\n",
    "df1 = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
    "df2 = pd.DataFrame({'A': [5, 6], 'B': [7, 8]})\n",
    "\n",
    "# Vertical concatenation (stacking rows)\n",
    "concat_vertical = pd.concat([df1, df2], ignore_index=True)\n",
    "print(\"Vertical concatenation:\")\n",
    "print(concat_vertical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal concatenation (stacking columns)\n",
    "df3 = pd.DataFrame({'C': [9, 10], 'D': [11, 12]})\n",
    "concat_horizontal = pd.concat([df1, df3], axis=1)\n",
    "print(\"Horizontal concatenation:\")\n",
    "print(concat_horizontal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. GroupBy Operations and Aggregations\n",
    "\n",
    "The `groupby()` function follows a **split-apply-combine** strategy:\n",
    "1. **Split** the data into groups based on criteria\n",
    "2. **Apply** a function to each group independently\n",
    "3. **Combine** the results into a data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sales DataFrame for groupby examples\n",
    "sales = pd.DataFrame({\n",
    "    'date': pd.to_datetime(['2025-01-15', '2025-01-15', '2025-01-16', '2025-01-16',\n",
    "                            '2025-01-17', '2025-01-17', '2025-01-18', '2025-01-18']),\n",
    "    'store': ['London', 'Manchester', 'London', 'Manchester',\n",
    "              'London', 'Manchester', 'London', 'Manchester'],\n",
    "    'product': ['Electronics', 'Clothing', 'Electronics', 'Electronics',\n",
    "                'Clothing', 'Clothing', 'Furniture', 'Furniture'],\n",
    "    'revenue': [1500, 800, 2000, 1200, 600, 900, 3000, 2500],\n",
    "    'units_sold': [10, 20, 15, 8, 15, 25, 5, 4]\n",
    "})\n",
    "print(\"Sales DataFrame:\")\n",
    "print(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic groupby with single aggregation\n",
    "print(\"Total revenue by store:\")\n",
    "print(sales.groupby('store')['revenue'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby with multiple aggregations using agg()\n",
    "print(\"Multiple aggregations by store:\")\n",
    "store_stats = sales.groupby('store').agg({\n",
    "    'revenue': ['sum', 'mean', 'max'],\n",
    "    'units_sold': ['sum', 'mean']\n",
    "})\n",
    "print(store_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named aggregations (cleaner column names)\n",
    "print(\"Named aggregations:\")\n",
    "named_agg = sales.groupby('store').agg(\n",
    "    total_revenue=('revenue', 'sum'),\n",
    "    avg_revenue=('revenue', 'mean'),\n",
    "    total_units=('units_sold', 'sum'),\n",
    "    transaction_count=('revenue', 'count')\n",
    ")\n",
    "print(named_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby multiple columns\n",
    "print(\"Revenue by store and product:\")\n",
    "multi_group = sales.groupby(['store', 'product'])['revenue'].sum().reset_index()\n",
    "print(multi_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom aggregation function\n",
    "def revenue_range(x: pd.Series) -> float:\n",
    "    \"\"\"Calculate the range of revenue values.\n",
    "    \n",
    "    Args:\n",
    "        x: Series of revenue values.\n",
    "    \n",
    "    Returns:\n",
    "        The difference between max and min values.\n",
    "    \"\"\"\n",
    "    return x.max() - x.min()\n",
    "\n",
    "print(\"Revenue range by store:\")\n",
    "print(sales.groupby('store')['revenue'].agg(revenue_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform - apply function and return same-shaped result\n",
    "print(\"Adding normalised revenue column:\")\n",
    "sales['revenue_normalised'] = sales.groupby('store')['revenue'].transform(\n",
    "    lambda x: (x - x.mean()) / x.std()\n",
    ")\n",
    "print(sales[['store', 'revenue', 'revenue_normalised']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Pivot Tables and Reshaping\n",
    "\n",
    "Pivot tables provide a way to summarise and reorganise data, similar to pivot tables in Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the normalised column for cleaner examples\n",
    "sales = sales.drop(columns=['revenue_normalised'])\n",
    "\n",
    "# Basic pivot table\n",
    "print(\"Pivot table - Revenue by store and product:\")\n",
    "pivot_basic = pd.pivot_table(\n",
    "    sales,\n",
    "    values='revenue',\n",
    "    index='store',\n",
    "    columns='product',\n",
    "    aggfunc='sum'\n",
    ")\n",
    "print(pivot_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table with multiple aggregations\n",
    "print(\"Pivot table with sum and mean:\")\n",
    "pivot_multi = pd.pivot_table(\n",
    "    sales,\n",
    "    values='revenue',\n",
    "    index='store',\n",
    "    columns='product',\n",
    "    aggfunc=['sum', 'mean'],\n",
    "    fill_value=0\n",
    ")\n",
    "print(pivot_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table with margins (totals)\n",
    "print(\"Pivot table with totals:\")\n",
    "pivot_margins = pd.pivot_table(\n",
    "    sales,\n",
    "    values='revenue',\n",
    "    index='store',\n",
    "    columns='product',\n",
    "    aggfunc='sum',\n",
    "    margins=True,\n",
    "    margins_name='Total'\n",
    ")\n",
    "print(pivot_margins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping with `melt()` and `stack()/unstack()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create wide format data\n",
    "wide_data = pd.DataFrame({\n",
    "    'name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'maths': [85, 90, 78],\n",
    "    'english': [88, 82, 95],\n",
    "    'science': [92, 88, 85]\n",
    "})\n",
    "print(\"Wide format:\")\n",
    "print(wide_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt - wide to long format\n",
    "long_data = pd.melt(\n",
    "    wide_data,\n",
    "    id_vars=['name'],\n",
    "    value_vars=['maths', 'english', 'science'],\n",
    "    var_name='subject',\n",
    "    value_name='score'\n",
    ")\n",
    "print(\"Long format (melted):\")\n",
    "print(long_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot - long to wide format\n",
    "back_to_wide = long_data.pivot(index='name', columns='subject', values='score')\n",
    "print(\"Back to wide format (pivoted):\")\n",
    "print(back_to_wide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Apply, Map, and Lambda Functions\n",
    "\n",
    "These functions allow you to apply custom transformations to your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame\n",
    "df_apply = pd.DataFrame({\n",
    "    'name': ['alice smith', 'bob jones', 'charlie brown'],\n",
    "    'salary': [50000, 60000, 75000],\n",
    "    'bonus_pct': [0.10, 0.15, 0.12]\n",
    "})\n",
    "print(\"Original DataFrame:\")\n",
    "print(df_apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply() on a Series - applies function to each element\n",
    "df_apply['name_title'] = df_apply['name'].apply(str.title)\n",
    "print(\"After applying str.title:\")\n",
    "print(df_apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply() with lambda function\n",
    "df_apply['total_compensation'] = df_apply.apply(\n",
    "    lambda row: row['salary'] * (1 + row['bonus_pct']),\n",
    "    axis=1\n",
    ")\n",
    "print(\"With total compensation:\")\n",
    "print(df_apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map() - for Series, maps values using a dictionary or function\n",
    "salary_bands = {\n",
    "    50000: 'Junior',\n",
    "    60000: 'Mid',\n",
    "    75000: 'Senior'\n",
    "}\n",
    "df_apply['band'] = df_apply['salary'].map(salary_bands)\n",
    "print(\"With salary bands mapped:\")\n",
    "print(df_apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function with apply\n",
    "def categorise_compensation(total_comp: float) -> str:\n",
    "    \"\"\"Categorise total compensation into bands.\n",
    "    \n",
    "    Args:\n",
    "        total_comp: The total compensation value.\n",
    "    \n",
    "    Returns:\n",
    "        Category string based on compensation level.\n",
    "    \"\"\"\n",
    "    if total_comp < 60000:\n",
    "        return 'Low'\n",
    "    elif total_comp < 75000:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "df_apply['comp_category'] = df_apply['total_compensation'].apply(categorise_compensation)\n",
    "print(\"With compensation category:\")\n",
    "print(df_apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applymap() / map() on DataFrame - applies to every element\n",
    "# Note: applymap() is deprecated in newer pandas, use map() instead\n",
    "numeric_df = pd.DataFrame({'A': [1.5, 2.3, 3.7], 'B': [4.2, 5.8, 6.1]})\n",
    "rounded_df = numeric_df.map(lambda x: round(x))\n",
    "print(\"Rounded DataFrame:\")\n",
    "print(rounded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Handling Missing Data\n",
    "\n",
    "Missing data is common in real-world datasets. Pandas uses `NaN` (Not a Number) and `None` to represent missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with missing values\n",
    "df_missing = pd.DataFrame({\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'age': [25, np.nan, 35, 28, np.nan],\n",
    "    'city': ['London', 'Manchester', None, 'London', 'Leeds'],\n",
    "    'salary': [50000, 60000, np.nan, np.nan, 65000]\n",
    "})\n",
    "print(\"DataFrame with missing values:\")\n",
    "print(df_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Is null (boolean mask):\")\n",
    "print(df_missing.isnull())\n",
    "\n",
    "print(\"\\nCount of missing values per column:\")\n",
    "print(df_missing.isnull().sum())\n",
    "\n",
    "print(\"\\nPercentage of missing values:\")\n",
    "print((df_missing.isnull().sum() / len(df_missing)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any missing values\n",
    "print(\"Drop rows with any NaN:\")\n",
    "print(df_missing.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where specific column is missing\n",
    "print(\"Drop rows where 'age' is NaN:\")\n",
    "print(df_missing.dropna(subset=['age']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with a constant\n",
    "print(\"Fill all NaN with 'Unknown':\")\n",
    "print(df_missing.fillna('Unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with different values per column\n",
    "fill_values = {\n",
    "    'age': df_missing['age'].mean(),\n",
    "    'city': 'Unknown',\n",
    "    'salary': df_missing['salary'].median()\n",
    "}\n",
    "df_filled = df_missing.fillna(fill_values)\n",
    "print(\"Fill with column-specific values:\")\n",
    "print(df_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward fill (propagate last valid value)\n",
    "print(\"Forward fill:\")\n",
    "print(df_missing.ffill())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward fill (propagate next valid value)\n",
    "print(\"Backward fill:\")\n",
    "print(df_missing.bfill())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate missing values (for numeric columns)\n",
    "df_numeric = pd.DataFrame({'values': [1.0, np.nan, np.nan, 4.0, 5.0, np.nan, 7.0]})\n",
    "print(\"Original:\")\n",
    "print(df_numeric)\n",
    "print(\"\\nInterpolated:\")\n",
    "print(df_numeric.interpolate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. String Operations\n",
    "\n",
    "Pandas provides vectorised string operations through the `.str` accessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with string data\n",
    "df_str = pd.DataFrame({\n",
    "    'full_name': ['  John Smith  ', 'jane doe', 'BOB WILSON', 'Alice Brown-Jones'],\n",
    "    'email': ['john@example.com', 'jane@test.co.uk', 'bob@sample.org', 'alice@demo.com'],\n",
    "    'phone': ['020-1234-5678', '0161-987-6543', '0121-555-1234', '0113-222-3333']\n",
    "})\n",
    "print(\"Original DataFrame:\")\n",
    "print(df_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case transformations\n",
    "print(\"Lowercase:\")\n",
    "print(df_str['full_name'].str.lower())\n",
    "\n",
    "print(\"\\nUppercase:\")\n",
    "print(df_str['full_name'].str.upper())\n",
    "\n",
    "print(\"\\nTitle case:\")\n",
    "print(df_str['full_name'].str.title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip whitespace\n",
    "print(\"Stripped whitespace:\")\n",
    "print(df_str['full_name'].str.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split strings\n",
    "df_str['first_name'] = df_str['full_name'].str.strip().str.split().str[0]\n",
    "df_str['last_name'] = df_str['full_name'].str.strip().str.split().str[-1]\n",
    "print(\"After splitting names:\")\n",
    "print(df_str[['full_name', 'first_name', 'last_name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contains check (returns boolean)\n",
    "print(\"Emails containing 'example':\")\n",
    "print(df_str[df_str['email'].str.contains('example')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace strings\n",
    "print(\"Replace '-' with space in phone:\")\n",
    "print(df_str['phone'].str.replace('-', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract using regex\n",
    "print(\"Extract email domain:\")\n",
    "df_str['domain'] = df_str['email'].str.extract(r'@(.+)$')\n",
    "print(df_str[['email', 'domain']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String length\n",
    "print(\"Email lengths:\")\n",
    "print(df_str['email'].str.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starts with / Ends with\n",
    "print(\"Names starting with 'J':\")\n",
    "print(df_str[df_str['full_name'].str.strip().str.lower().str.startswith('j')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Date/Time Operations\n",
    "\n",
    "Pandas has excellent support for working with dates and times through the `datetime64` dtype and `.dt` accessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with datetime data\n",
    "df_datetime = pd.DataFrame({\n",
    "    'event': ['Meeting', 'Conference', 'Workshop', 'Presentation', 'Review'],\n",
    "    'date_str': ['2025-01-15', '2025-02-20', '2025-03-10', '2025-04-05', '2025-05-25'],\n",
    "    'timestamp': pd.to_datetime(['2025-01-15 09:30:00', '2025-02-20 14:00:00',\n",
    "                                  '2025-03-10 10:00:00', '2025-04-05 15:30:00',\n",
    "                                  '2025-05-25 11:00:00'])\n",
    "})\n",
    "print(\"DataFrame with datetime:\")\n",
    "print(df_datetime)\n",
    "print(\"\\nData types:\")\n",
    "print(df_datetime.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string to datetime\n",
    "df_datetime['date'] = pd.to_datetime(df_datetime['date_str'])\n",
    "print(\"After conversion:\")\n",
    "print(df_datetime.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract datetime components\n",
    "df_datetime['year'] = df_datetime['timestamp'].dt.year\n",
    "df_datetime['month'] = df_datetime['timestamp'].dt.month\n",
    "df_datetime['day'] = df_datetime['timestamp'].dt.day\n",
    "df_datetime['hour'] = df_datetime['timestamp'].dt.hour\n",
    "df_datetime['day_name'] = df_datetime['timestamp'].dt.day_name()\n",
    "df_datetime['month_name'] = df_datetime['timestamp'].dt.month_name()\n",
    "\n",
    "print(\"Extracted components:\")\n",
    "print(df_datetime[['event', 'timestamp', 'year', 'month', 'day', 'hour', 'day_name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date arithmetic\n",
    "df_datetime['days_from_now'] = (df_datetime['timestamp'] - pd.Timestamp.now()).dt.days\n",
    "print(\"Days from now:\")\n",
    "print(df_datetime[['event', 'timestamp', 'days_from_now']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by date range\n",
    "start_date = '2025-02-01'\n",
    "end_date = '2025-04-30'\n",
    "mask = (df_datetime['timestamp'] >= start_date) & (df_datetime['timestamp'] <= end_date)\n",
    "print(f\"Events between {start_date} and {end_date}:\")\n",
    "print(df_datetime.loc[mask, ['event', 'timestamp']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create date range\n",
    "date_range = pd.date_range(start='2025-01-01', periods=7, freq='D')\n",
    "print(\"Date range (7 days):\")\n",
    "print(date_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample time series data\n",
    "ts_data = pd.DataFrame({\n",
    "    'date': pd.date_range(start='2025-01-01', periods=30, freq='D'),\n",
    "    'sales': np.random.randint(100, 500, 30)\n",
    "})\n",
    "ts_data.set_index('date', inplace=True)\n",
    "\n",
    "print(\"Weekly sum of sales:\")\n",
    "print(ts_data.resample('W').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling window calculations\n",
    "ts_data['rolling_mean_7d'] = ts_data['sales'].rolling(window=7).mean()\n",
    "print(\"With 7-day rolling mean:\")\n",
    "print(ts_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Practice Questions\n",
    "\n",
    "Test your Pandas skills with these practice questions. Each question has a hidden solution that you can reveal after attempting it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample datasets for practice questions\n",
    "np.random.seed(42)\n",
    "\n",
    "# Employee dataset\n",
    "employees_practice = pd.DataFrame({\n",
    "    'employee_id': range(1, 21),\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank', 'Grace', 'Henry', \n",
    "             'Ivy', 'Jack', 'Kate', 'Leo', 'Mia', 'Noah', 'Olivia', 'Peter', \n",
    "             'Quinn', 'Rose', 'Sam', 'Tina'],\n",
    "    'department': np.random.choice(['Engineering', 'Sales', 'HR', 'Marketing', 'Finance'], 20),\n",
    "    'salary': np.random.randint(40000, 100000, 20),\n",
    "    'years_experience': np.random.randint(1, 15, 20),\n",
    "    'hire_date': pd.date_range(start='2015-01-01', periods=20, freq='120D'),\n",
    "    'performance_score': np.random.choice([np.nan, 3.0, 3.5, 4.0, 4.5, 5.0], 20)\n",
    "})\n",
    "\n",
    "# Transactions dataset\n",
    "transactions = pd.DataFrame({\n",
    "    'transaction_id': range(1001, 1031),\n",
    "    'employee_id': np.random.choice(range(1, 21), 30),\n",
    "    'amount': np.random.uniform(50, 5000, 30).round(2),\n",
    "    'category': np.random.choice(['Travel', 'Equipment', 'Training', 'Supplies', 'Software'], 30),\n",
    "    'transaction_date': pd.date_range(start='2025-01-01', periods=30, freq='D')\n",
    "})\n",
    "\n",
    "print(\"Employees Dataset:\")\n",
    "print(employees_practice.head())\n",
    "print(f\"\\nShape: {employees_practice.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nTransactions Dataset:\")\n",
    "print(transactions.head())\n",
    "print(f\"\\nShape: {transactions.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Basic Filtering\n",
    "\n",
    "Find all employees in the Engineering department who have more than 5 years of experience. Display only the `name`, `department`, `salary`, and `years_experience` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "result = employees_practice[\n",
    "    (employees_practice['department'] == 'Engineering') & \n",
    "    (employees_practice['years_experience'] > 5)\n",
    "][['name', 'department', 'salary', 'years_experience']]\n",
    "print(result)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: GroupBy with Multiple Aggregations\n",
    "\n",
    "Calculate the following statistics for each department:\n",
    "- Total number of employees\n",
    "- Average salary\n",
    "- Maximum years of experience\n",
    "- Minimum salary\n",
    "\n",
    "Sort the results by average salary in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "dept_stats = employees_practice.groupby('department').agg(\n",
    "    employee_count=('employee_id', 'count'),\n",
    "    avg_salary=('salary', 'mean'),\n",
    "    max_experience=('years_experience', 'max'),\n",
    "    min_salary=('salary', 'min')\n",
    ").sort_values('avg_salary', ascending=False)\n",
    "\n",
    "print(dept_stats)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Merging DataFrames\n",
    "\n",
    "Merge the `employees_practice` and `transactions` DataFrames to show each transaction along with the employee's name and department. Include all transactions even if the employee information is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "merged_data = pd.merge(\n",
    "    transactions,\n",
    "    employees_practice[['employee_id', 'name', 'department']],\n",
    "    on='employee_id',\n",
    "    how='left'\n",
    ")\n",
    "print(merged_data.head(10))\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Pivot Table\n",
    "\n",
    "Create a pivot table showing the total transaction amount for each department and category combination. Include row and column totals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "# First merge to get department info\n",
    "merged_for_pivot = pd.merge(\n",
    "    transactions,\n",
    "    employees_practice[['employee_id', 'department']],\n",
    "    on='employee_id'\n",
    ")\n",
    "\n",
    "# Create pivot table\n",
    "pivot_result = pd.pivot_table(\n",
    "    merged_for_pivot,\n",
    "    values='amount',\n",
    "    index='department',\n",
    "    columns='category',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0,\n",
    "    margins=True,\n",
    "    margins_name='Total'\n",
    ")\n",
    "print(pivot_result.round(2))\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Handling Missing Data\n",
    "\n",
    "In the `employees_practice` DataFrame:\n",
    "1. Count how many employees have missing `performance_score` values\n",
    "2. Fill the missing `performance_score` values with the average score of their respective department\n",
    "3. Show the employees who had missing scores (before and after filling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "# Count missing values\n",
    "missing_count = employees_practice['performance_score'].isnull().sum()\n",
    "print(f\"Number of missing performance scores: {missing_count}\")\n",
    "\n",
    "# Identify employees with missing scores\n",
    "missing_mask = employees_practice['performance_score'].isnull()\n",
    "print(\"\\nEmployees with missing scores:\")\n",
    "print(employees_practice.loc[missing_mask, ['name', 'department', 'performance_score']])\n",
    "\n",
    "# Fill with department average using transform\n",
    "employees_filled = employees_practice.copy()\n",
    "employees_filled['performance_score'] = employees_filled.groupby('department')['performance_score'].transform(\n",
    "    lambda x: x.fillna(x.mean())\n",
    ")\n",
    "\n",
    "print(\"\\nAfter filling with department averages:\")\n",
    "print(employees_filled.loc[missing_mask, ['name', 'department', 'performance_score']])\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: Apply and Lambda Functions\n",
    "\n",
    "Create a new column called `salary_band` in the `employees_practice` DataFrame that categorises employees as:\n",
    "- 'Junior' if salary < 50000\n",
    "- 'Mid' if salary >= 50000 and < 70000\n",
    "- 'Senior' if salary >= 70000 and < 85000\n",
    "- 'Lead' if salary >= 85000\n",
    "\n",
    "Then show the count of employees in each salary band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "def assign_salary_band(salary: int) -> str:\n",
    "    \"\"\"Assign a salary band based on salary value.\n",
    "    \n",
    "    Args:\n",
    "        salary: The employee's salary.\n",
    "    \n",
    "    Returns:\n",
    "        The salary band category.\n",
    "    \"\"\"\n",
    "    if salary < 50000:\n",
    "        return 'Junior'\n",
    "    elif salary < 70000:\n",
    "        return 'Mid'\n",
    "    elif salary < 85000:\n",
    "        return 'Senior'\n",
    "    else:\n",
    "        return 'Lead'\n",
    "\n",
    "employees_practice['salary_band'] = employees_practice['salary'].apply(assign_salary_band)\n",
    "\n",
    "print(\"Salary band distribution:\")\n",
    "print(employees_practice['salary_band'].value_counts())\n",
    "\n",
    "print(\"\\nSample of results:\")\n",
    "print(employees_practice[['name', 'salary', 'salary_band']].head(10))\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: String Operations\n",
    "\n",
    "Using the transactions DataFrame, create a new column `category_code` that contains the first 3 letters of the category in uppercase. Then filter for transactions where the category code starts with 'TR'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "transactions['category_code'] = transactions['category'].str[:3].str.upper()\n",
    "\n",
    "print(\"All transactions with category codes:\")\n",
    "print(transactions[['transaction_id', 'category', 'category_code']].head(10))\n",
    "\n",
    "print(\"\\nTransactions where category starts with 'TR':\")\n",
    "tr_transactions = transactions[transactions['category_code'].str.startswith('TR')]\n",
    "print(tr_transactions[['transaction_id', 'category', 'category_code', 'amount']])\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8: Date/Time Operations\n",
    "\n",
    "Using the `employees_practice` DataFrame:\n",
    "1. Calculate how many days each employee has been with the company (from their hire date to today)\n",
    "2. Find employees who were hired in the first quarter (January-March) of any year\n",
    "3. Calculate the average tenure (in years) by department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "# Calculate days with company\n",
    "today = pd.Timestamp.now()\n",
    "employees_practice['days_employed'] = (today - employees_practice['hire_date']).dt.days\n",
    "employees_practice['years_employed'] = employees_practice['days_employed'] / 365.25\n",
    "\n",
    "print(\"Employee tenure:\")\n",
    "print(employees_practice[['name', 'hire_date', 'days_employed', 'years_employed']].head())\n",
    "\n",
    "# Find Q1 hires\n",
    "q1_hires = employees_practice[employees_practice['hire_date'].dt.month.isin([1, 2, 3])]\n",
    "print(\"\\nEmployees hired in Q1:\")\n",
    "print(q1_hires[['name', 'hire_date']])\n",
    "\n",
    "# Average tenure by department\n",
    "avg_tenure = employees_practice.groupby('department')['years_employed'].mean().round(2)\n",
    "print(\"\\nAverage tenure by department (years):\")\n",
    "print(avg_tenure.sort_values(ascending=False))\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9: Complex Aggregation\n",
    "\n",
    "For each employee, calculate:\n",
    "1. Total transaction amount\n",
    "2. Number of transactions\n",
    "3. Average transaction amount\n",
    "4. Most frequent transaction category\n",
    "\n",
    "Then join this with the employee information to create a comprehensive view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "# Aggregate transaction data\n",
    "transaction_summary = transactions.groupby('employee_id').agg(\n",
    "    total_amount=('amount', 'sum'),\n",
    "    transaction_count=('transaction_id', 'count'),\n",
    "    avg_amount=('amount', 'mean'),\n",
    "    most_frequent_category=('category', lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else None)\n",
    ").reset_index()\n",
    "\n",
    "# Round the monetary values\n",
    "transaction_summary['total_amount'] = transaction_summary['total_amount'].round(2)\n",
    "transaction_summary['avg_amount'] = transaction_summary['avg_amount'].round(2)\n",
    "\n",
    "# Merge with employee data\n",
    "employee_transactions = pd.merge(\n",
    "    employees_practice[['employee_id', 'name', 'department', 'salary']],\n",
    "    transaction_summary,\n",
    "    on='employee_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"Employee Transaction Summary:\")\n",
    "print(employee_transactions)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10: Data Transformation Challenge\n",
    "\n",
    "Create a summary report that shows:\n",
    "1. For each department and salary band combination, show the count of employees and average performance score\n",
    "2. Add a column showing what percentage of the department falls into each salary band\n",
    "3. Sort by department and then by salary band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "# Ensure salary_band column exists\n",
    "if 'salary_band' not in employees_practice.columns:\n",
    "    employees_practice['salary_band'] = employees_practice['salary'].apply(\n",
    "        lambda x: 'Junior' if x < 50000 else ('Mid' if x < 70000 else ('Senior' if x < 85000 else 'Lead'))\n",
    "    )\n",
    "\n",
    "# Fill missing performance scores for this analysis\n",
    "df_analysis = employees_practice.copy()\n",
    "df_analysis['performance_score'] = df_analysis.groupby('department')['performance_score'].transform(\n",
    "    lambda x: x.fillna(x.mean())\n",
    ")\n",
    "\n",
    "# Group by department and salary band\n",
    "summary = df_analysis.groupby(['department', 'salary_band']).agg(\n",
    "    employee_count=('employee_id', 'count'),\n",
    "    avg_performance=('performance_score', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate department totals for percentage\n",
    "dept_totals = summary.groupby('department')['employee_count'].transform('sum')\n",
    "summary['pct_of_department'] = (summary['employee_count'] / dept_totals * 100).round(1)\n",
    "\n",
    "# Round performance score\n",
    "summary['avg_performance'] = summary['avg_performance'].round(2)\n",
    "\n",
    "# Sort by department and salary band\n",
    "band_order = ['Junior', 'Mid', 'Senior', 'Lead']\n",
    "summary['salary_band'] = pd.Categorical(summary['salary_band'], categories=band_order, ordered=True)\n",
    "summary = summary.sort_values(['department', 'salary_band'])\n",
    "\n",
    "print(\"Department and Salary Band Summary:\")\n",
    "print(summary.to_string(index=False))\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11: Window Functions\n",
    "\n",
    "Using the transactions DataFrame:\n",
    "1. Add a column showing the running total of transaction amounts (cumulative sum)\n",
    "2. Add a column showing the 3-day rolling average of transaction amounts\n",
    "3. Add a column showing the rank of each transaction by amount within each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "# Sort by date first\n",
    "transactions_sorted = transactions.sort_values('transaction_date').copy()\n",
    "\n",
    "# Running total (cumulative sum)\n",
    "transactions_sorted['running_total'] = transactions_sorted['amount'].cumsum().round(2)\n",
    "\n",
    "# 3-day rolling average\n",
    "transactions_sorted['rolling_avg_3d'] = transactions_sorted['amount'].rolling(window=3).mean().round(2)\n",
    "\n",
    "# Rank within each category\n",
    "transactions_sorted['rank_in_category'] = transactions_sorted.groupby('category')['amount'].rank(\n",
    "    method='dense', ascending=False\n",
    ").astype(int)\n",
    "\n",
    "print(\"Transactions with window functions:\")\n",
    "print(transactions_sorted[['transaction_date', 'category', 'amount', \n",
    "                           'running_total', 'rolling_avg_3d', 'rank_in_category']].head(15))\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12: Data Cleaning Challenge\n",
    "\n",
    "Given the following messy DataFrame, clean it by:\n",
    "1. Standardising the name format (title case, stripped whitespace)\n",
    "2. Fixing the salary column (remove currency symbols and commas, convert to numeric)\n",
    "3. Standardising the department names (consistent capitalisation)\n",
    "4. Converting the date column to datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Messy DataFrame to clean\n",
    "messy_data = pd.DataFrame({\n",
    "    'name': ['  john smith  ', 'JANE DOE', 'bob Wilson', '  alice BROWN  '],\n",
    "    'salary': ['50,000', '65,000', '45000', '72,500'],\n",
    "    'department': ['engineering', 'SALES', 'Engineering', 'sales'],\n",
    "    'start_date': ['01/03/2020', '15-06-2021', '2022/01/10', '01-12-2019']\n",
    "})\n",
    "print(\"Messy data:\")\n",
    "print(messy_data)\n",
    "\n",
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "clean_data = messy_data.copy()\n",
    "\n",
    "# Standardise names\n",
    "clean_data['name'] = clean_data['name'].str.strip().str.title()\n",
    "\n",
    "# Fix salary - remove  and commas, convert to numeric\n",
    "clean_data['salary'] = clean_data['salary'].str.replace('', '', regex=False)\n",
    "clean_data['salary'] = clean_data['salary'].str.replace(',', '', regex=False)\n",
    "clean_data['salary'] = pd.to_numeric(clean_data['salary'])\n",
    "\n",
    "# Standardise department names\n",
    "clean_data['department'] = clean_data['department'].str.strip().str.title()\n",
    "\n",
    "# Convert dates - handle mixed formats\n",
    "clean_data['start_date'] = pd.to_datetime(clean_data['start_date'], dayfirst=True)\n",
    "\n",
    "print(\"Cleaned data:\")\n",
    "print(clean_data)\n",
    "print(\"\\nData types:\")\n",
    "print(clean_data.dtypes)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook covered the essential Pandas concepts commonly tested in data science assessments:\n",
    "\n",
    "1. **DataFrame Creation** - Multiple methods to create DataFrames from dictionaries, lists, and arrays\n",
    "2. **Indexing and Selection** - Using `.loc[]`, `.iloc[]`, and boolean indexing for data access\n",
    "3. **Merging and Joining** - Combining DataFrames with `merge()`, `join()`, and `concat()`\n",
    "4. **GroupBy Operations** - Split-apply-combine pattern with aggregations\n",
    "5. **Pivot Tables** - Reshaping data for analysis and reporting\n",
    "6. **Apply and Lambda** - Custom transformations on data\n",
    "7. **Missing Data** - Detection, removal, and imputation strategies\n",
    "8. **String Operations** - Vectorised string manipulation with `.str` accessor\n",
    "9. **Date/Time Operations** - Working with temporal data using `.dt` accessor\n",
    "\n",
    "### Key Tips for Assessments\n",
    "\n",
    "- Always check data types with `.dtypes` before operations\n",
    "- Use `.shape` and `.info()` to understand your data structure\n",
    "- Prefer vectorised operations over loops for better performance\n",
    "- Use `copy()` when you need to modify a DataFrame without affecting the original\n",
    "- Remember that `.loc[]` uses labels and `.iloc[]` uses integer positions\n",
    "- Use parentheses around each condition in boolean indexing with `&` and `|`\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Pandas Official Documentation](https://pandas.pydata.org/docs/)\n",
    "- [Pandas Cheat Sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
