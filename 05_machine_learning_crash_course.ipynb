{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Scikit-Learn Crash Course\n",
    "\n",
    "**Date:** January 2026\n",
    "\n",
    "This notebook provides a comprehensive overview of machine learning concepts using scikit-learn, designed for data science assessment preparation.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Imports](#1-setup-and-imports)\n",
    "2. [Train/Test Split and Cross-Validation](#2-traintestsplit-and-cross-validation)\n",
    "3. [Classification Algorithms](#3-classification-algorithms)\n",
    "4. [Regression Algorithms](#4-regression-algorithms)\n",
    "5. [Model Evaluation Metrics](#5-model-evaluation-metrics)\n",
    "6. [Confusion Matrix Interpretation](#6-confusion-matrix-interpretation)\n",
    "7. [Feature Scaling](#7-feature-scaling)\n",
    "8. [Feature Selection Techniques](#8-feature-selection-techniques)\n",
    "9. [Hyperparameter Tuning](#9-hyperparameter-tuning)\n",
    "10. [Pipelines in Scikit-Learn](#10-pipelines-in-scikit-learn)\n",
    "11. [Handling Imbalanced Datasets](#11-handling-imbalanced-datasets)\n",
    "12. [Overfitting and Underfitting](#12-overfitting-and-underfitting)\n",
    "13. [Practice Questions](#13-practice-questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import all the necessary libraries we'll use throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, List, Dict, Any\n",
    "\n",
    "# Datasets\n",
    "from sklearn.datasets import load_iris, load_wine, load_breast_cancer, load_diabetes, make_classification, make_regression\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, SelectFromModel\n",
    "\n",
    "# Classification Models\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Regression Models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix, classification_report,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train/Test Split and Cross-Validation\n",
    "\n",
    "### Why Split Data?\n",
    "We split data to evaluate how well our model generalises to unseen data. Training on all data and testing on the same data leads to overfitting.\n",
    "\n",
    "### Train/Test Split\n",
    "The simplest approach: divide data into training set (typically 70-80%) and test set (20-30%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Feature names: {iris.feature_names}\")\n",
    "print(f\"Target names: {iris.target_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Split data into training and testing sets.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix.\n",
    "        y: Target vector.\n",
    "        test_size: Proportion of data for testing.\n",
    "        random_state: Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (X_train, X_test, y_train, y_test).\n",
    "    \"\"\"\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(X, y)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(f\"Training set class distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Test set class distribution: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "\n",
    "Cross-validation provides a more robust estimate of model performance by training and evaluating on multiple subsets of the data.\n",
    "\n",
    "**K-Fold Cross-Validation:**\n",
    "- Splits data into K equal parts (folds)\n",
    "- Trains on K-1 folds, tests on remaining fold\n",
    "- Repeats K times, each fold serving as test set once\n",
    "- Final score is average of all K iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_cross_validation(\n",
    "    model: Any,\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    cv: int = 5,\n",
    "    scoring: str = 'accuracy'\n",
    ") -> Tuple[np.ndarray, float, float]:\n",
    "    \"\"\"Perform k-fold cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        model: Scikit-learn estimator.\n",
    "        X: Feature matrix.\n",
    "        y: Target vector.\n",
    "        cv: Number of folds.\n",
    "        scoring: Scoring metric.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (scores array, mean score, standard deviation).\n",
    "    \"\"\"\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)\n",
    "    return scores, scores.mean(), scores.std()\n",
    "\n",
    "# Example with Logistic Regression\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "scores, mean_score, std_score = perform_cross_validation(model, X, y)\n",
    "\n",
    "print(f\"Cross-validation scores: {scores}\")\n",
    "print(f\"Mean accuracy: {mean_score:.4f} (+/- {std_score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K-Fold (maintains class distribution in each fold)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold_scores = []\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "    y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "    \n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "    score = model.score(X_val_fold, y_val_fold)\n",
    "    fold_scores.append(score)\n",
    "    print(f\"Fold {fold}: Accuracy = {score:.4f}\")\n",
    "\n",
    "print(f\"\\nMean accuracy: {np.mean(fold_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification Algorithms\n",
    "\n",
    "Classification predicts discrete class labels. Let's explore the main algorithms using the Iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 K-Nearest Neighbours (KNN)\n",
    "\n",
    "KNN classifies a sample based on the majority class of its K nearest neighbours.\n",
    "\n",
    "**Key Hyperparameters:**\n",
    "- `n_neighbors`: Number of neighbours to consider\n",
    "- `weights`: 'uniform' (all neighbours equal) or 'distance' (closer neighbours weighted more)\n",
    "- `metric`: Distance metric (euclidean, manhattan, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_knn(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    n_neighbors: int = 5,\n",
    "    weights: str = 'uniform'\n",
    ") -> KNeighborsClassifier:\n",
    "    \"\"\"Train a K-Nearest Neighbours classifier.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features.\n",
    "        y_train: Training labels.\n",
    "        n_neighbors: Number of neighbours.\n",
    "        weights: Weight function for prediction.\n",
    "    \n",
    "    Returns:\n",
    "        Trained KNN classifier.\n",
    "    \"\"\"\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights)\n",
    "    knn.fit(X_train, y_train)\n",
    "    return knn\n",
    "\n",
    "# Scale features (important for KNN)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "knn = train_knn(X_train_scaled, y_train, n_neighbors=5)\n",
    "y_pred_knn = knn.predict(X_test_scaled)\n",
    "\n",
    "print(f\"KNN Accuracy: {accuracy_score(y_test, y_pred_knn):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Logistic Regression\n",
    "\n",
    "Despite its name, Logistic Regression is used for classification. It models the probability of class membership using the logistic function.\n",
    "\n",
    "**Key Hyperparameters:**\n",
    "- `C`: Inverse regularisation strength (smaller = stronger regularisation)\n",
    "- `penalty`: Regularisation type ('l1', 'l2', 'elasticnet')\n",
    "- `solver`: Optimisation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    C: float = 1.0,\n",
    "    max_iter: int = 1000\n",
    ") -> LogisticRegression:\n",
    "    \"\"\"Train a Logistic Regression classifier.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features.\n",
    "        y_train: Training labels.\n",
    "        C: Inverse regularisation strength.\n",
    "        max_iter: Maximum iterations for solver.\n",
    "    \n",
    "    Returns:\n",
    "        Trained Logistic Regression classifier.\n",
    "    \"\"\"\n",
    "    lr = LogisticRegression(C=C, max_iter=max_iter, random_state=42)\n",
    "    lr.fit(X_train, y_train)\n",
    "    return lr\n",
    "\n",
    "lr = train_logistic_regression(X_train_scaled, y_train)\n",
    "y_pred_lr = lr.predict(X_test_scaled)\n",
    "\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"\\nCoefficients shape: {lr.coef_.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Decision Trees\n",
    "\n",
    "Decision Trees make predictions by learning decision rules from features. They're interpretable but prone to overfitting.\n",
    "\n",
    "**Key Hyperparameters:**\n",
    "- `max_depth`: Maximum tree depth\n",
    "- `min_samples_split`: Minimum samples to split a node\n",
    "- `min_samples_leaf`: Minimum samples in a leaf node\n",
    "- `criterion`: Split quality measure ('gini', 'entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decision_tree(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    max_depth: int = None,\n",
    "    min_samples_split: int = 2\n",
    ") -> DecisionTreeClassifier:\n",
    "    \"\"\"Train a Decision Tree classifier.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features.\n",
    "        y_train: Training labels.\n",
    "        max_depth: Maximum depth of tree.\n",
    "        min_samples_split: Minimum samples to split.\n",
    "    \n",
    "    Returns:\n",
    "        Trained Decision Tree classifier.\n",
    "    \"\"\"\n",
    "    dt = DecisionTreeClassifier(\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        random_state=42\n",
    "    )\n",
    "    dt.fit(X_train, y_train)\n",
    "    return dt\n",
    "\n",
    "dt = train_decision_tree(X_train, y_train, max_depth=5)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "print(f\"Decision Tree Accuracy: {accuracy_score(y_test, y_pred_dt):.4f}\")\n",
    "print(f\"Feature importances: {dict(zip(iris.feature_names, dt.feature_importances_.round(3)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Random Forest\n",
    "\n",
    "Random Forest is an ensemble of Decision Trees. It reduces overfitting by averaging predictions from multiple trees trained on random subsets.\n",
    "\n",
    "**Key Hyperparameters:**\n",
    "- `n_estimators`: Number of trees\n",
    "- `max_depth`: Maximum depth of each tree\n",
    "- `max_features`: Number of features to consider for best split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    n_estimators: int = 100,\n",
    "    max_depth: int = None\n",
    ") -> RandomForestClassifier:\n",
    "    \"\"\"Train a Random Forest classifier.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features.\n",
    "        y_train: Training labels.\n",
    "        n_estimators: Number of trees.\n",
    "        max_depth: Maximum depth of trees.\n",
    "    \n",
    "    Returns:\n",
    "        Trained Random Forest classifier.\n",
    "    \"\"\"\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    return rf\n",
    "\n",
    "rf = train_random_forest(X_train, y_train, n_estimators=100)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "print(f\"Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"Feature importances: {dict(zip(iris.feature_names, rf.feature_importances_.round(3)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Support Vector Machine (SVM)\n",
    "\n",
    "SVM finds the optimal hyperplane that maximises the margin between classes. It can handle non-linear boundaries using kernel functions.\n",
    "\n",
    "**Key Hyperparameters:**\n",
    "- `C`: Regularisation parameter\n",
    "- `kernel`: Kernel type ('linear', 'rbf', 'poly')\n",
    "- `gamma`: Kernel coefficient for 'rbf' and 'poly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    C: float = 1.0,\n",
    "    kernel: str = 'rbf'\n",
    ") -> SVC:\n",
    "    \"\"\"Train a Support Vector Machine classifier.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features.\n",
    "        y_train: Training labels.\n",
    "        C: Regularisation parameter.\n",
    "        kernel: Kernel type.\n",
    "    \n",
    "    Returns:\n",
    "        Trained SVM classifier.\n",
    "    \"\"\"\n",
    "    svm = SVC(C=C, kernel=kernel, random_state=42, probability=True)\n",
    "    svm.fit(X_train, y_train)\n",
    "    return svm\n",
    "\n",
    "svm = train_svm(X_train_scaled, y_train, kernel='rbf')\n",
    "y_pred_svm = svm.predict(X_test_scaled)\n",
    "\n",
    "print(f\"SVM Accuracy: {accuracy_score(y_test, y_pred_svm):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Classification Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'Algorithm': ['KNN', 'Logistic Regression', 'Decision Tree', 'Random Forest', 'SVM'],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_test, y_pred_knn),\n",
    "        accuracy_score(y_test, y_pred_lr),\n",
    "        accuracy_score(y_test, y_pred_dt),\n",
    "        accuracy_score(y_test, y_pred_rf),\n",
    "        accuracy_score(y_test, y_pred_svm)\n",
    "    ]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('Accuracy', ascending=False)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Regression Algorithms\n",
    "\n",
    "Regression predicts continuous numerical values. Let's use the diabetes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X_reg, y_reg = diabetes.data, diabetes.target\n",
    "\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dataset shape: {X_reg.shape}\")\n",
    "print(f\"Feature names: {diabetes.feature_names}\")\n",
    "print(f\"Target range: {y_reg.min():.2f} to {y_reg.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Linear Regression\n",
    "\n",
    "Linear Regression fits a linear relationship between features and target: y = w1*x1 + w2*x2 + ... + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_regression(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray\n",
    ") -> LinearRegression:\n",
    "    \"\"\"Train a Linear Regression model.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features.\n",
    "        y_train: Training target.\n",
    "    \n",
    "    Returns:\n",
    "        Trained Linear Regression model.\n",
    "    \"\"\"\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    return lr\n",
    "\n",
    "lin_reg = train_linear_regression(X_train_reg, y_train_reg)\n",
    "y_pred_lin = lin_reg.predict(X_test_reg)\n",
    "\n",
    "print(f\"Linear Regression R2 Score: {r2_score(y_test_reg, y_pred_lin):.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test_reg, y_pred_lin)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Ridge Regression (L2 Regularisation)\n",
    "\n",
    "Ridge adds L2 penalty to prevent overfitting by shrinking coefficients. The penalty term is the sum of squared coefficients.\n",
    "\n",
    "**Key Hyperparameter:**\n",
    "- `alpha`: Regularisation strength (higher = more regularisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ridge_regression(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    alpha: float = 1.0\n",
    ") -> Ridge:\n",
    "    \"\"\"Train a Ridge Regression model.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features.\n",
    "        y_train: Training target.\n",
    "        alpha: Regularisation strength.\n",
    "    \n",
    "    Returns:\n",
    "        Trained Ridge Regression model.\n",
    "    \"\"\"\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    return ridge\n",
    "\n",
    "ridge = train_ridge_regression(X_train_reg, y_train_reg, alpha=1.0)\n",
    "y_pred_ridge = ridge.predict(X_test_reg)\n",
    "\n",
    "print(f\"Ridge Regression R2 Score: {r2_score(y_test_reg, y_pred_ridge):.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test_reg, y_pred_ridge)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Lasso Regression (L1 Regularisation)\n",
    "\n",
    "Lasso adds L1 penalty, which can shrink some coefficients to exactly zero (feature selection). The penalty term is the sum of absolute values of coefficients.\n",
    "\n",
    "**Key Hyperparameter:**\n",
    "- `alpha`: Regularisation strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lasso_regression(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    alpha: float = 1.0\n",
    ") -> Lasso:\n",
    "    \"\"\"Train a Lasso Regression model.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features.\n",
    "        y_train: Training target.\n",
    "        alpha: Regularisation strength.\n",
    "    \n",
    "    Returns:\n",
    "        Trained Lasso Regression model.\n",
    "    \"\"\"\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    return lasso\n",
    "\n",
    "lasso = train_lasso_regression(X_train_reg, y_train_reg, alpha=0.1)\n",
    "y_pred_lasso = lasso.predict(X_test_reg)\n",
    "\n",
    "print(f\"Lasso Regression R2 Score: {r2_score(y_test_reg, y_pred_lasso):.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test_reg, y_pred_lasso)):.4f}\")\n",
    "print(f\"\\nNon-zero coefficients: {np.sum(lasso.coef_ != 0)} out of {len(lasso.coef_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: Ridge vs Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare coefficients\n",
    "coef_comparison = pd.DataFrame({\n",
    "    'Feature': diabetes.feature_names,\n",
    "    'Linear': lin_reg.coef_.round(2),\n",
    "    'Ridge': ridge.coef_.round(2),\n",
    "    'Lasso': lasso.coef_.round(2)\n",
    "})\n",
    "print(coef_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation Metrics\n",
    "\n",
    "### Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use breast cancer dataset for binary classification metrics\n",
    "cancer = load_breast_cancer()\n",
    "X_cancer, y_cancer = cancer.data, cancer.target\n",
    "\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
    ")\n",
    "\n",
    "# Train a model\n",
    "scaler_c = StandardScaler()\n",
    "X_train_c_scaled = scaler_c.fit_transform(X_train_c)\n",
    "X_test_c_scaled = scaler_c.transform(X_test_c)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_c_scaled, y_train_c)\n",
    "y_pred_c = clf.predict(X_test_c_scaled)\n",
    "y_prob_c = clf.predict_proba(X_test_c_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_classification_metrics(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    y_prob: np.ndarray = None\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Calculate classification metrics.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels.\n",
    "        y_pred: Predicted labels.\n",
    "        y_prob: Predicted probabilities (for ROC-AUC).\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of metric names and values.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred),\n",
    "        'Recall': recall_score(y_true, y_pred),\n",
    "        'F1 Score': f1_score(y_true, y_pred),\n",
    "    }\n",
    "    if y_prob is not None:\n",
    "        metrics['ROC-AUC'] = roc_auc_score(y_true, y_prob)\n",
    "    return metrics\n",
    "\n",
    "metrics = calculate_classification_metrics(y_test_c, y_pred_c, y_prob_c)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metric Explanations:**\n",
    "\n",
    "- **Accuracy**: Proportion of correct predictions (TP + TN) / Total\n",
    "- **Precision**: Of predicted positives, how many are actually positive? TP / (TP + FP)\n",
    "- **Recall (Sensitivity)**: Of actual positives, how many did we predict correctly? TP / (TP + FN)\n",
    "- **F1 Score**: Harmonic mean of precision and recall. 2 * (Precision * Recall) / (Precision + Recall)\n",
    "- **ROC-AUC**: Area under the ROC curve. Measures ability to distinguish between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test_c, y_prob_c)\n",
    "auc = roc_auc_score(y_test_c, y_prob_c)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_regression_metrics(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Calculate regression metrics.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True values.\n",
    "        y_pred: Predicted values.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of metric names and values.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'MSE': mean_squared_error(y_true, y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'R2 Score': r2_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "reg_metrics = calculate_regression_metrics(y_test_reg, y_pred_lin)\n",
    "for metric, value in reg_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metric Explanations:**\n",
    "\n",
    "- **MAE (Mean Absolute Error)**: Average of absolute differences. Easy to interpret.\n",
    "- **MSE (Mean Squared Error)**: Average of squared differences. Penalises larger errors more.\n",
    "- **RMSE (Root Mean Squared Error)**: Square root of MSE. Same units as target variable.\n",
    "- **R2 Score**: Proportion of variance explained by the model. 1 = perfect, 0 = baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Confusion Matrix Interpretation\n",
    "\n",
    "A confusion matrix shows the counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    labels: List[str] = None\n",
    ") -> None:\n",
    "    \"\"\"Plot a confusion matrix heatmap.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels.\n",
    "        y_pred: Predicted labels.\n",
    "        labels: Class labels for display.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt='d', cmap='Blues',\n",
    "        xticklabels=labels, yticklabels=labels\n",
    "    )\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(y_test_c, y_pred_c, labels=['Malignant', 'Benign'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_c, y_pred_c, target_names=['Malignant', 'Benign']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the Confusion Matrix:**\n",
    "\n",
    "```\n",
    "                  Predicted\n",
    "              Negative  Positive\n",
    "Actual  Neg     TN        FP\n",
    "        Pos     FN        TP\n",
    "```\n",
    "\n",
    "- **True Positives (TP)**: Correctly predicted positive\n",
    "- **True Negatives (TN)**: Correctly predicted negative\n",
    "- **False Positives (FP)**: Incorrectly predicted positive (Type I error)\n",
    "- **False Negatives (FN)**: Incorrectly predicted negative (Type II error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Scaling\n",
    "\n",
    "Feature scaling is crucial for algorithms that are distance-based (KNN, SVM) or gradient-based (neural networks, logistic regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 StandardScaler (Z-score normalisation)\n",
    "\n",
    "Transforms features to have mean=0 and standard deviation=1.\n",
    "\n",
    "Formula: z = (x - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_standard_scaling(\n",
    "    X_train: np.ndarray,\n",
    "    X_test: np.ndarray\n",
    ") -> Tuple[np.ndarray, np.ndarray, StandardScaler]:\n",
    "    \"\"\"Apply standard scaling to features.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features.\n",
    "        X_test: Test features.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (scaled training data, scaled test data, fitted scaler).\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "X_train_std, X_test_std, std_scaler = apply_standard_scaling(X_train, X_test)\n",
    "\n",
    "print(\"Before StandardScaler:\")\n",
    "print(f\"  Mean: {X_train.mean(axis=0).round(2)}\")\n",
    "print(f\"  Std: {X_train.std(axis=0).round(2)}\")\n",
    "\n",
    "print(\"\\nAfter StandardScaler:\")\n",
    "print(f\"  Mean: {X_train_std.mean(axis=0).round(2)}\")\n",
    "print(f\"  Std: {X_train_std.std(axis=0).round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 MinMaxScaler\n",
    "\n",
    "Scales features to a fixed range, typically [0, 1].\n",
    "\n",
    "Formula: x_scaled = (x - min) / (max - min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_minmax_scaling(\n",
    "    X_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    feature_range: Tuple[int, int] = (0, 1)\n",
    ") -> Tuple[np.ndarray, np.ndarray, MinMaxScaler]:\n",
    "    \"\"\"Apply min-max scaling to features.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features.\n",
    "        X_test: Test features.\n",
    "        feature_range: Desired range of scaled features.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (scaled training data, scaled test data, fitted scaler).\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler(feature_range=feature_range)\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "X_train_mm, X_test_mm, mm_scaler = apply_minmax_scaling(X_train, X_test)\n",
    "\n",
    "print(\"Before MinMaxScaler:\")\n",
    "print(f\"  Min: {X_train.min(axis=0).round(2)}\")\n",
    "print(f\"  Max: {X_train.max(axis=0).round(2)}\")\n",
    "\n",
    "print(\"\\nAfter MinMaxScaler:\")\n",
    "print(f\"  Min: {X_train_mm.min(axis=0).round(2)}\")\n",
    "print(f\"  Max: {X_train_mm.max(axis=0).round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to use which scaler?**\n",
    "\n",
    "- **StandardScaler**: When features follow a roughly Gaussian distribution. Good default choice.\n",
    "- **MinMaxScaler**: When you need bounded values (e.g., neural networks with bounded activations). Sensitive to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Selection Techniques\n",
    "\n",
    "Feature selection helps reduce dimensionality, improve model performance, and reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 SelectKBest (Filter Method)\n",
    "\n",
    "Selects the K highest-scoring features based on a statistical test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_k_best_features(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    k: int = 5,\n",
    "    score_func=f_classif\n",
    ") -> Tuple[np.ndarray, SelectKBest]:\n",
    "    \"\"\"Select K best features using statistical test.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix.\n",
    "        y: Target vector.\n",
    "        k: Number of features to select.\n",
    "        score_func: Scoring function.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (selected features, fitted selector).\n",
    "    \"\"\"\n",
    "    selector = SelectKBest(score_func=score_func, k=k)\n",
    "    X_selected = selector.fit_transform(X, y)\n",
    "    return X_selected, selector\n",
    "\n",
    "X_selected, selector = select_k_best_features(X, y, k=2)\n",
    "\n",
    "print(f\"Original shape: {X.shape}\")\n",
    "print(f\"Selected shape: {X_selected.shape}\")\n",
    "print(f\"Feature scores: {dict(zip(iris.feature_names, selector.scores_.round(2)))}\")\n",
    "print(f\"Selected features: {np.array(iris.feature_names)[selector.get_support()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Recursive Feature Elimination (RFE)\n",
    "\n",
    "Recursively removes least important features based on model coefficients/importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_feature_elimination(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    n_features_to_select: int = 2,\n",
    "    estimator=None\n",
    ") -> Tuple[np.ndarray, RFE]:\n",
    "    \"\"\"Perform recursive feature elimination.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix.\n",
    "        y: Target vector.\n",
    "        n_features_to_select: Number of features to select.\n",
    "        estimator: Base estimator (default: LogisticRegression).\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (selected features, fitted RFE object).\n",
    "    \"\"\"\n",
    "    if estimator is None:\n",
    "        estimator = LogisticRegression(max_iter=1000)\n",
    "    \n",
    "    rfe = RFE(estimator=estimator, n_features_to_select=n_features_to_select)\n",
    "    X_selected = rfe.fit_transform(X, y)\n",
    "    return X_selected, rfe\n",
    "\n",
    "X_rfe, rfe = recursive_feature_elimination(X, y, n_features_to_select=2)\n",
    "\n",
    "print(f\"Feature ranking: {dict(zip(iris.feature_names, rfe.ranking_))}\")\n",
    "print(f\"Selected features: {np.array(iris.feature_names)[rfe.support_]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 SelectFromModel (Embedded Method)\n",
    "\n",
    "Selects features based on importance weights from a fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_from_model(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    threshold: str = 'mean'\n",
    ") -> Tuple[np.ndarray, SelectFromModel]:\n",
    "    \"\"\"Select features using model-based importance.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix.\n",
    "        y: Target vector.\n",
    "        threshold: Threshold for feature selection.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (selected features, fitted selector).\n",
    "    \"\"\"\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    \n",
    "    selector = SelectFromModel(rf, threshold=threshold, prefit=True)\n",
    "    X_selected = selector.transform(X)\n",
    "    return X_selected, selector\n",
    "\n",
    "X_sfm, sfm_selector = select_from_model(X, y)\n",
    "\n",
    "print(f\"Original shape: {X.shape}\")\n",
    "print(f\"Selected shape: {X_sfm.shape}\")\n",
    "print(f\"Selected features: {np.array(iris.feature_names)[sfm_selector.get_support()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning\n",
    "\n",
    "Hyperparameters are parameters set before training (e.g., learning rate, number of trees). Tuning finds optimal values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 GridSearchCV\n",
    "\n",
    "Exhaustively searches over all combinations of specified hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_cv(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    estimator: Any,\n",
    "    param_grid: Dict[str, List],\n",
    "    cv: int = 5\n",
    ") -> GridSearchCV:\n",
    "    \"\"\"Perform grid search cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix.\n",
    "        y: Target vector.\n",
    "        estimator: Model to tune.\n",
    "        param_grid: Dictionary of parameters to search.\n",
    "        cv: Number of cross-validation folds.\n",
    "    \n",
    "    Returns:\n",
    "        Fitted GridSearchCV object.\n",
    "    \"\"\"\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X, y)\n",
    "    return grid_search\n",
    "\n",
    "# Example: Tune KNN\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "grid_search = grid_search_cv(X_train_scaled, y_train, KNeighborsClassifier(), param_grid)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 RandomizedSearchCV\n",
    "\n",
    "Samples a fixed number of parameter combinations from specified distributions. More efficient than grid search for large parameter spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint, uniform\n",
    "\n",
    "def randomized_search_cv(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    estimator: Any,\n",
    "    param_distributions: Dict[str, Any],\n",
    "    n_iter: int = 20,\n",
    "    cv: int = 5\n",
    ") -> RandomizedSearchCV:\n",
    "    \"\"\"Perform randomised search cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix.\n",
    "        y: Target vector.\n",
    "        estimator: Model to tune.\n",
    "        param_distributions: Dictionary of parameter distributions.\n",
    "        n_iter: Number of parameter settings sampled.\n",
    "        cv: Number of cross-validation folds.\n",
    "    \n",
    "    Returns:\n",
    "        Fitted RandomizedSearchCV object.\n",
    "    \"\"\"\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=estimator,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=n_iter,\n",
    "        cv=cv,\n",
    "        scoring='accuracy',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    random_search.fit(X, y)\n",
    "    return random_search\n",
    "\n",
    "# Example: Tune Random Forest\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_depth': [None, 5, 10, 15, 20],\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search = randomized_search_cv(\n",
    "    X_train, y_train,\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_distributions,\n",
    "    n_iter=20\n",
    ")\n",
    "\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {random_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Pipelines in Scikit-Learn\n",
    "\n",
    "Pipelines chain multiple processing steps into a single estimator. Benefits:\n",
    "- Cleaner code\n",
    "- Prevents data leakage during cross-validation\n",
    "- Easy deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classification_pipeline(\n",
    "    scaler: Any = StandardScaler(),\n",
    "    classifier: Any = LogisticRegression(max_iter=1000)\n",
    ") -> Pipeline:\n",
    "    \"\"\"Create a classification pipeline with scaling and classifier.\n",
    "    \n",
    "    Args:\n",
    "        scaler: Feature scaler.\n",
    "        classifier: Classification model.\n",
    "    \n",
    "    Returns:\n",
    "        Scikit-learn Pipeline object.\n",
    "    \"\"\"\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "# Create and train pipeline\n",
    "pipeline = create_classification_pipeline(\n",
    "    scaler=StandardScaler(),\n",
    "    classifier=SVC(kernel='rbf', random_state=42)\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred_pipe = pipeline.predict(X_test)\n",
    "\n",
    "print(f\"Pipeline Accuracy: {accuracy_score(y_test, y_pred_pipe):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline with feature selection\n",
    "pipeline_with_selection = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_selection', SelectKBest(score_func=f_classif, k=3)),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "pipeline_with_selection.fit(X_train, y_train)\n",
    "y_pred_pipe2 = pipeline_with_selection.predict(X_test)\n",
    "\n",
    "print(f\"Pipeline with Feature Selection Accuracy: {accuracy_score(y_test, y_pred_pipe2):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV with Pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', SVC(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid_pipe = {\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__kernel': ['linear', 'rbf'],\n",
    "    'classifier__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "grid_pipe = GridSearchCV(pipe, param_grid_pipe, cv=5, scoring='accuracy')\n",
    "grid_pipe.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_pipe.best_params_}\")\n",
    "print(f\"Best CV score: {grid_pipe.best_score_:.4f}\")\n",
    "print(f\"Test accuracy: {grid_pipe.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Handling Imbalanced Datasets\n",
    "\n",
    "Imbalanced datasets have significantly more samples of one class than others. This can bias models towards the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an imbalanced dataset\n",
    "X_imb, y_imb = make_classification(\n",
    "    n_samples=1000, n_features=20, n_informative=10,\n",
    "    n_redundant=5, n_clusters_per_class=1,\n",
    "    weights=[0.9, 0.1], flip_y=0, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Class distribution: {np.bincount(y_imb)}\")\n",
    "print(f\"Class 0: {np.sum(y_imb==0)} ({np.sum(y_imb==0)/len(y_imb)*100:.1f}%)\")\n",
    "print(f\"Class 1: {np.sum(y_imb==1)} ({np.sum(y_imb==1)/len(y_imb)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Techniques for Handling Imbalanced Data\n",
    "\n",
    "1. **Class Weights**: Assign higher weights to minority class\n",
    "2. **Resampling**: Over-sample minority or under-sample majority\n",
    "3. **Use appropriate metrics**: F1, precision, recall instead of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imb, X_test_imb, y_train_imb, y_test_imb = train_test_split(\n",
    "    X_imb, y_imb, test_size=0.2, random_state=42, stratify=y_imb\n",
    ")\n",
    "\n",
    "# Without class weights\n",
    "clf_no_weights = LogisticRegression(max_iter=1000)\n",
    "clf_no_weights.fit(X_train_imb, y_train_imb)\n",
    "y_pred_no_weights = clf_no_weights.predict(X_test_imb)\n",
    "\n",
    "# With class weights\n",
    "clf_balanced = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "clf_balanced.fit(X_train_imb, y_train_imb)\n",
    "y_pred_balanced = clf_balanced.predict(X_test_imb)\n",
    "\n",
    "print(\"Without class weights:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_test_imb, y_pred_no_weights):.4f}\")\n",
    "print(f\"  F1 Score: {f1_score(y_test_imb, y_pred_no_weights):.4f}\")\n",
    "print(f\"  Recall (minority): {recall_score(y_test_imb, y_pred_no_weights):.4f}\")\n",
    "\n",
    "print(\"\\nWith class weights (balanced):\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_test_imb, y_pred_balanced):.4f}\")\n",
    "print(f\"  F1 Score: {f1_score(y_test_imb, y_pred_balanced):.4f}\")\n",
    "print(f\"  Recall (minority): {recall_score(y_test_imb, y_pred_balanced):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison with confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for ax, y_pred, title in zip(\n",
    "    axes,\n",
    "    [y_pred_no_weights, y_pred_balanced],\n",
    "    ['Without Class Weights', 'With Balanced Class Weights']\n",
    "):\n",
    "    cm = confusion_matrix(y_test_imb, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Overfitting and Underfitting\n",
    "\n",
    "- **Overfitting**: Model learns training data too well, including noise. High training accuracy, low test accuracy.\n",
    "- **Underfitting**: Model is too simple to capture patterns. Low accuracy on both training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_fitting_issues(\n",
    "    X_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    y_test: np.ndarray\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Demonstrate overfitting and underfitting with Decision Trees.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features.\n",
    "        X_test: Test features.\n",
    "        y_train: Training labels.\n",
    "        y_test: Test labels.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with training and test scores for different model complexities.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for max_depth in [1, 2, 3, 5, 10, 20, None]:\n",
    "        dt = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
    "        dt.fit(X_train, y_train)\n",
    "        \n",
    "        train_score = dt.score(X_train, y_train)\n",
    "        test_score = dt.score(X_test, y_test)\n",
    "        \n",
    "        results.append({\n",
    "            'max_depth': str(max_depth),\n",
    "            'train_accuracy': train_score,\n",
    "            'test_accuracy': test_score,\n",
    "            'gap': train_score - test_score\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "fitting_results = demonstrate_fitting_issues(X_train, X_test, y_train, y_test)\n",
    "print(fitting_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise overfitting/underfitting\n",
    "depths = [1, 2, 3, 5, 10, 20, 30]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for depth in depths:\n",
    "    dt = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    dt.fit(X_train, y_train)\n",
    "    train_scores.append(dt.score(X_train, y_train))\n",
    "    test_scores.append(dt.score(X_test, y_test))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(depths, train_scores, 'o-', label='Training Score')\n",
    "plt.plot(depths, test_scores, 'o-', label='Test Score')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Overfitting vs Underfitting: Decision Tree Depth')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.axhline(y=1.0, color='r', linestyle='--', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solutions to Overfitting:**\n",
    "- Use regularisation (L1/L2)\n",
    "- Reduce model complexity\n",
    "- Get more training data\n",
    "- Feature selection\n",
    "- Cross-validation\n",
    "- Early stopping\n",
    "- Dropout (for neural networks)\n",
    "\n",
    "**Solutions to Underfitting:**\n",
    "- Use more complex model\n",
    "- Add more features\n",
    "- Reduce regularisation\n",
    "- Increase training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13. Practice Questions\n",
    "\n",
    "Test your understanding with the following exercises. Try to solve them before revealing the answers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Train/Test Split\n",
    "\n",
    "Load the wine dataset from sklearn, split it into training (80%) and test (20%) sets with stratification, and print the class distribution in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Load the wine dataset\n",
    "wine = load_wine()\n",
    "X_wine, y_wine = wine.data, wine.target\n",
    "\n",
    "# Split with stratification\n",
    "X_train_w, X_test_w, y_train_w, y_test_w = train_test_split(\n",
    "    X_wine, y_wine, test_size=0.2, random_state=42, stratify=y_wine\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train_w)}\")\n",
    "print(f\"Test set size: {len(X_test_w)}\")\n",
    "print(f\"\\nTraining class distribution: {np.bincount(y_train_w)}\")\n",
    "print(f\"Test class distribution: {np.bincount(y_test_w)}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Cross-Validation\n",
    "\n",
    "Perform 5-fold cross-validation on the Iris dataset using a Random Forest classifier with 50 trees. Print the mean and standard deviation of the accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "X_iris, y_iris = iris.data, iris.target\n",
    "\n",
    "# Create model\n",
    "rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(rf, X_iris, y_iris, cv=5, scoring='accuracy')\n",
    "\n",
    "print(f\"Cross-validation scores: {scores}\")\n",
    "print(f\"Mean accuracy: {scores.mean():.4f}\")\n",
    "print(f\"Standard deviation: {scores.std():.4f}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: KNN Classification\n",
    "\n",
    "Train a KNN classifier on the breast cancer dataset with k=7 and distance-weighted voting. Remember to scale the features. Report the accuracy, precision, and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Load data\n",
    "cancer = load_breast_cancer()\n",
    "X_cancer, y_cancer = cancer.data, cancer.target\n",
    "\n",
    "# Split data\n",
    "X_train_bc, X_test_bc, y_train_bc, y_test_bc = train_test_split(\n",
    "    X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_bc)\n",
    "X_test_scaled = scaler.transform(X_test_bc)\n",
    "\n",
    "# Train KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=7, weights='distance')\n",
    "knn.fit(X_train_scaled, y_train_bc)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test_bc, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test_bc, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test_bc, y_pred):.4f}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Decision Tree with Pruning\n",
    "\n",
    "Train a Decision Tree on the Iris dataset with max_depth=3 and min_samples_leaf=5. Visualise the feature importances as a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "X_iris, y_iris = iris.data, iris.target\n",
    "\n",
    "# Train Decision Tree with pruning\n",
    "dt = DecisionTreeClassifier(\n",
    "    max_depth=3,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42\n",
    ")\n",
    "dt.fit(X_iris, y_iris)\n",
    "\n",
    "# Plot feature importances\n",
    "importances = dt.feature_importances_\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_names, importances)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Decision Tree Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training accuracy: {dt.score(X_iris, y_iris):.4f}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Ridge vs Lasso Regression\n",
    "\n",
    "Using the diabetes dataset, compare Ridge and Lasso regression with alpha=0.5. Print the R2 score and count how many coefficients Lasso sets to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "diabetes = load_diabetes()\n",
    "X_diab, y_diab = diabetes.data, diabetes.target\n",
    "\n",
    "# Split data\n",
    "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(\n",
    "    X_diab, y_diab, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train Ridge\n",
    "ridge = Ridge(alpha=0.5)\n",
    "ridge.fit(X_train_d, y_train_d)\n",
    "y_pred_ridge = ridge.predict(X_test_d)\n",
    "\n",
    "# Train Lasso\n",
    "lasso = Lasso(alpha=0.5)\n",
    "lasso.fit(X_train_d, y_train_d)\n",
    "y_pred_lasso = lasso.predict(X_test_d)\n",
    "\n",
    "print(\"Ridge Regression:\")\n",
    "print(f\"  R2 Score: {r2_score(y_test_d, y_pred_ridge):.4f}\")\n",
    "print(f\"  Non-zero coefficients: {np.sum(ridge.coef_ != 0)}\")\n",
    "\n",
    "print(\"\\nLasso Regression:\")\n",
    "print(f\"  R2 Score: {r2_score(y_test_d, y_pred_lasso):.4f}\")\n",
    "print(f\"  Non-zero coefficients: {np.sum(lasso.coef_ != 0)}\")\n",
    "print(f\"  Zero coefficients: {np.sum(lasso.coef_ == 0)}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: Complete Pipeline\n",
    "\n",
    "Create a pipeline that:\n",
    "1. Scales features with StandardScaler\n",
    "2. Selects the top 10 features with SelectKBest\n",
    "3. Classifies with SVM (RBF kernel)\n",
    "\n",
    "Apply this pipeline to the breast cancer dataset and report the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "cancer = load_breast_cancer()\n",
    "X_bc, y_bc = cancer.data, cancer.target\n",
    "\n",
    "# Split data\n",
    "X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(\n",
    "    X_bc, y_bc, test_size=0.2, random_state=42, stratify=y_bc\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_selection', SelectKBest(score_func=f_classif, k=10)),\n",
    "    ('classifier', SVC(kernel='rbf', random_state=42))\n",
    "])\n",
    "\n",
    "# Train and evaluate\n",
    "pipeline.fit(X_train_p, y_train_p)\n",
    "y_pred_p = pipeline.predict(X_test_p)\n",
    "\n",
    "print(f\"Pipeline Accuracy: {accuracy_score(y_test_p, y_pred_p):.4f}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: GridSearchCV\n",
    "\n",
    "Use GridSearchCV to find the best hyperparameters for a Random Forest on the wine dataset. Search over:\n",
    "- n_estimators: [50, 100, 150]\n",
    "- max_depth: [5, 10, None]\n",
    "- min_samples_split: [2, 5, 10]\n",
    "\n",
    "Report the best parameters and best cross-validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "# Load data\n",
    "wine = load_wine()\n",
    "X_wine, y_wine = wine.data, wine.target\n",
    "\n",
    "# Split data\n",
    "X_train_g, X_test_g, y_train_g, y_test_g = train_test_split(\n",
    "    X_wine, y_wine, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Create and run GridSearchCV\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train_g, y_train_g)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Test accuracy: {grid_search.score(X_test_g, y_test_g):.4f}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8: Confusion Matrix Analysis\n",
    "\n",
    "Train a Logistic Regression model on the breast cancer dataset. Create a confusion matrix and calculate precision, recall, and F1-score manually from the matrix values (TP, TN, FP, FN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load and prepare data\n",
    "cancer = load_breast_cancer()\n",
    "X_cm, y_cm = cancer.data, cancer.target\n",
    "\n",
    "X_train_cm, X_test_cm, y_train_cm, y_test_cm = train_test_split(\n",
    "    X_cm, y_cm, test_size=0.2, random_state=42, stratify=y_cm\n",
    ")\n",
    "\n",
    "# Scale and train\n",
    "scaler = StandardScaler()\n",
    "X_train_cm_s = scaler.fit_transform(X_train_cm)\n",
    "X_test_cm_s = scaler.transform(X_test_cm)\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_cm_s, y_train_cm)\n",
    "y_pred_cm = lr.predict(X_test_cm_s)\n",
    "\n",
    "# Get confusion matrix\n",
    "cm = confusion_matrix(y_test_cm, y_pred_cm)\n",
    "print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "# Extract values\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "print(f\"\\nTN={TN}, FP={FP}, FN={FN}, TP={TP}\")\n",
    "\n",
    "# Calculate metrics manually\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f\"\\nManually calculated:\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9: Handling Imbalanced Data\n",
    "\n",
    "Create an imbalanced dataset using make_classification with 95% majority class. Train two Random Forest models - one without class weights and one with balanced class weights. Compare their F1 scores on the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Create imbalanced dataset\n",
    "X_imb, y_imb = make_classification(\n",
    "    n_samples=2000, n_features=20, n_informative=10,\n",
    "    weights=[0.95, 0.05], flip_y=0, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Class distribution: {np.bincount(y_imb)}\")\n",
    "\n",
    "# Split data\n",
    "X_train_i, X_test_i, y_train_i, y_test_i = train_test_split(\n",
    "    X_imb, y_imb, test_size=0.2, random_state=42, stratify=y_imb\n",
    ")\n",
    "\n",
    "# Without class weights\n",
    "rf_no_weights = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_no_weights.fit(X_train_i, y_train_i)\n",
    "y_pred_no = rf_no_weights.predict(X_test_i)\n",
    "\n",
    "# With balanced class weights\n",
    "rf_balanced = RandomForestClassifier(\n",
    "    n_estimators=100, class_weight='balanced', random_state=42\n",
    ")\n",
    "rf_balanced.fit(X_train_i, y_train_i)\n",
    "y_pred_bal = rf_balanced.predict(X_test_i)\n",
    "\n",
    "print(\"\\nWithout class weights:\")\n",
    "print(f\"F1 Score (minority class): {f1_score(y_test_i, y_pred_no, pos_label=1):.4f}\")\n",
    "\n",
    "print(\"\\nWith balanced class weights:\")\n",
    "print(f\"F1 Score (minority class): {f1_score(y_test_i, y_pred_bal, pos_label=1):.4f}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10: Complete ML Workflow\n",
    "\n",
    "Implement a complete machine learning workflow for the wine dataset:\n",
    "1. Load and split data (80/20)\n",
    "2. Create a pipeline with StandardScaler and SVM\n",
    "3. Use GridSearchCV to tune C (0.1, 1, 10) and kernel ('linear', 'rbf')\n",
    "4. Evaluate on test set with accuracy and classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Step 1: Load and split data\n",
    "wine = load_wine()\n",
    "X_w, y_w = wine.data, wine.target\n",
    "\n",
    "X_train_w, X_test_w, y_train_w, y_test_w = train_test_split(\n",
    "    X_w, y_w, test_size=0.2, random_state=42, stratify=y_w\n",
    ")\n",
    "\n",
    "# Step 2: Create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC(random_state=42))\n",
    "])\n",
    "\n",
    "# Step 3: GridSearchCV\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10],\n",
    "    'svm__kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train_w, y_train_w)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Step 4: Evaluate on test set\n",
    "y_pred_w = grid_search.predict(X_test_w)\n",
    "\n",
    "print(f\"\\nTest Accuracy: {accuracy_score(y_test_w, y_pred_w):.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_w, y_pred_w, target_names=wine.target_names))\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11: ROC Curve\n",
    "\n",
    "Train a Logistic Regression model on the breast cancer dataset. Plot the ROC curve and calculate the AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and prepare data\n",
    "cancer = load_breast_cancer()\n",
    "X_roc, y_roc = cancer.data, cancer.target\n",
    "\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(\n",
    "    X_roc, y_roc, test_size=0.2, random_state=42, stratify=y_roc\n",
    ")\n",
    "\n",
    "# Scale and train\n",
    "scaler = StandardScaler()\n",
    "X_train_r_s = scaler.fit_transform(X_train_r)\n",
    "X_test_r_s = scaler.transform(X_test_r)\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_r_s, y_train_r)\n",
    "\n",
    "# Get probabilities\n",
    "y_prob = lr.predict_proba(X_test_r_s)[:, 1]\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test_r, y_prob)\n",
    "auc = roc_auc_score(y_test_r, y_prob)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Breast Cancer Classification')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"AUC Score: {auc:.4f}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12: Comparing Multiple Models\n",
    "\n",
    "Compare the performance of KNN, Logistic Regression, Decision Tree, Random Forest, and SVM on the Iris dataset using 5-fold cross-validation. Display results in a pandas DataFrame sorted by mean accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "X_comp, y_comp = iris.data, iris.target\n",
    "\n",
    "# Define models with pipelines (for scaling)\n",
    "models = {\n",
    "    'KNN': Pipeline([('scaler', StandardScaler()), ('clf', KNeighborsClassifier())]),\n",
    "    'Logistic Regression': Pipeline([('scaler', StandardScaler()), ('clf', LogisticRegression(max_iter=1000))]),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': Pipeline([('scaler', StandardScaler()), ('clf', SVC(random_state=42))])\n",
    "}\n",
    "\n",
    "# Compare models\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_comp, y_comp, cv=5, scoring='accuracy')\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Mean Accuracy': scores.mean(),\n",
    "        'Std Deviation': scores.std()\n",
    "    })\n",
    "\n",
    "# Create and display DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('Mean Accuracy', ascending=False)\n",
    "results_df['Mean Accuracy'] = results_df['Mean Accuracy'].round(4)\n",
    "results_df['Std Deviation'] = results_df['Std Deviation'].round(4)\n",
    "\n",
    "print(results_df.to_string(index=False))\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook covered the essential scikit-learn concepts for data science assessments:\n",
    "\n",
    "1. **Data Splitting**: train_test_split with stratification, cross-validation\n",
    "2. **Classification**: KNN, Logistic Regression, Decision Trees, Random Forest, SVM\n",
    "3. **Regression**: Linear, Ridge (L2), Lasso (L1)\n",
    "4. **Metrics**: Accuracy, Precision, Recall, F1, ROC-AUC, RMSE, MAE, R2\n",
    "5. **Feature Scaling**: StandardScaler, MinMaxScaler\n",
    "6. **Feature Selection**: SelectKBest, RFE, SelectFromModel\n",
    "7. **Hyperparameter Tuning**: GridSearchCV, RandomizedSearchCV\n",
    "8. **Pipelines**: Chaining preprocessing and models\n",
    "9. **Imbalanced Data**: Class weights, appropriate metrics\n",
    "10. **Overfitting/Underfitting**: Recognition and solutions\n",
    "\n",
    "**Key Interview Tips:**\n",
    "- Always split data before any preprocessing to prevent data leakage\n",
    "- Use pipelines for clean, reproducible workflows\n",
    "- Choose metrics appropriate for your problem (e.g., F1 for imbalanced data)\n",
    "- Understand the trade-offs between different algorithms\n",
    "- Know when to use regularisation and which type (L1 vs L2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
